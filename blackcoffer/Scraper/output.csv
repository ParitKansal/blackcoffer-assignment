avg_number_of_words_per_sentence,avg_sentence_length,avg_word_length,complex_word_count,content,fog_index,negative_score,percentage_of_complex_words,personal_pronouns,polarity_score,positive_score,subjectivity_score,syllable_per_word,url,word_count
12.22439024390244,12.22439024390244,7.839760999456817,1097,"ml and ai-based insurance premium model to predict premium to be charged by the insurance company Client Background Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The Problem The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Data Complexity: The relevant data for predicting insurance premiums in this context is multifaceted, involving financial data, legal precedents, market trends, and individual directorial histories. Integrating and interpreting this diverse set of data poses a significant challenge. Dynamic Risk Factors: The risk factors influencing insider trading public lawsuits are dynamic and subject to rapid changes. The models must be capable of adapting to evolving market conditions, legal landscapes, and individual company dynamics. Fairness and Ethics: Ensuring fairness in premium calculation is critical. The models should be designed to avoid biases and discriminatory practices, considering the diverse backgrounds and contexts of Public Company Directors. Regulatory Compliance: The insurance industry is subject to regulatory frameworks that vary across jurisdictions. The developed models need to comply with these regulations while providing accurate and reliable predictions. Interpretability: Transparency in model predictions is crucial, especially in an industry where decisions can have significant financial implications. Ensuring that the AI and ML models are interpretable and explainable is vital for gaining the trust of stakeholders. Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. Blackcoffer Solution To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. Data Collection and Preprocessing: Financial Data: Gather financial data related to the insured companies, including revenue, profit margins, and financial stability indicators. Incorporate stock market data and trading patterns to capture potential insider trading signals. Legal History: Collect historical legal cases related to insider trading lawsuits, with a focus on outcomes and financial implications. Integrate legal precedents to understand patterns and potential future risks. Directorial Profiles: Compile individual profiles for each Public Company Director, including their professional history, prior legal involvements, and any relevant affiliations. Market Trends and Regulatory Changes: Monitor market trends and regulatory changes affecting the insurance landscape. Incorporate external data sources for real-time updates on legal and market conditions. Feature Engineering: Risk Factors: Identify key risk factors contributing to the likelihood of insider trading allegations. Develop features that encapsulate financial stability, market conditions, and individual directorial behaviors. Sentiment Analysis: Implement sentiment analysis on news articles and social media to gauge public perception and potential legal scrutiny. Machine Learning Models: Supervised Learning: Employ supervised learning algorithms such as Random Forests, Gradient Boosting, or ensemble models. Train the model on historical data with labeled outcomes related to insider trading lawsuits. Anomaly Detection: Implement anomaly detection techniques to identify unusual patterns that may indicate potential insider trading activities. Dynamic Risk Assessment: Real-Time Updates: Design the model to continuously update with real-time data to adapt to evolving risk factors. Implement a feedback loop to capture the impact of recent legal cases and market events. Scenario Analysis: Develop scenario analysis capabilities to assess the impact of hypothetical events on premium calculations. Fairness and Transparency: Fairness Metrics: Integrate fairness metrics to ensure unbiased predictions across diverse directorial profiles. Regularly audit and refine the model to address any identified biases. Explainability: Implement model explainability tools to provide clear insights into premium calculations. Ensure transparency in how the model arrives at its predictions. Model Integration and Deployment: User-Friendly Interface: Develop a user-friendly interface for underwriters to interact with the model. Ensure seamless integration into the existing insurance company workflow. API Integration: Provide API endpoints for easy integration with existing insurance systems. Monitoring and Maintenance: Model Monitoring: Implement continuous monitoring to detect model drift and performance degradation. Regularly update the model with new data and retrain it to maintain accuracy. Scalability: Design the solution to scale horizontally to accommodate an increasing volume of data. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA. Solution Architecture Diagram Data Collection and Integration: Data Sources: Financial records, legal databases, directorial profiles, market data. Integration Layer: ETL processes, SQL/NoSQL databases. Feature Engineering: Feature Selection and Engineering Module. Machine Learning Models: Model Training Module: Scikit-Learn, TensorFlow, or PyTorch. Model Evaluation Component. Dynamic Risk Assessment: Real-Time Data Integration Component: Apache Kafka. Scenario Analysis Module. Fairness and Transparency: Fairness Metrics Integration. Explainability Module: SHAP or Lime. Model Integration and Deployment: API Layer: RESTful API. User Interface (UI). Documentation for Integration. Monitoring and Maintenance: Monitoring Dashboard: Prometheus, Grafana. Automated Model Update Pipeline: CI/CD. General Documentation: Model Architecture Document. Technical User Manual. Compliance Documentation: Regulatory Compliance Report. Data Privacy and Security Documentation. Post-Implementation Support: Support and Maintenance Plan. Training and Knowledge Transfer: Training Sessions. Knowledge Transfer Documentation. Scalability and Future-Proofing: Scalable Infrastructure. Flexibility for Future Enhancements. Tools & Technology Used By Blackcoffer Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: Data Collection and Preprocessing: Python: A versatile programming language commonly used for data manipulation and preprocessing. Pandas: A Python library for data manipulation and analysis, useful for handling structured data. NumPy: A library for numerical operations in Python, often used for efficient array operations. SQL/NoSQL Databases: To store and retrieve structured and unstructured data efficiently. Feature Engineering: Scikit-Learn: A machine learning library in Python that includes tools for feature extraction and preprocessing. NLTK (Natural Language Toolkit): For processing and analyzing textual data, particularly for sentiment analysis. Machine Learning Models: Scikit-Learn: Provides various machine learning algorithms for classification tasks, including Random Forests and Gradient Boosting. XGBoost or LightGBM: Powerful gradient boosting frameworks for improved predictive performance. TensorFlow or PyTorch: Deep learning frameworks for building and training neural networks if the complexity of the model demands it. Dynamic Risk Assessment: Apache Kafka or RabbitMQ: Message brokers to facilitate real-time data streaming and updates. Airflow: A platform to programmatically author, schedule, and monitor workflows, useful for scheduling model updates. Fairness and Transparency: Aequitas or Fairness Indicators: Libraries for assessing and mitigating bias in machine learning models. SHAP (SHapley Additive exPlanations): An algorithm for model interpretability. Model Integration and Deployment: Flask or Django: Web frameworks for building the model deployment API. Docker: Containerization tool for packaging the model and its dependencies. Kubernetes: Container orchestration for deploying and managing containerized applications at scale. RESTful API: For communication between the model and other components in the insurance company’s infrastructure. Monitoring and Maintenance: Prometheus: An open-source monitoring and alerting toolkit. Grafana: A platform for monitoring and observability with beautiful, customizable dashboards. Jenkins or GitLab CI/CD: Continuous integration and continuous deployment tools for automating model updates and deployment. MLflow: An open-source platform to manage the end-to-end machine learning lifecycle. General Development Environment: Jupyter Notebooks: Interactive computing environment for exploratory data analysis and model development. Git: Version control system for collaborative development. VS Code or PyCharm: Integrated development environments (IDEs) for coding and debugging. It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. Blackcoffer Deliverables The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1. Project Documentation: 1.1 Project Proposal: Clearly outlines the objectives, scope, and methodology of the premium prediction model. 1.2 Requirements Document: Specifies the functional and non-functional requirements of the model, considering the insurance company’s needs and regulatory compliance. 2. Data Collection and Preprocessing: 2.1 Data Collection Report: Details the sources and types of data gathered, including financial records, legal cases, and directorial profiles. 2.2 Cleaned and Preprocessed Dataset: A structured dataset ready for model training, containing relevant features and properly handled missing or inconsistent data. 3. Feature Engineering: 3.1 Feature Selection and Engineering Report: Documents the process of selecting and creating features, highlighting their relevance to the prediction task. 4. Machine Learning Models: 4.1 Trained ML Models: Includes the serialized models trained on historical data, such as Random Forests, Gradient Boosting, or other chosen algorithms. 4.2 Model Evaluation Report: Evaluates the performance of the models on validation and test datasets, including metrics like accuracy, precision, recall, and F1-score. 5. Dynamic Risk Assessment: 5.1 Real-Time Integration Component: Code or module that integrates real-time data for dynamic risk assessment. 5.2 Scenario Analysis Module: Component allowing the assessment of premium changes based on hypothetical scenarios. 6. Fairness and Transparency: 6.1 Fairness Assessment Report: Evaluates and mitigates bias, documenting fairness metrics and any adjustments made. 6.2 Explainability Module: Implementation of tools or methodologies for model interpretability and explanation. 7. Model Integration and Deployment: 7.1 Deployed API: RESTful API endpoint for seamless integration into the insurance company’s systems. 7.2 User Interface (UI): User-friendly interface for underwriters to interact with the model, providing insights and entering necessary information. 7.3 Documentation for Integration: Comprehensive guide on integrating the model into the existing workflow, including API documentation. 8. Monitoring and Maintenance: 8.1 Monitoring Dashboard: Visual representation of key metrics and alerts for model performance, developed using tools like Grafana. 8.2 Automated Model Update Pipeline: CI/CD pipeline or automated process for updating and retraining the model with new data. 9. General Documentation: 9.1 Model Architecture Document: Detailed explanation of the model’s architecture, including components and their interactions. 9.2 Technical User Manual: Documentation guiding technical users on deploying, maintaining, and troubleshooting the model. 10. Training and Knowledge Transfer: 10.1 Training Sessions: Conducted for the insurance company’s staff, including underwriters and IT personnel, to ensure effective use and understanding of the model. 10.2 Knowledge Transfer Documentation: Detailed documentation covering model usage, maintenance procedures, and troubleshooting tips. 11. Compliance Documentation: 11.1 Regulatory Compliance Report: Ensures that the model adheres to relevant insurance regulations in the USA. 11.2 Data Privacy and Security Documentation: Outlines measures taken to ensure the privacy and security of sensitive data. 12. Post-Implementation Support: 12.1 Support and Maintenance Plan: Document outlining the support and maintenance plan for the model post-implementation, including response times and escalation procedures. By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. Business Impacts The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: 1. Improved Accuracy and Risk Assessment: Impact: Enhanced accuracy in predicting premiums based on advanced data analysis and machine learning algorithms. Benefit: Better risk assessment leads to more precise premium calculations, reducing the likelihood of underpricing or overpricing policies. 2. Increased Competitiveness: Impact: Utilizing cutting-edge technology to provide more accurate and dynamic premium predictions. Benefit: Positions the insurance firm as a leader in the market, attracting more clients seeking innovative and reliable insurance solutions. 3. Tailored Coverage and Pricing: Impact: Customizing coverage and premiums based on individual directorial profiles and evolving risk factors. Benefit: Attracts clients with diverse risk profiles, offering tailored solutions that align with their specific needs. 4. Faster Decision-Making: Impact: Automation of premium calculations and decision-making processes. Benefit: Speeds up underwriting processes, enabling quicker responses to client inquiries and facilitating faster policy issuance. 5. Reduced Operational Costs: Impact: Automation of routine tasks related to premium calculation and risk assessment. Benefit: Decreases manual workload, leading to operational efficiency and cost savings. 6. Real-Time Adaptation to Market Changes: Impact: Integration of real-time data for dynamic risk assessment. Benefit: Enables the insurance firm to adapt quickly to changes in market conditions, ensuring that premiums remain reflective of current risk landscapes. 7. Enhanced Customer Satisfaction: Impact: Accurate pricing, fair premium calculations, and transparent communication. Benefit: Increases customer satisfaction by providing a reliable and customer-centric insurance experience. 8. Mitigation of Regulatory Risks: Impact: Implementation of a solution that complies with insurance regulations and industry standards. Benefit: Reduces the risk of regulatory non-compliance, protecting the firm from legal and financial repercussions. 9. Data-Driven Decision-Making: Impact: Utilizing data-driven insights for decision-making processes. Benefit: Empowers the firm’s leadership with actionable insights, contributing to strategic decision-making and business planning. 10. Brand Reputation and Trust: Impact: Adoption of fairness-aware and transparent AI models. Benefit: Builds trust among clients and stakeholders by demonstrating a commitment to fairness, transparency, and ethical AI practices. 11. Risk Mitigation for Clients: Impact: Providing insurance coverage that reflects the evolving nature of insider trading public lawsuits. Benefit: Assists Public Company Directors in mitigating financial risks associated with legal actions, enhancing the value proposition for clients. 12. Scalability and Future-Proofing: Impact: Designing the solution to scale and adapt to future industry developments. Benefit: Ensures the longevity and relevance of the insurance firm’s technology infrastructure in the face of evolving business and technological landscapes. 13. Revenue Growth: Impact: Attracting a larger customer base and retaining existing clients through innovative and accurate insurance solutions. Benefit: Contributes to revenue growth by expanding the firm’s market share and increasing customer loyalty. By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA. Summarize Summarized: https://blackcoffer.com/ This project was done by Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",28.724628449543594,-45,59.58718087995655,16,1,141,0.1024229074325865,2.9261271048343294,https://insights.blackcoffer.com/ml-and-ai-based-insurance-premium-model-to-predict-premium-to-be-charged-by-the-insurance-company/,1841
8.241379310344827,8.241379310344827,7.44743935309973,190,"effective management of social media data extraction: strategies for authentication, security, and reliability Client Background Client: A leading tech firm in the USA Industry Type: IT Products & Services: Consulting, Product & Services Organization Size: 100+ The Problem Handling complex authentication mechanisms for social media platforms. Efficiently extracting data from social media profiles. Preventing IP blocking and ensuring API reliability. Managing and storing extracted data securely. Abiding by social media platform policies and avoiding legal issues. Handling rate limiting and throttling. Providing comprehensive and up-to-date documentation. Dealing with changes in social media platform APIs. Optimizing API performance for rapid response. Ensuring user privacy and data protection. Our Solution Implement OAuth2 or API tokens for authentication. Utilize web scraping libraries like BeautifulSoup and Scrapy. Employ proxy rotation and request throttling. Use databases like MongoDB or AWS S3 for data storage. Regularly check and update API usage against platform policies. Implement rate limiting and queue-based processing. Maintain versioned API documentation. Monitor platform API changes and adapt accordingly. Optimize code and database queries for performance. Encrypt sensitive data and follow data protection regulations. Solution Architecture Authentication layer for social media logins. API endpoints for data extraction. Web scraping components for profile details. Throttling and rate-limiting mechanisms. Data storage and caching layers. Documentation portal for API users. Monitoring and logging infrastructure. Error handling and alerting mechanisms. Compliance checks and privacy safeguards. Load balancers and auto-scaling for API servers. Deliverables Project Github Source Code Tech Stack Tools used BeautifulSoup Requests Django rest Framework Language/techniques used Python Models used Django ORM Skills used Python WebScraping Python Django Python Django REST Framework Databases used SQLite Database Web Cloud Servers used None What are the technical Challenges Faced during Project Execution Frequent changes and updates to social media APIs. Evolving security and authentication requirements. Handling CAPTCHAs and bot detection mechanisms. Maintaining data consistency and accuracy. Adhering to rate limits and avoiding IP blocks. Scaling the infrastructure to accommodate increased usage. Dealing with diverse data formats from different platforms. Ensuring privacy and compliance with data protection laws. Balancing performance and cost-effectiveness. Handling user-specific customizations and options. How the Technical Challenges were Solved Regularly monitoring and adapting to API changes. Implementing robust authentication strategies. Using CAPTCHA solving services when necessary. Implementing data validation and cleansing routines. Employing IP rotation and rate limiting strategies. Utilizing cloud-based auto-scaling solutions. Developing data parsers for various formats. Implementing encryption and anonymization techniques. Profiling and optimizing code for performance. Providing configurable options for users to customize their data extraction. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",23.78172692629427,-6,51.21293800539084,5,1,13,0.05277777763117284,2.7601078167115904,https://insights.blackcoffer.com/effective-management-of-social-media-data-extraction-strategies-for-authentication-security-and-reliability/,371
7.827586206896552,7.827586206896552,7.309248554913295,173,"efficient aws infrastructure setup and management: addressing security, scalability, and compliance Client Background Client: A leading Consulting firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 1000+ The Problem Setting up and configuring AWS services. Designing an efficient database schema. Integrating email and calling services securely. Ensuring data privacy and compliance. Handling system scalability. Managing user authentication and authorization. Monitoring and logging system activities. Implementing backup and recovery strategies. Debugging and troubleshooting issues. Balancing cost and performance. Our Solution Utilize AWS CloudFormation or AWS CDK for infrastructure as code. Normalize the database schema to minimize redundancy. Implement OAuth or JWT for secure authentication. Encrypt data at rest and in transit. Use AWS Auto Scaling to handle increased traffic. Set up AWS CloudWatch for monitoring and AWS CloudTrail for auditing. Regularly backup data to Amazon S3. Implement comprehensive error handling and logs. Perform unit, integration, and load testing. Optimize AWS resource usage through cost analysis. Solution Architecture AWS RDS for customer and employee data storage. AWS Lambda functions for processing calls and emails. AWS SES and SNS for sending emails and notifications. Amazon S3 for storing backups and static assets. AWS Cognito for user authentication. AWS API Gateway for managing APIs. AWS CloudWatch and CloudTrail for monitoring and auditing. AWS Auto Scaling for handling variable workloads. Python codebase for application logic. Implementing security groups and VPC for network isolation. Deliverables Project Github Source Code Tech Stack Tools used Requests Boto3 Language/techniques used Python Models used None Skills used Python AWS Databases used AWS RDS Web Cloud Servers used Amazon Web Services (AWS) What are the technical Challenges Faced during Project Execution Integrating multiple AWS services. Designing a scalable database schema. Ensuring data security and compliance. Handling complex user authentication and authorization. Managing API versioning and changes. Optimizing cost and resource usage. Debugging and resolving performance issues. Maintaining high availability and reliability. Handling data synchronization between tiers. Adapting to evolving AWS services and best practices. How the Technical Challenges were Solved Extensive research and leveraging AWS documentation and support. Collaboration with experienced database architects. Thorough security audits and compliance checks. Implementing OAuth and fine-grained access control. Clear versioning and documentation for APIs. Regular cost analysis and optimization efforts. Profiling and performance tuning of critical components. Implementing redundancy and failover mechanisms. Developing data synchronization algorithms. Continuous learning and adaptation to AWS updates and community insights. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",23.131034482758622,-11,50.0,5,-1,10,0.06176470570069204,2.679190751445087,https://insights.blackcoffer.com/efficient-aws-infrastructure-setup-and-management-addressing-security-scalability-and-compliance/,346
12.866666666666667,12.866666666666667,7.253275109170306,205,"efficient data integration and user-friendly interface development: navigating challenges in web application deployment Client Background Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 100+ The Problem Data Complexity: Handling and integrating multiple data sources with different formats and cleaning/preprocessing them for use in a web application. Spatial Data Integration: Managing and converting complex spatial data into a suitable format for storage and display. User-Friendly Data Access: Providing an easy-to-use interface for users to query and visualize data efficiently. Secure Authentication: Implementing secure user authentication to protect sensitive data and user accounts. Deployment Considerations: Exploring the potential challenges of deploying the application on Azure. Our Solution Project Setup and ETL: Set up Django, developed ETL scripts, cleaned data, and loaded it into PostgreSQL. Web Application Development: Designed user-friendly templates, implemented APIs for data display, and used session storage for queries. User Authentication: Created login/signup pages and implemented secure user authentication. Data Management and Integration: Ensured dynamic tables and error handling for queries, created Docker image, and documented deployment. Spatial Data Handling: Processed and stored spatial data, integrated it with Django views, and converted data types. API Development: Built APIs for JSON data retrieval and handled various file extensions for data extraction. Frontend and User Interaction: Designed frontend components and implemented data upload and retrieval. SQL Dump and Azure Deployment: Created SQL Dump template, developed a view for uploading .sql files, and explored Azure deployment options. Solution Architecture Backend Framework: Python Django for building the web application’s backend. Database: PostgreSQL for storing cleaned and spatial data. ETL Processes: Python scripts for data extraction, transformation, and loading. Frontend: HTML templates and JavaScript for user interaction. APIs: Custom APIs for data retrieval and spatial data handling. Deployment: Dockerization for containerized deployment. Authentication: Implementing user authentication using Django’s built-in features. Spatial Data Handling: Using Python libraries to process and convert spatial data. SQL Dump: Creating an SQL Dump feature for running PostgreSQL queries. Deliverables Project Resouces well be access via github Only Github Link : https://github.com/AjayBidyarthy/Sheeban-Wasi-Full-stack.git Tech Stack Tools used Pillow psycopg2 arcgis==1.8.2 geopandas pyproj pandas numpy matplotlib pyshp Language/techniques used Python Models used Django ORM Skills used Python Django ETL Docker Databases used postgresql Web Cloud Servers used MS Azure What are the technical Challenges Faced during Project Execution Data Cleaning and Integration: Managing data from different sources and ensuring consistency was challenging. Spatial Data Transformation: Converting complex spatial data into suitable database formats posed a technical hurdle. User Authentication: Implementing secure user authentication without vulnerabilities required careful consideration. File Handling: Handling various file extensions and extracting data from them was a technical challenge. Deployment: Ensuring smooth deployment, especially on Azure, presented its own set of challenges. How the Technical Challenges were Solved Data Cleaning and Integration: Python scripts were used to clean and preprocess data, aligning it with column datatypes. Spatial Data Transformation: Libraries were utilized to process and convert spatial data to appropriate formats. User Authentication: Django’s built-in authentication features were leveraged for secure user management. File Handling: Custom Python scripts were developed to handle different file extensions and extract data. Deployment: Dockerization simplified deployment, and research on Azure ensured potential future deployment options were explored. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",23.050596797671034,-10,44.75982532751092,11,1,23,0.07534246558140989,2.593886462882096,https://insights.blackcoffer.com/efficient-data-integration-and-user-friendly-interface-development-navigating-challenges-in-web-application-deployment/,458
12.774193548387096,12.774193548387096,7.202846975088968,117,"automated orthopedic case report generation: harnessing web scraping and ai integration Client Background Client: A leading health-tech firm in the USA Industry Type: Healthcare Products & Services: Medical solutions, healthcare Organization Size: 100+ The Problem The problem is to efficiently create orthopedic case reports by extracting data from online sources, including articles, videos, and user comments. It involves summarizing and citing relevant articles from PubMed.gov for the past 5 years related to the case. This requires automating the extraction and summarization of data from websites, making it a time-consuming task if done manually. Our Solution Develops a Python tool that accepts a website URL as input and generates a case report. Integrates web scraping to extract data from websites. Utilizes AI, such as ChatGPT, for creating summaries and responses. Leverages PubMed for citing and summarizing recent articles. Provides a web application for user-friendly access to these capabilities. Solution Architecture Utilizes web scraping techniques to gather data from trusted medical websites. Combines web scraping with AI, including ChatGPT, for generating case reports and responding to queries. Utilizes PubMed for retrieving and summarizing recent articles related to the case. Deploys a web application for user interaction and input. Deliverables Project Github Source Code Tech Stack Tools used ChatGPT BeautifulSoup Requests Language/techniques used Python Models used None Skills used Python WebScraping ChatGPT prompting Databases used None Web Cloud Servers used None What are the technical Challenges Faced during Project Execution Accurate and reliable web scraping from diverse medical websites. Integration of AI components for text generation and summarization. Efficient querying and retrieval of articles from PubMed. Handling different data formats and structures from various online sources. Developing a user-friendly web interface for input and interaction. How the Technical Challenges were Solved Extensive research and testing of web scraping techniques for medical websites. Integration of AI models and libraries for text generation. Utilization of PubMed API for article retrieval and summarization. Custom data parsers for handling diverse data structures. Collaboration with medical experts for user interface design and feedback. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",21.764481689817472,-4,41.637010676156585,5,1,8,0.045283018697045214,2.5587188612099645,https://insights.blackcoffer.com/automated-orthopedic-case-report-generation-harnessing-web-scraping-and-ai-integration/,281
8.672413793103448,8.672413793103448,7.7686170212765955,182,"streamlined integration: interactive brokers api with python for desktop trading application Client Background Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+ The Problem Integrating the Interactive Brokers API with Python. Creating a user-friendly desktop application interface. Managing concurrent processes and threads. Developing the margin calculator with accurate calculations. Handling data synchronization between TWS and the application. Ensuring security and authentication for TWS access. Providing real-time market data to users. Maintaining a responsive and reliable application. Resolving any potential compatibility issues. Ensuring thorough documentation for users Our Solution Leverage Interactive Brokers API documentation and libraries. Design an intuitive and responsive PyQT5-based desktop UI. Implement threading and preprocessing for concurrent tasks. Develop a robust margin calculator algorithm. Use data synchronization mechanisms provided by TWS. Implement secure authentication for TWS access. Utilize the Interactive Brokers API for real-time market data. Conduct extensive testing and quality assurance. Address compatibility issues through rigorous testing. Document every aspect of the project for users and developers. Solution Architecture Interactive Brokers API for live data and trading access. Python-based server using Django for APIs and data storage. PyQT5-based desktop application for trading dashboard. PostgreSQL database for storing relevant data. Threading and concurrency management for parallel processes. Margin calculator component within the desktop app. Integration with Trader Workstation (TWS). Real-time market data feeds from TWS. Responsive front-end using Bootstrap, HTML, and CSS. Detailed documentation for users and developers. Deliverables Project Github Source Code : https://github.com/AjayBidyarthy/Sunil-Misir Tech Stack Tools used Requests Threading and Multiprocessing PyQT5 Language/techniques used Python Models used Django ORM Skills used Python Python Django Python Django REST Framework PyQT5 MultiThreading and MultiProcessing Databases used POstgresql Web Cloud Servers used None What are the technical Challenges Faced during Project Execution Complex integration with the Interactive Brokers API. Designing an efficient and user-friendly desktop interface. Coordinating and managing multiple concurrent threads and processes. Accurate implementation of the margin calculator. Ensuring real-time data synchronization with TWS. Handling authentication and security for TWS access. Providing timely and reliable market data. Resolving compatibility issues on various user machines. Optimizing performance for a responsive application. Documenting every aspect comprehensively. How the Technical Challenges were Solved Extensive research and consultation of Interactive Brokers API documentation. User-centered design principles for the desktop interface. Thorough testing and debugging of multi-threading scenarios. Careful design and testing of margin calculation algorithms. Regular data synchronization checks with TWS. Implementation of secure authentication protocols. Utilization of Interactive Brokers’ data streaming features. Compatibility testing on various configurations. Profiling and optimization of code for responsiveness. Comprehensive documentation created throughout the development process. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",22.830667644900956,-6,48.40425531914894,3,1,21,0.07458563515307835,2.8111702127659575,https://insights.blackcoffer.com/streamlined-integration-interactive-brokers-api-with-python-for-desktop-trading-application/,376
15.805555555555555,15.805555555555555,7.126865671641791,167,"streamlined trading operations interface for metatrader 4: empowering efficient management and monitoring Client Background Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Investment, Financing Organization Size: 100+ The Problem Trading Operations Interface: The project aims to create a Windows-based Display Application that provides an intuitive interface for managing trading activities in MetaTrader 4 (MT4). EA Control and Monitoring: Users need a tool to interact with and monitor the EA running in MT4, which follows predefined rules for trading. Hedging and Configuration: The application should allow users to hedge positions, configure trading settings, close orders manually, and add new orders. Real-time Monitoring: Real-time monitoring and control of trading activities are crucial for efficient trading. EA Functionality: The specific functionality and rules of the EA will be defined based on pricing and requirements. Our Solution Display Application: Developed a Windows-based application for trading operations management, order placement, and monitoring. EA Interaction: Created a loosely coupled system where the Display Application can control and influence the EA running on MT4. Functionality: Implemented order placement, hedging, settings configuration, order closing, and real-time monitoring features. Dynamic EA: The EA’s specific rules and functionality are determined based on pricing and requirements. Communication: Established a mechanism for the Display Application to communicate with MT4, facilitating trading instructions and updates. Solution Architecture UI Development: UI development using Python libraries such as Kivy and Tkinter for the Windows-based application. VPS with MT4: The client operates a Virtual Private Server (VPS) with MT4 running. Expert Advisor (EA): An EA on MT4 executes trading operations based on predefined rules. Communication: A mechanism for the Display Application to communicate with MT4, possibly through an API or other methods. Dynamic EA Parameters: The exact rules and functionality of the EA will be determined based on pricing and client requirements. Deliverables Project Code can be accessed via this github link : https://github.com/AjayBidyarthy/Patrick-Oliveri-Applcation.git Since, this is private Git Reporsitory , User will need permission to clone it. Tech Stack Tools used TKinter Kivy Language/techniques used Python Models used No Model Used Skills used Python Kivy Python TKinter Databases used No Db Used Web Cloud Servers used No Web Services Used What are the technical Challenges Faced during Project Execution UI Responsiveness: Challenges in achieving responsive UI in Python libraries like Kivy and Tkinter. Integration with MT4: Ensuring effective communication between the Display Application and MT4. Dynamic EA Rules: Defining and integrating dynamic rules for the EA based on user requirements. Deployment: Preparing for potential deployment but no deployment has occurred yet. Version Control: Managing code changes and documentation using Git. How the Technical Challenges were Solved UI Responsiveness: The project transitioned to seek C or C# development for better responsiveness and flexibility. Integration with MT4: A communication mechanism, possibly an API, was explored to facilitate communication between the Display Application and MT4. Dynamic EA Rules: The exact rules for the EA were to be determined based on client requirements and pricing, ensuring flexibility. Deployment: Deployment has not occurred yet, and it may be addressed in the future. Version Control: Git was used to manage code changes and documentation, ensuring version control and collaboration. Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",22.939137645107795,-3,41.54228855721393,5,1,17,0.052356020805350734,2.554726368159204,https://insights.blackcoffer.com/streamlined-trading-operations-interface-for-metatrader-4-empowering-efficient-management-and-monitoring/,402
14.055555555555555,14.055555555555555,6.756172839506172,139,"streamlining time calculation in warehouse management: leveraging shiphero api and google bigquery integration Client Background Client: A leading retail firm in the USA Industry Type: Retail Products & Services: Retail Solutions, Supply Chain, Warehouse Management Organization Size: 100+ The Problem The problem was to efficiently calculate the time taken by each personnel on their shifts in a warehouse management system. Data needed to be extracted from ShipHero API and processed to generate meaningful insights. There was a need for a web interface to provide user-friendly access to the data and allow for data filtering. There is a mapping issue in Python Script which occurred in December of 2022. Maybe due to the addition of another warehouse. This is an open issue and ShipHero is unable to provide any reliable solution for the same. [Issue has been highlighted below in the section of ‘’Known Issues’’] Our Solution Creating an API to Google BigQuery using a Python script deployed on Google Cloud. The Python script automated data extraction from ShipHero API, transformation, and loading into Google BigQuery. Google Data Studio was used to create a dashboard for reporting and visualization. Solution Architecture The solution involved two main components: a Python script and a web interface (Web App). The Python script utilized ShipHero API to fetch data and calculate personnel shift times. It then stored the processed data in Google BigQuery. The web interface allowed users to log in, apply filters to data tables fetched from BigQuery, and visualize the data. Google Cloud services were used for hosting the Python script and deploying the web app. Deliverables [GitHub Repositories URL: https://github.com/AjayBidyarthy/Jake-Brenner-API-to-google-big-query-to-google-data-studio . https://github.com/AjayBidyarthy/Jake-Brenner-frontend/tree/himanshu https://github.com/AjayBidyarthy/Jake-Brenner-frontend/tree/master Tech Stack Tools used Google API Beautifulsoup Numpy and Pandas Language/techniques used Python React JS Models used Django ORM Model Skills used Python Python Django React JS Databases used GCP BigQuery Database Web Cloud Servers used Google Cloud Platform (GCP) What are the technical Challenges Faced during Project Execution Accessing and understanding ShipHero API endpoints and data structures. Developing and deploying the Python script to run daily on Google Cloud Scheduler. Integrating and linking databases effectively. Handling and automating complex data manipulation and calculations. How the Technical Challenges were Solved Comprehensive research and analysis of the ShipHero API and its endpoints. The Python script was developed to handle data extraction, transformation, and loading tasks efficiently. Google Cloud services were used to automate the script and schedule daily runs. Collaboration and communication with the client to ensure the API data met the dashboard requirements. Project website url http://app.shiphero.com/ Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",22.782716049382717,-15,42.901234567901234,6,-1,8,0.07491856653120989,2.4382716049382718,https://insights.blackcoffer.com/streamlining-time-calculation-in-warehouse-management-leveraging-shiphero-api-and-google-bigquery-integration/,324
9.942857142857143,9.942857142857143,7.098540145985401,118,"streamlined equity waterfall calculation and deal management system Client Background Client: A leading real estate firm in the USA Industry Type: Real Estate Products & Services: Real Estate, Construction, Financing Organization Size: 100+ The Problem Calculating equity waterfalls based on CSV data. Implementing different user roles and their permissions. Creating a user-friendly dashboard for each user type. Managing deal creation, invitations, and subscriptions. Handling user invitations and registration. Copying deals while preserving specific data. Our Solution Develop Python code to calculate equity waterfalls. Implement role-based access control for admin, sponsors, and investors. Create distinct dashboards with relevant data using ReactJS. Design intuitive UI for deal management. Develop invitation mechanisms and registration flows. Implement copying deals with proper data handling. Solution Architecture Backend built with Django for handling data, authentication, and API endpoints. Frontend developed using ReactJS for user interfaces. SQLite database for data storage. Google Cloud for application deployment. Deliverables Project Github Source Code : Tech Stack Tools used Pillow Requests GCP VM Language/techniques used Python React JS Models used Django ORM Skills used Python Python Django Python Django REST Framework Databases used SQLite3 database Web Cloud Servers used GCP What are the technical Challenges Faced during Project Execution Equity waterfall calculations based on dynamic CSV data. Managing user permissions and access control. Designing and implementing complex user registration and invitation flows. Copying deals while maintaining data integrity. Ensuring data consistency and security. How the Technical Challenges were Solved Developed Python scripts to parse CSV files and perform required calculations. Utilized Django’s built-in authentication system and implemented role-based permissions. Designed clear and user-friendly registration and invitation processes. Implemented a controlled deal copying mechanism. Conducted thorough testing and used encryption for data security. Project website url : https://stackshares.io/ Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",21.20342022940563,-4,43.06569343065693,4,1,8,0.04615384597633136,2.4817518248175183,https://insights.blackcoffer.com/streamlined-equity-waterfall-calculation-and-deal-management-system/,274
18.842105263157894,18.842105263157894,7.58252427184466,265,"efficient database design and management: streamlining access and integration for partner entity management Client Background Client: A leading IT firm in the Europe Industry Type: IT Products & Services: IT Services, Consulting and Automation Organization Size: 100+ The Problem Database designing which enables access to each related/important table data via other db table The project required the development of a user-friendly web application for managing partner entities with diverse attributes. Ensuring data accuracy, security, scalability, and compliance with regulations while integrating seamlessly with a Data Warehouse posed significant technical challenges. Our Solution Our solution successfully addressed the technical challenges by leveraging Django’s capabilities and implementing custom solutions where needed. It provided a robust and scalable web application for partner entity management while ensuring data accuracy, security, and compliance. The dynamic attribute management system and integration with the Database facilitated efficient data handling and reporting, supporting data-driven decision-making. We have designed and implemented database related changes and UI related changes that Client have suggested according to which a separate db table which contains all data which will be created , updated and deleted as other table rows created, updated and deleted Solution Architecture Django ORM for abstracting database complexities. Scalability through cloud resources and optimization techniques. Security measures, including encryption and access controls for Admin Users. Performance optimization strategies such as removing redundancy in db tables. We have provided many database design solutions as well as User Interface solution regarding which client have given positive response. We have successfully developed and implemented design and changes related to project after multiple discussion with client regarding database architecture design as well as database model and their related UI panel with authentication Deliverables Python Django Source Code (Github Repository) Tech Stack Tools used Python Django web Framework Language/techniques used Python Models used Django Database Model and Django ORM Skills used Python Django Databases used Postgresql Web Cloud Servers used Not Used from Side What are the technical Challenges Faced during Project Execution Database Complexity : Designing a comprehensive database schema to represent multiple partner entities with varying attributes posed a challenge. Each entity had unique characteristics and relationships. Scalability : Ensuring the application’s scalability to handle a potentially large volume of partner data while maintaining performance was a significant concern. Dynamic Attributes : Allowing users to dynamically manage entity attributes presented difficulties in database design and user interface implementation. Data Validation : Implementing robust data validation rules to maintain data accuracy and consistency across various partner entities was complex due to the diversity of data. Integration with Remote Database : Establishing seamless data export capabilities to feed the Database while maintaining data compatibility was a technical hurdle. Security : Ensuring data security and compliance with relevant regulations, including encryption and access control, required careful consideration and implementation. Performance Optimization : Optimizing the application’s performance, especially when dealing with complex queries and large datasets, was a continual challenge. How the Technical Challenges were Solved Database Abstraction : Utilizing Django’s ORM (Object-Relational Mapping) allowed for an abstract representation of entities and their attributes, simplifying database management. Scalability Planning : Employing efficient indexing and caching mechanisms to accommodate scalability and performance needs. Additionally, using cloud resources for scalability. User Management : Implementing a flexible User management system that allowed users to Create , Read , Update and Delete other Users and their related permissions. Data Validation Middleware : Developing custom middleware to enforce data validation rules and ensure data accuracy before database interactions. Integration Layer : Creating a dedicated integration layer that transformed and exported data from the database to the User Interface, adhering to data compatibility standards. Security Best Practices : Adhering to best practices for securing data, including User Authentication, Changes in Django template to Remove Other Important Database in db options and Permission Required for other User to use database table on UI panel from Admin User. Performance Tuning : Conducting performance tuning by optimizing database model and related admin file for better fetching of db table data on UI panel. Project website url http://34.18.45.30:8000/api/admin/login/?next=/api/admin/ Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",28.119366377107816,-9,51.45631067961165,14,1,26,0.07014028042056056,2.7844660194174757,https://insights.blackcoffer.com/efficient-database-design-and-management-streamlining-access-and-integration-for-partner-entity-management/,515
17.829787234042552,17.829787234042552,7.057142857142857,215,"text summarizing tool to scrape and summarize pubmed medical papers Client Background Client: A leading medical R&D firm in the USA Industry Type: Medical Products & Services: R&D Organization Size: 10000+ The Problem An advanced AI tool designed specifically for doctors to assist them in retrieving answers to their queries. Powered by state-of-the-art AI technologies, including web scraping and ChatGPT, The AI Assistant aims to streamline information retrieval and provide valuable insights to professionals. This AI Assistant leverages the capabilities of AI to facilitate seamless and efficient access to knowledge and information. It combines web scraping techniques to gather relevant data from trusted sources with ChatGPT and PubMed, providing accurate responses to doctors’ queries. Query Retrieval: AI Assistant utilizes web scraping techniques to fetch information from credible websites, academic journals, medical databases, and other trusted sources. It provides doctors with immediate access to a vast array of knowledge and resources. Benefits: Time Efficiency: By quickly retrieving information and answering queries, AI Assistant saves valuable time for doctors, allowing them to focus more on patient care and critical tasks. Access to Knowledge: AI Assistant grants doctors easy access to a vast repository of knowledge, ensuring they stay updated with the latest research, treatment guidelines, and best practices. Decision Support: The tool provides valuable insights and recommendations, assisting doctors in making informed decisions about diagnosis, treatment plans, and patient management. Our Solution To address this problem, we will build a web scraping tool that uses Python libraries such as BeautifulSoup, Selenium, and OpenAI’s GPT-3. The program will work as follows: A user inputs the URL of the case report they want to extract data from. The program sends a GET request to the webpage and parses the HTML content using BeautifulSoup. The program then identifies the relevant sections of the webpage (such as the title, introduction, report, conclusion, and keywords) and extracts the text content. For each reference linked in the case report, the program sends a GET request to the reference’s webpage and parses the HTML content. The program then sends a prompt to the GPT-3 model, asking it to summarize the content of the reference, and receives a summarized response. The program collects all the summarized references and adds them to the case report. The program also identifies any images associated with the case report and downloads them. Finally, the program creates a Word document and adds all the collected information (including the summarized references and downloaded images) to the document. Solution Architecture Deliverables A fully functional web scraping tool that can extract data from a given webpage and generate a case report. A detailed documentation explaining how to use the tool and what kind of data it can extract. Tech Stack Tools used Python BeautifulSoup Selenium OpenAI’s GPT-3 Language/techniques used Python Models used OpenAI’s GPT-3 Skills used Web Scraping Natural Language Processing Machine Learning What are the technical Challenges Faced during Project Execution Handling dynamic websites that load content via JavaScript. Managing rate limits and CAPTCHAs imposed by the target websites. Ensuring the accuracy and relevance of the summarized content generated by the GPT-3 model. How the Technical Challenges were Solved Using Selenium to interact with the JavaScript-rendered content of the target websites. Implementing strategies to bypass rate limits and CAPTCHAs. Fine-tuning the parameters of the GPT-3 model to improve the quality of the summarized content. Business Impact The implementation of our web scraping and summarization tool has had significant positive impacts on our business operations. Firstly, it has streamlined our research process by automating the extraction of crucial information from various online sources. This has saved us considerable time and effort, allowing us to focus on more complex tasks. Secondly, the summarization feature has improved our understanding of the information we collect. By reducing large volumes of text down to a few key points, we’ve been able to quickly grasp the main ideas and insights presented in the articles, videos, and user comments. Thirdly, the tool has enabled us to stay up-to-date with the latest advancements in the field of orthopedics. By pulling data from recent articles on PubMed.gov, we’ve been able to stay informed about the latest research and treatments. Finally, the tool has facilitated the creation of comprehensive case reports. These reports have been instrumental in our ability to present detailed and accurate information to our clients, thereby enhancing our reputation and credibility in the industry. Overall, the implementation of this tool has greatly improved our efficiency and effectiveness, contributing significantly to our business success Project Snapshots Project Video Link: https://www.loom.com/share/535828aad7184c1b82db707dcca8e52c?sid=c79d19b1-b963-45a1-bec5-6228cc753cc2 Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",23.512867274569402,-6,40.95238095238095,28,1,35,0.08506224048742274,2.4838095238095237,https://insights.blackcoffer.com/text-summarizing-tool-to-scrape-and-summarize-pubmed-medical-papers/,525
18.418181818181818,18.418181818181818,7.178743961352657,302,"end-to-end tool to optimize routing and planning of field engineers using google’s cvrp-tw algorithm Client Background Client: A leading hardware firm in the USA Industry Type: IT Products & Services: IT Consulting, Support, Hardware Installations Organization Size: 300+ The Problem The client specializes in installing blinds and related products in customers’ homes. They are currently struggling with scheduling appointments efficiently due to a variety of factors such as location, installation duration, team member availability, and customer preferences. We need a tool that can suggest optimal schedules based on these criteria and adapt to changes as customers approve or reject proposed appointment times. The goal is to create a proof of concept for a route and job planning model that can potentially streamline our scheduling process and make a significant impact on our business operations. Our Solution The To address this challenge, we propose developing a proof of concept for a route and job planning model. This model will be based on the concept of Constrained Vehicle Routing Problem with Time Windows (CVRP-TW), a well-established approach in operations research and logistics. The model will take a dataset, which could be extracted from a Google sheet or converted from a CSV file, and generate optimal schedules. The development process will involve several stages: Understanding the data: We’ll analyze the data to identify the relevant variables and constraints. These may include the locations of installations, the duration of installations, the availability of team members, and customer preferences. Defining the objective and constraints: The objective will be to minimize the total travel time or maximize the number of installations completed within a given time frame. The constraints will include the geographical distances between locations, the working hours of team members, and the specific requirements of each installation. Implementing the algorithm: We’ll use an optimization algorithm, such as the Traveling Salesman Problem (TSP) solver, to find the optimal routes. The algorithm will consider all possible routes and choose the one that best meets the objectives while adhering to the constraints. Running simulations: To ensure the feasibility of the model, we’ll run simulations using different scenarios and adjust the parameters as needed. Saving the output: The final output will be the suggested schedules, which can then be reviewed and approved by the relevant parties. In terms of technology, we’ll use Python, a popular language for data analysis and machine learning. We’ll also use the Anaconda distribution, which provides a powerful environment for scientific computing and data analysis. Solution Architecture Deliverables A Python script implementing the CVRP-TW model. Test data and scripts for simulating different scenarios. Documentation explaining how to use the model and interpret the results. Tech Stack Tools used Python: The primary programming language. Anaconda: The Python distribution used for data analysis and machine learning. Visual Studio Code: The code editor used during development. Google App Script for deployment integrated with Google Sheets Language/techniques used Python Models used Constrained Vehicle Routing Problem with Time Windows (CVRP-TW) Skills used Data Analysis Machine Learning Optimization Algorithms Python Programming Databases used CSV, Google Sheets: The data will initially be stored in a CSV file, which can be easily imported into Python using libraries like pandas. What are the technical Challenges Faced during Project Execution One of the main challenges we anticipated is dealing with the complexity and variability of the data. The locations of installations, the duration of installations, the availability of team members, and customer preferences all need to be taken into account, and these factors can vary widely. Additionally, the model needs to be flexible enough to adapt to changes in the criteria as customers approve or reject appointment times. How the Technical Challenges were Solved To overcome these challenges, we used advanced data analysis techniques to extract meaningful insights from the data. We’ll also develop a flexible model that can handle changes in the criteria. Furthermore, we’ll thoroughly test the model under different scenarios to ensure its robustness and reliability. Business Impact Implementing an efficient route and job planning model had a significant positive impact on our business operations. By automating the scheduling process, we were able to reduce manual errors and streamline our workflow, resulting in quicker response times and deliveries. This not only improved our operational efficiency but also enhanced our ability to provide better service to our customers. Moreover, the model allowed us to maximize each driver’s productivity by optimizing routes, which led to cost savings in fuel and vehicle maintenance. The automated nature of the system also enabled us to make real-time adjustments to the route in response to last-minute orders or unexpected situations, such as a driver being unavailable. The model also provided us with valuable insights into our operations, allowing us to identify bottlenecks and areas for improvement. This helped us to proactively address potential issues and continuously enhance our processes, thereby increasing our overall business performance. As a result of these improvements, we were able to attract more skilled workers by focusing on cutting down unskilled labor. This shift towards more automation allowed us to invest more in our workforce, leading to higher employee satisfaction and retention rates. Lastly, the successful implementation of the route and job planning model has opened up new opportunities for our business. With the ability to efficiently cover our market and manage our resources effectively, we have been able to consider expanding our territory by entering new markets. This strategic route planning has helped us determine whether we need to acquire more vehicles or hire more operators before moving, providing a clear pathway for future growth. Project Snapshots Project website url https://docs.google.com/spreadsheets/d/1kS7Em9NitvMD_49MoLCpt_KoPJGGIAGjCES_KI8rEQk/edit?userstoinvite=raymondchow%40stanbondsa.com.au#gid=766964619 Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",26.819768701507833,-12,48.631239935587764,37,1,42,0.09782608677930056,2.5877616747181964,https://insights.blackcoffer.com/end-to-end-tool-to-optimize-routing-and-planning-of-field-engineers-using-googles-cvrp-tw-algorithm/,621
19.452380952380953,19.452380952380953,7.088291746641075,257,"data engineering and management tool (airbyte) with custom data connectors to manage crm database Client Background Client: A leading tech firm in Europe Industry Type: IT Products & Services: IT & Consulting, Software Development Organization Size: 1000+ The Problem Our company requires a robust, scalable, and secure data integration solution that can handle thousands of connections. We need to develop Airbyte connectors for various software applications listed in 2-nx-integration, including Join Portal, ClickUp, Coach Accountable, Hubspot, Quickbooks, Quickbooks Time, and Sales Flow. These connectors should be developed in Python and then wrapped into Docker images. The code should be housed in GitHub and automatically applied to Airbyte for execution using a CI/CD pipeline from GitHub to Airbyte. We also need a full production-ready version of Airbyte hosted on Google Cloud Platform (GCP) Kubernetes, secured via Google Sign In. Moreover, we want to add custom features to Airbyte to control BigQuery projects/datasets. Both Airbyte and BigQuery should be monitored via Sentry, which will also be housed/hosted in the same project for all error reporting/monitoring. We also need to develop transformations to clean and transform the data from the software source to the client’s GCP Project for BigQuery. The code for these transformations should be stored in GitHub. Our Solution We propose to develop an instance of Airbyte that is production-ready on GCP over Kubernetes. This will be secured using Google Sign On linked to our organization. We will deploy Airbyte using the official documentation 8. To secure the Kubernetes setup, we plan to use Traefik’s ForwardAuth feature. Next, we will code Airbyte Python integrations for our needed software list. We have already gathered the API documentation for each software application and have started coding the integrations. Once the initial integration is complete, we will document the process in ClickUp to guide future integrations. We will use GitHub to host both the source code and Docker images of Airbyte integrations. We will also use Google Cloud’s Sentry for error reporting and monitoring. Solution Architecture Deliverables Production-ready Airbyte instance on GCP Kubernetes Secured Airbyte instance using Google Sign On Developed Airbyte Python integrations for required software Error reporting and monitoring setup with Sentry Documentation of integration process in ClickUp Tech Stack Tools used Airbyte Docker GitHub Google Cloud Platform Google Sign In Traefik Sentry Language/techniques used Python Models used Airbyte ETL Skills used Web Scraping Database Management API Connectors Databases used Google BigQuery What are the technical Challenges Faced during Project Execution One of the main challenges we anticipate is managing the scalability of the system to handle thousands of connections. Another challenge could be securing the system effectively while ensuring smooth operation. How the Technical Challenges were Solved To address the scalability issue, we will leverage the inherent scalability of Kubernetes and BigQuery. Kubernetes allows us to easily scale our services based on demand, while BigQuery is designed to handle large datasets and high query loads. To ensure effective security, we will use Google Sign In for user authentication, and we will follow best practices for securing our Docker containers and GCP environment. Regular audits and penetration testing will also be conducted to identify and rectify any potential security vulnerabilities. Business Impact By developing a robust and scalable data integration solution using Airbyte, we aim to significantly enhance our business operations. This solution will enable us to efficiently manage and analyze data from various software applications, leading to improved decision-making processes. Firstly, the ability to extract and load data from different software applications will allow us to centralize our data management, reducing the complexity of handling multiple data sources. This will streamline our data analysis processes and provide a unified view of our business data. Secondly, the scalability of our solution means that it can handle a growing volume of data as our business grows. This is crucial in today’s digital age where businesses generate vast amounts of data daily. Lastly, by securing our data integration solution with Google Sign In, we can ensure that only authorized individuals can access our sensitive business data. This adds an extra layer of security to our data management practices and helps protect against potential data breaches. Moreover, by using Google Cloud Platform (GCP) for hosting our solution, we can take advantage of its advanced features and robust infrastructure. This will further enhance the reliability and performance of our data integration solution. Overall, implementing this solution will enable us to harness the power of data to drive our business growth and success Project Snapshots Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",27.51223836943607,-9,49.32821497120921,43,1,23,0.06751054838077944,2.63531669865643,https://insights.blackcoffer.com/data-engineering-and-management-tool-airbyte-with-custom-data-connectors-to-manage-crm-database/,521
22.107142857142858,22.107142857142858,6.784119106699752,181,"grafana dashboard to visualize and analyze sensors’ data Client Background Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT & Consulting, Software Development, DevOps Organization Size: 100+ The Problem The client requires a Grafana dashboard that can fetch data from a web API providing historical data of building automation systems. The dashboard needs to allow manual entry of a target URL for individual buildings, selection of a history name from a dropdown or search bar, selectable time range for displaying history data, and the ability to choose from various chart types for visualization. Additionally, the client wants to set up alarms for certain metrics like CPU, RAM, and hard disk usage. Each user should only be able to view their own STier API data, which is controlled by their IP. Our Solution To meet these requirements, we will set up a Grafana dashboard using the Grafana API. We will configure the dashboard to connect to the web API and fetch data based on the user’s input for the target URL, history name, and time range. For visualization, we will implement various chart types including Bar, Line, and Scatter plot charts. To set up alarms for specific metrics, we will utilize Grafana’s built-in alerting feature. Solution Architecture Deliverables A fully functional Grafana dashboard connected to the web API Ability to manually enter a target URL for individual buildings Selection of history name from a dropdown or search bar Selection of time range for displaying history data Various chart types for data visualization Setup of alarms for specific metrics Tech Stack Tools used Python Grafana Grafana API Web API for historical data of building automation systems Language/techniques used Javascript SQL Skills used Data Visualization API Integration User Interface Design Databases used Grafana Database What are the technical Challenges Faced during Project Execution Implementing user permissions for individual users Setting up alarms for specific metrics How the Technical Challenges were Solved For connecting Grafana to the web API, we used the Grafana API and configured it to fetch data from the web API based on user input. To implement user permissions, we used Grafana’s built-in user management feature and set up roles and permissions accordingly. For setting up alarms, we leveraged Grafana’s built-in alerting feature and configured it to trigger alerts based on specific conditions. Business Impact The proposed Grafana dashboard will significantly enhance the business’s ability to monitor and manage building automation systems. By providing real-time data visualization and the ability to set alarms for specific metrics, the business can quickly identify and address potential issues, ensuring optimal system performance and efficiency. Furthermore, the user-specific permissions will ensure that sensitive data remains secure and accessible only to authorized individuals. This will not only streamline operations but also boost confidence among staff members who can now make informed decisions based on accurate and timely data. The dashboard’s flexibility in terms of selectable history names and time ranges will allow for comprehensive analysis of historical data, leading to improved decision-making processes. Overall, this solution will contribute to increased operational efficiency, reduced downtime, and improved customer satisfaction by ensuring smooth operation of building automation systems. Project website url https://mailhvac.postman.co/workspace/Team-Workspace~902b44a6-966b-4e59-8400-3ae02c12ce6b/collection/17767455-eb2c775e-421d-4f7c-9ec5-b4f6a73f1a5a?action=share&creator=17767455 Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",26.80811768876285,-4,44.91315136476427,16,1,17,0.05555555540858319,2.531017369727047,https://insights.blackcoffer.com/grafana-dashboard-to-visualize-and-analyze-sensors-data/,403
12.689655172413794,12.689655172413794,6.907368421052632,207,"ai-driven data analysis ai tool using langchain for a leading real estate and financing firm worldwide Client Background Client: A leading real estate and financing firm worldwide Industry Type: Real Estate Products & Services: Infrastructure Development, Financing, Real Estate Organization Size: 10000+ The Problem Creating a user-friendly data analysis tool capable of interpreting natural language queries and providing insightful analyses from CSV data. The tool should facilitate seamless interaction, enabling users to gain valuable insights without the need for technical expertise. Key functionalities should include data exploration, trend identification, pattern recognition, and anomaly detection, all presented in a comprehensible format. The tool must also ensure efficient handling of CSV datasets while maintaining accuracy and reliability in its analyses. Our Solution Data Ingestion and Conversion: CSV data is acquired from a source (local file system, cloud storage, etc.). The data is then converted into a pandas DataFrame using the read_csv() function or similar methods provided by the pandas library. Data Cleaning: Data Cleaning operations are performed on the dataframe so that it serves as an ideal input for Pandas Agent. These may include: Column Data type conversion. Handling Duplicates Handling unnecessary columns, etc. Initialization of Langchain’s Pandas Agent: Langchain’s Pandas Agent is initialized with the necessary parameters. These parameters include: System prompt: A custom prompt provided by the user or defined in the application. Temperature: A parameter controlling the randomness of the model’s outputs. Model: The specific model or model configuration to be used by the agent. Other relevant parameters based on the requirements and capabilities of the agent. Integration with Pandas DataFrame: The DataFrame created in the previous step serves as input for the Pandas Agent. It contains the structured data which will serve as input for the Pandas Agent. Natural Language Query Interpretation: The user interacts with the system by posing queries in natural language. Langchain’s Pandas Agent interprets these queries using GPT-4 backend and converts them into executable commands or operations on the DataFrame. DataFrame Operations: The Pandas Agent executes the operations needed on the DataFrame. These operations may include: Filtering : Selecting rows or columns based on specified criteria. Aggregation : Computing summary statistics or aggregating data based on groups. Transformation : Modifying data in the DataFrame (e.g., adding or removing columns, changing data types). Joining/Merging : Combining multiple DataFrames based on common keys or indices. Sorting : Arranging rows or columns in a specified order. Other pandas DataFrame operations as required by the user queries. Delivery to End User: The processed output is delivered to the end user through the streamlit user interface. The user can review the insights provided by the system and further refine their queries if needed. Solution Architecture Deliverables Data Analysis Tool with Streamlit frontend. Tech Stack Tools used Langchain, OpenAI gpt-4 API Language/techniques used Python Models used Pandas Agent, GPT-4 Skills used Python, Streamlit, Streamlit cloud deployment, Langchain Web Cloud Servers used Streamlit cloud What are the technical Challenges Faced during Project Execution To make the tool follow the Indian standards in terms of Financial Year Quarters, currency and human readable values instead of exponential values. How the Technical Challenges were Solved The challenge was solved by decreasing the temperature of Pandas agent to 0 and make a custom system prompt to introduce maximum bias approximating the desirable answers. Business Impact The user was able get data analysis insights without expertise in python, pandas and other tools used in the process of Data Analysis in a fraction of time compared to what it would have been if the process was done manually. Project Snapshots Frontend Streamlit Interface IDE Environment Project website url URL: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/ (Non-Functional due to the expiry of OpenAI API Key) Project Video Link: https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63 Important Links Video Demo: https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63 URL to test App: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/ Project Success Story: https://docs.google.com/document/d/17VZukkZW6LsXVmb6IDIZWpp61sRQY_cE/edit?usp=sharing&ouid=111848530990018600604&rtpof=true&sd=true Solution Diagram: https://drive.google.com/file/d/16T56xrxBHioAIRnoA0EmHlSdMcmzEWP3/view?usp=sharing Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",22.50744101633394,-8,43.57894736842105,9,1,19,0.06026785700833068,2.528421052631579,https://insights.blackcoffer.com/ai-driven-data-analysis-ai-tool-using-langchain-for-a-leading-real-estate-and-financing-firm-worldwide/,475
18.825,18.825,7.216216216216216,246,"mvp for a software that analyses content from audio (pharma-based) Client Background Client: A leading pharma-tech firm in the USA Industry Type: Healthcare Products & Services: Pharma Apps Organization Size: 100+ The Problem The problem lies in creating a backend model for an application that records audio responses from students and uses AI to analyze the content. The backend needs to convert audio to text, transform the text into analytics KPIs, handle login/logout operations, and manage analytics API calls. The application should also calculate the cosine similarity of the student’s response with the expected response. Our Solution To solve this problem, we will use Python as the primary programming language for backend development. The solution involves several steps: Audio to Text Conversion: We will use a speech recognition library in Python such as SpeechRecognition to convert audio inputs into text. Text Analysis: After converting the audio to text, we will apply Natural Language Processing (NLP) techniques to analyze the text. This includes sentiment analysis, readability analysis, and named entity recognition (NER). We will use libraries like NLTK and SpaCy for this purpose. User Authentication: We will build a secure authentication system using JWT tokens for handling login and logout operations. API Creation: We will use Flask, a lightweight Python framework, to create APIs for managing user sessions and handling analytics data. Data Storage: We will use a relational database like PostgreSQL to store user session data, user profiles, and analytics data. Deployment: Finally, we will deploy the application on a cloud platform like AWS or Google Cloud. Solution Architecture Deliverables Backend model developed using Python APIs for managing user sessions and analytics data Secure user authentication system System capable of converting audio to text Text analysis capabilities including sentiment analysis, readability analysis, and NER Deployed application on a cloud platform Tech Stack Tools used Python Flask JWT PostgreSQL AWS/Google Cloud Language/techniques used Python Models used SpeechRecognition for audio to text conversion NLTK and SpaCy for text analysis Skills used Backend development API creation Text Sentiment analysis – Cosine Similarity Scoring Machine learning (Natural Language Processing) What are the technical Challenges Faced during Project Execution One of the main challenges faced during development was ensuring accurate audio to text conversion. Poor audio quality or heavy accents can make it difficult for speech recognition algorithms to correctly transcribe the audio. How the Technical Challenges were Solved To overcome this challenge, we decided to use a robust speech recognition library that supports multiple languages and dialects. Additionally, we implemented a mechanism to allow users to manually edit the transcribed text, providing them with more control over the accuracy of the transcription. Business Impact The implementation of this backend model will have significant business impacts: Enhanced Student Engagement: By providing immediate feedback on student responses, the system can foster a more engaging learning environment. Students can receive instant insights into their communication style and areas of improvement, encouraging them to enhance their responses and overall academic performance. Improved Learning Outcomes: The detailed analytics provided by the system can aid educators in understanding student learning patterns and identifying areas where students struggle. This can inform instructional strategies and curriculum adjustments, leading to improved learning outcomes. Cost Savings: Automating the conversion of audio to text and the generation of analytics can significantly reduce manual labor costs associated with grading and feedback provision. Scalability: The use of scalable technologies like Python and Flask allows the system to handle increasing volumes of student responses without compromising performance. Data Insights: The system generates valuable data insights, including sentiment scores, readability metrics, and named entity recognition counts. These insights can inform strategic decisions and policy changes. Customer Satisfaction: By providing a seamless, efficient experience for both students and educators, the system can enhance customer satisfaction, potentially leading to increased usage and positive word-of-mouth referrals. These impacts align with the objectives of the business, making the project a high priority. The business impact analysis will ensure that the project is aligned with the organization’s strategic goals and that potential disruptions are identified and managed effectively Project Snapshots Project website url Domain and SSL setup is completed : https://www.pharmacyinterns.com.au/ Web App is running successfully on URL – http://34.30.224.139/ Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",26.526138996138997,-11,47.49034749034749,18,1,30,0.08453608229992561,2.5984555984555984,https://insights.blackcoffer.com/mvp-for-a-software-that-analyses-content-from-audio-pharma-based/,518
14.392857142857142,14.392857142857142,6.9563492063492065,208,"7up7down, 10updown, snakes and ladder games built using oops Client Background Client: A leading game development firm in the USA Industry Type: Gaming Software Products & Services: Gaming Software Development Organization Size: 200+ The Problem Our client sends records of millions of sports bets in real time from all over the world via an API. These bets are recorded in MySQL servers. We are tasked with processing and calculating the expected Profit and Loss (PNL) as per the bets records for each sport. Our goal is to analyze these records in real time via API and calculate PNL as per the game records history provided via API. This requires building a serverless application in Python (or similar) that reads all bets records and updates PNL in real time (within milliseconds, records need to be updated). The application should be capable of handling 10,000+ records of bets per second for numbers of different games, with PNL needing to be updated for each game separately. Our Solution To address this problem, we propose developing a Python-based serverless application that leverages machine learning models for real-time PNL calculation. The application will use the MySQL database to store and retrieve betting records. It will employ parallel computing techniques to ensure efficient processing of high volumes of data. The application will also utilize APIs to fetch real-time data and update PNL accordingly. The application will follow these steps: Connect to the MySQL database to access the betting records. Use an API to fetch real-time betting data. Process the data using Python scripts. Apply machine learning models to predict the outcome of each bet. Calculate the PNL for each bet according to the predicted outcome. Update the PNL in the MySQL database in real time. Solution Architecture Deliverables A Python-based serverless application for real-time PNL calculation. An interface for visualizing the calculated PNL in real time. Documentation detailing how to use and maintain the application. Tech Stack Tools used Python: For writing the serverless application. MySQL: For storing and retrieving betting records. Machine Learning Models: For predicting the outcome of bets. Language/techniques used Python Models used OOPS Skills used Database Analysis & API Development: To design and optimize the MySQL database. Python Programming: To write the serverless application. OOPS: To make the game functioning algorithms. Databases used SQL What are the technical Challenges Faced during Project Execution One of the main challenges we faced was handling the high volume of data coming in real time. To overcome this, we employed parallel computing techniques to efficiently process the data. Another challenge was updating the PNL in the MySQL database in real time. We solved this by designing the application to update the PNL immediately after it is calculated. How the Technical Challenges were Solved We addressed the high volume of data challenge by using parallel computing techniques. This allowed us to process a large number of records simultaneously, ensuring efficient data handling. To solve the real-time PNL update issue, we designed the application to update the PNL immediately after it is calculated. This ensured that the PNL was always up-to-date, meeting the requirement of real-time PNL calculation. Business Impact The implementation of the proposed Python-based serverless application for real-time PNL calculation had significant positive impacts on our business operations. Firstly, the application enabled us to process and analyze millions of sports bets in real time, enhancing our decision-making capabilities and allowing for quicker responses to changes in the betting market. This improved our ability to predict outcomes and adjust our betting strategies accordingly. Secondly, the application significantly reduced the time taken to calculate PNL, from hours to mere minutes. This resulted in faster decision-making processes and timely financial reporting, which were crucial for our clients and investors. Lastly, the application’s ability to handle high volumes of data and provide real-time updates facilitated a more globalized betting market. With real-time data and digital platforms, geographical boundaries became less relevant, allowing bettors from around the world to place bets on any event globally, with real-time odds reflecting local nuances and dynamics. This led to increased liquidity and more competitive odds. Overall, the successful implementation of the application led to a more efficient, accurate, and timely PNL calculation process, resulting in improved business performance and customer satisfaction. Project Snapshots Project website url https://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edit https://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboard Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",22.265079365079362,-4,41.269841269841265,20,1,19,0.04946236548502717,2.4523809523809526,https://insights.blackcoffer.com/7up7down-10updown-snakes-and-ladder-games-built-using-oops/,504
14.983333333333333,14.983333333333333,6.750877192982456,215,"automate the data management process Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ Project Description Businesses now have more access to data than ever before in today’s digital economy. This information is utilised to make key business choices. Businesses should invest in data management systems that increase visibility, dependability, security, and scalability to ensure that workers have the required data for decision-making. The client wanted to get the data management process automated using a tool from Python. Multiple operations like merging,sorting, filtering had to be performed on data from various resources. The data resources were mainly csv files and data from SQL queries in PostgreSQL. Our Solution The project solution contained two tools that would aid in automatic efficient data storage. The first tool will concatenate all of the CSV files before merging them with the data from the SQL file. The acquired Excel file will be used as input for the second tool. The second tool will sort, filter, and lookup the Excel file received in the first tool. This tool will add columns that will be useful for the client’s analysis. The major goal is to assist the client with data management while requiring as little manual labour as possible. The files obtain the needed data in an Excel file by giving the proper input files. Project Deliverables The project deliverables can be divided into two parts: Excel Tool1: ExcelTool1 generates an Excel file that contains two sheets RSLTS IN and RSLTS OUT. The RSLTS IN is obtained by concatenating all the csv files in the Output folder. The RSLTS OUT is the result of merging the data from vwr egeas.sql query and RSLTS IN. Excel Tool2: Excel Tool2 creates another Excel file with one sheet RSLTS and csv files like vwr_instructions_new table, vwr proto and INST_RTR. This tool performs excel operations like lookups, arithmetic calculations and merging of data from multiple sources. Tools used For the whole data management and automation, we have made our own tool by python scripts. PostgreSQL was used to merge the csv files provided by the client with the python scripts. The automation tool will store data in the excel sheets. Language/techniques used PyCharm for compiling and running the code. The scripts for the automation tool were written in the Python programming language. OS, glob, pandas, numpy and psycopg2 were thePython libraries used in the project. Skills used Configuration and Data moving using PostgreSQL. Automation of tools Exception Handling from Python Databases used Two types of databases were used: Google excel sheets and PostgreSQL. What are the technical Challenges Faced during Project Execution Some minor challenges were faced such as data discrepancies generated during the automation process. How the Technical Challenges were Solved The challenges were solved by reworking on the automation tool and consulting with the clients for their requirements. Business Impact It is critical to use appropriate data management procedures to ensure the smooth running of a firm. Furthermore, data management must be very precise, cost-effective, and completed as soon as possible. The inability to handle data can result in costly consequences and a permanent stain on the company’s image. Every company is responsible for developing a robust data management plan. The following are some of the reasons why data management is critical to the success of the firm. Instant Availability of Information: Data management makes information easily available for quick access based on company needs. Data management is also essential for accounting procedures like auditing and other strategy-based operations like company planning. The more time you spend hunting for misplaced files and missing documents, the less productive you will be. And you are aware that time is money. Keeping all of your documents structured might therefore assist to make procedures run more smoothly and quickly. Compliance: The government passed legislation requiring businesses to maintain these data. There are also periodical checks to verify that there is no manipulation. Furthermore, if a corporation is involved in a dispute, they must maintain these records for years until a solid verdict on the matter is reached. Faster Transitions to New Technology: Because technology trends change so quickly, organizations must embrace whatever comes their way. Losing information due to obsolete or outdated systems is the last thing you want for your company. Every piece of data preserved in the firm records is essential for everyday operations, managing multiple divisions, completing computations, audits, and so on. Make Right Business Decisions: Businesses use a variety of information sources for company planning, trend research, and performance management. To execute the same activity, different departments’ teams employ different sources of information. Because the legitimacy and precision of information are highly dependent on the source, analyzing several sources may have a detrimental influence on the organization. Robust data management prevents this from happening. Project Snapshots Fig.1: Python code of Exceltool1 Fig.2: Python code of Exceltool1 Fig.3: Python code of Exceltool2 Fig.4: Python code of csv tables Fig.5: RSLTS_OUT worksheet in output Exceltool1 Fig.6: RSLTS worksheet in output Exceltool2 Fig.7: RSLTS worksheet in output Exceltool2 Fig.8: INST_RTR table as output from Exceltool2 Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",21.08105263157895,-11,37.719298245614034,18,1,38,0.097999999804,2.414035087719298,https://insights.blackcoffer.com/automate-the-data-management-process/,570
13.452380952380953,13.452380952380953,7.408114558472554,217,"dockerize the aws lambda for serverless architecture Client Background Client: A leading tech firm in the USA Industry Type: IT & Consulting Products & Services: IT Solutions, Software Development Organization Size: 100+ The Problem AWS Lambda, a powerful serverless compute service, faces limitations in terms of runtime customization, dependency management, and execution environment isolation. Our Solution To overcome the challenges mentioned above, we propose a comprehensive solution that involves Dockerizing AWS Lambda functions for improved flexibility, control, and efficiency within a serverless architecture. Solution Architecture Below is a high-level architecture diagram: Key Components: AWS Lambda Function: Contains the original Lambda function code and dependencies. Dockerfile: Describes the steps to build the Docker image, including installing dependencies, copying Lambda function code, and setting the handler function. Docker Image: The containerized version of the Lambda function, including its code and dependencies. Amazon ECR Repository: Stores the Docker image. The image is tagged with the repository URI. Updated Lambda Function: Refers to the Docker image in the ECR repository. The Lambda function configuration is updated to use this reference. Deliverables Some of the key deliverables: Dockerfile: A Dockerfile in the root of your Lambda function project, specifying the instructions to build the Docker image. This file includes the base image, installation of dependencies, copying of Lambda function code, and setting the handler function. Docker Image: The Docker image built from the Dockerfile. This image encapsulates your Lambda function code and its dependencies. Pushed Image to ECR: The Docker image pushed to your Amazon Elastic Container Registry (ECR) repository. This involves tagging the image with the ECR repository URI and pushing it to the repository. Updated Lambda Function Configuration: The Lambda function configuration was updated to use the Docker image from ECR. This may involve specifying the ECR URI in the Lambda configuration. Documentation: Documentation outlining the steps to Dockerize the Lambda function and push it to ECR. This documentation should include prerequisites, step-by-step instructions, and any additional considerations. Tech Stack Tools used Docker Amazon ECR Amazon Lambda. AWS Management Console. Language/techniques used NodeJS Docker commands Skills used AWS services (Lambda, ECR, etc.). Docker Web Cloud Servers used Amazon Web Services What are the technical Challenges Faced during Project Execution Dependency Management: Challenge: AWS Lambda imposes constraints on runtime dependencies, making it challenging to manage and control library versions. Execution Environment Isolation: Challenge: AWS Lambda’s managed environment may lack certain runtime configurations and isolation. Monitoring and Logging Integration: Challenge: Efficiently capturing and analyzing performance metrics and logs from Dockerized Lambda functions. How the Technical Challenges were solved Dependency Management: Solution: Use a containerization approach to package dependencies along with the Lambda function, providing better control and isolation. Implement a robust dependency management system within the Docker container. Execution Environment Isolation: Solution: Docker containers offer enhanced isolation. Utilize containers to encapsulate the Lambda function and its dependencies, ensuring consistent execution environments. Monitoring and Logging Integration: Solution: Integrate AWS CloudWatch for basic monitoring. Project Snapshots Create ECR Repository: Create directory and initialize npm: View Docker commands: Login to ECR and Build Docker image: Create Lambda Function: Testing Lambda Function: Project Video Dockerizing a Lambda Function: https://www.loom.com/share/e90438538dbb43fd884a51dab6c175e9?t=586&sid=b2e4112e-16b9-4d78-a955-77a289453e59 Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",26.096942834413,-11,51.78997613365155,15,-1,10,0.05198019789113813,2.747016706443914,https://insights.blackcoffer.com/dockerize-the-aws-lambda-for-serverless-architecture/,419
15.511627906976743,15.511627906976743,7.583333333333333,257,"database discovery tool using openai Client Background Client: A leading retail firm in the USA Industry Type: Retail Products & Services: Retail Business, e-commerce Organization Size: 100+ Problem Statement: Organizations often face challenges in managing and understanding their vast and complex databases. As data infrastructure evolves, new databases are introduced, and existing ones are modified, leading to a lack of comprehensive visibility into the entire data landscape. This lack of awareness poses several issues, including increased difficulty in ensuring data quality, security vulnerabilities, and inefficiencies in database administration. To address these challenges, there is a need for a Database Discovery Tool using OpenAI, aimed at providing an automated and intelligent solution for discovering, cataloging, and understanding the various databases within an organization’s ecosystem. Key Problems to Solve: Database Proliferation: Challenge: The rapid growth of databases within an organization makes it challenging to keep track of all data storage systems. Impact: Increased difficulty in managing, securing, and optimizing databases. Data Schema Variability: Challenge: Databases often have diverse schemas, making it hard to understand the structure of stored data. Impact: Inefficient data integration and difficulty in ensuring data consistency across the organization. Limited Metadata Documentation: Challenge: Lack of comprehensive metadata documentation for databases, including information about tables, columns, relationships, and data types. Impact: Time-consuming manual efforts for understanding data structures and dependencies. Security and Compliance Risks: Challenge: Inability to identify and monitor sensitive data across databases may lead to security and compliance risks. Impact: Increased likelihood of data breaches and non-compliance with regulatory standards. Operational Inefficiencies: Challenge: Manual efforts required for discovering and documenting databases result in operational inefficiencies. Impact: Increased workload for database administrators, leading to potential errors and delays. Lack of Intelligent Insights: Challenge: Absence of intelligent insights into database usage patterns, performance metrics, and optimization opportunities. Impact: Missed opportunities for improving database performance and resource utilization. Proposed Solution: Develop an OpenAI-powered Database Discovery Tool that leverages natural language processing (NLP) and machine learning capabilities to automatically discover, catalog, and provide insights into the organization’s databases. The tool should be able to: Automatically scan and identify databases across different environments. Extract and catalog metadata, including schema details, relationships, and data types. Provide intelligent insights into database usage patterns and performance metrics. Identify and classify sensitive data for enhanced security and compliance. Enable efficient search and navigation of the entire database landscape. Support ongoing updates and synchronization with changes in the data infrastructure. By addressing these challenges, the Database Discovery Tool using OpenAI aims to empower organizations with a holistic view of their data landscape, facilitating better management, security, and optimization of databases. Solution Architecture Step by Step Execution Step 1 . Database Support In this step we communicate with different types of databases, like SQL and Oracle. This means it can connect and retrieve information from a variety of database systems using Python, providing users with more flexibility and compatibility across various database environments. Step 2 . Data Extraction In this step we are using python for our Extract, Transform, Load (ETL) processes this involves efficiently reading and extracting data from the connected databases. Python handled the data-related tasks, ensuring a robust and effective extraction process and save the result in csv files which in turn are converted to .db files for sqlite. Step 3 . Fine-Tuning In this step fine-tuning mechanisms to optimize the performance and accuracy of data extraction processes. This Ensures the ETL tool finds data accurately and quickly. Step 4 . Integration with OpenAI In this step we have utilized SQL Agent for communication with OpenAI, By communicating with OpenAI, the SQL agent get the ability to understand and respond in a more intelligent and context-aware manner. Step 5 . API Integration In this step we made Django API endpoints for requesting and receiving data. This means that external systems or applications can interact with the SQL Agent through OpenAI by sending requests and receiving responses through these APIs. Step 6 . Streamlit Frontend In this step we made a streamlit frontend to chat with the SQL Agent. The user can ask question about the database and receive responses in form of insights. Video Demo",28.170463128602666,-23,54.91452991452992,11,1,28,0.11434977552836373,2.8333333333333335,https://insights.blackcoffer.com/database-discovery-tool-using-openai/,468
19.26829268292683,19.26829268292683,7.030534351145038,226,"end-to-end tool to predict biofuel prices using ieso data Client Background Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting, Software Development Organization Size: 100+ The Problem The task involves creating an end-to-end data pipeline to extract data from various reports, store it in a Google Cloud Platform (GCP) database, build a dashboard, and develop a machine learning model for price forecasting. The data is pulled from different links, each having a slightly different report layout, with some being in CSV and others in XML format. The goal is to extract data daily and hourly for the past three years. The extracted data is intended to be used for building a dashboard and training/testing a model based on user-defined inputs on the dashboard. The challenge lies in handling the varied formats of the data, ensuring accurate extraction, and maintaining the integrity of the data throughout the pipeline. Our Solution To solve this problem, we will use Python, along with libraries such as pandas and BeautifulSoup, to scrape data from various report links. The scraped data is stored in dataframes and then loaded into Google Cloud Storage buckets. This data is then transferred to BigQuery tables for efficient processing. The data extraction process is automated with a Cronjob/Google Cloud Scheduler. For the machine learning part, we will build and run various machine learning models in GCP’s BigQuery to predict future fuel/energy prices. We will test LSTM univariate/multivariate, GRU for time series problems, and ANN Regressor, Random Forests regression for regression problems. The ANN regression model will provide the best results for our use case. After modeling, we will generate a data visualization report on Google Data Studio for further insights. The report includes a pie chart about the distribution of fuel generated by each fuel type, a stacked column chart about the distribution of fuel generated each month, and a time series visualization of fuel generation during each quarter of the year. Solution Architecture Deliverables End-to-end data pipeline Data stored in Google Cloud Platform (GCP) database Dashboard built on Google Data Studio Machine learning model for price forecasting Tech Stack Tools used Python pandas BeautifulSoup Google Cloud Platform (GCP) Google Cloud Storage Google BigQuery Google Data Studio Language/techniques used Python Models used LSTM GRU ANN Regressor Random Forests Regression Skills used Web Scraping Database Management Data Visualization Machine Learning Model Development Databases used Google BigQuery What are the technical Challenges Faced during Project Execution Handling varied data formats (CSV, XML) Ensuring accurate extraction of data Maintaining data integrity throughout the pipeline How the Technical Challenges were Solved Utilizing Python libraries like pandas and BeautifulSoup for web scraping and data manipulation Automating the data extraction process using Cronjob/Google Cloud Scheduler Testing various machine learning models to select the best fit for our use case Using Google Cloud Platform services for storing, processing, and visualizing data. Business Impact The successful implementation of the end-to-end data pipeline project had several significant business impacts. Firstly, it led to improved data quality and accessibility. The project streamlined the process of data extraction from various sources, ensuring that the data was clean, consistent, and readily available for analysis. This resulted in more reliable and accurate predictions, leading to better decision-making and strategic planning. Secondly, the project enhanced operational efficiency. By automating the data extraction process with a Cronjob/Google Cloud Scheduler, the team saved considerable time and effort. This allowed the team to focus on more strategic tasks, thereby increasing productivity. Thirdly, the project facilitated informed decision-making. The dashboard built on Google Data Studio provided users with real-time insights into fuel consumption patterns and energy prices. This helped stakeholders make informed decisions regarding energy usage and pricing strategies. Lastly, the project demonstrated the company’s commitment to leveraging advanced technologies for business growth. The use of Google Cloud Platform, BigQuery, and Google Data Studio showcased the company’s ability to innovate and stay competitive in the rapidly evolving digital landscape. Overall, the project had a positive impact on the company’s operations, decision-making processes, and reputation among stakeholders. It underscored the importance of data-driven decision making and highlighted the potential benefits of investing in advanced technologies. Project Snapshots Project website url https://console.cloud.google.com/compute/instances?authuser=1&project=ieso&pli=1 Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",24.9592254701173,-20,43.12977099236641,14,1,29,0.10359408011925142,2.5610687022900764,https://insights.blackcoffer.com/end-to-end-tool-to-predict-biofuel-prices-using-ieso-data/,524
24.11627906976744,24.11627906976744,6.321192052980132,223,"realtime kibana dashboard for a financial tech firm Client Background Client: A leading fintech firm in the USA Industry Type: Finance Services: Financial services Organization Size: 100+ Project Objective Create a real-time Kibana dashboard to monitor the real-time movement and activities related to company/stock on the AWS to analyse data and get insights through dashboards to prevent due diligence. Dashboard should include visualizations of sentiments, FOIA requests, stock prices, volume, borrow rate, etc. Project Description Create real-time dashboards to get insights about the data and to analyse the relative change in different activities. Someone filing FOIA SEC request or FOIA FDA request and/or registering for conference calls might also have posted some negative tweets on tweeter to influence the market. Dashboard should display data of requests, sentiments, stock prices, etc on the same timeline, so that we will be able to observe the changes and relative changes with respect to time. Make separate dashboard for 2 stock symbols to analyse the activities and changes specific to that and a dashboard for all the data, eg. stocks, requests, etc. Change in sentiments effecting the price of the stock, borrow rate, trading volume, etc. should be noticeable. There is a list of names, make alert on the dashboard when the requests are filed by them on the same timeline used for other data. Also include the candlestick chart to view the stock details like open, close, high, low, volume with respect to time. Our Solution For FOIA SEC and FDA requests, made a metric chart representing the total number of requests and requesters, created a date histogram to view the frequency of requests and requesters with respect to time, bar chart to view the top requester name, organization, category, pie chart to view the proportion of final disposition of requests and tag cloud for the description of the requests for the entries present in the selected time range and a search table that contains the selected columns (only relevant ones) for both SEC filings and FDA filings. Similarly, for citation data, created a date histogram to view the frequency of citations and names of firms who posted with respect to time and bar chart to view number of citations by firm in the selected time range and a search table that contains the selected columns (only relevant ones). Index containing fail to deliver data is used to plot the date histogram in which volume failed is represented by the bar along the line representing the price at that time, bar chart where bars represents the total volume failed to deliver with respect to stock symbol and average price of the stock symbol in the selected time range by a dot size add on and tag cloud of the stock symbol as per fail to delivers. For twitter data (short seller’s data), made a pie chart to show the proportion of polarity, metric table to show the highest 10 average retweets with respect to user name, made a date histogram to show the frequency of tweets as per time and another date histogram representing the amount of positive and negative sentiments with the help of bars as per time to leverage us to observe if change in amount of sentiments is affecting price of stock, volume in trade and fail to deliver, etc., bar chart to show the total posts and number of posts in the selected time range and another bar chart to show the count of followers and friends in the index in selected time range. A search table is made with columns like polarity, follower counts, retweets and post with timestamp to get precise info of what we have in visualizations. For the list of names to be tracked on requests made and to make alert for them, added a annotation on the TSVB graph and added all of these along with the above visualizations on the dashboard on Kibana to make it a real-time dashboard and we can use this dashboard to do relative analysis. For the dedicated dashboards to the stock, created and added following visualizations: Metric to show number of requests and requesters in FOIA SEC and FDA indexes where description contains terms related to that stock symbol or product of the company. TSVB of FOIA SEC and FDA and added annotation where the request against the stock or company is filed. Fail to deliver and price on the same timeline to notice the relative change. Sentiment and stock details is to be added in these but the data isn’t ready yet from the client’s end. Project Deliverables 3 dashboards- 1 dashboard for complete data and 2 dashboards dedicatedly for one stock each. Tools used Kibana and Elasticsearch Skills used Visualizations and analytical skills were used Databases used Following databases are used to: FOIA SEC filings FOIA FDA filings Citations Fail to deliver Tweeter Short seller data Stock price Web Cloud Servers used AWS Management Console What are the technical Challenges Faced during Project Execution As I was using Kibana and studying the stock data for the first time, I faced challenges in making complex visualizations and understanding the terms related to stock data. Using filters while making Vega Charts to make candlestick chart with inconsistent data was displeasing. How the Technical Challenges were Solved Challenges related to the creation of complex visualization was solved exploring options on the Kibana and getting reference from the online sources. In order to understand the stock information and how things work, I got immense amount of knowledge from the client and from my project manager. For filtering of data in Vega charts I took help from the online sources. Project Snapshots Project website url https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/33529a85d7949871c0833dab8c3b3322 https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/255f9ffe21bb76f96d1be5d49c7f75a7 https://search-r2-analytics-elasticsearch-7ikdbjjl6wpkvryfdq65wxh3iq.us-east-1.es.amazonaws.com/_plugin/kibana/goto/f8f6ad6a627f6f74ce2a775288bdbc5c Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",24.414723548436783,-18,36.920529801324506,15,1,22,0.07561436658674033,2.281456953642384,https://insights.blackcoffer.com/realtime-kibana-dashboard-for-a-financial-tech-firm/,604
18.9,18.9,6.981132075471698,269,"data studio dashboard with a data pipeline tool synced with podio using custom webhooks and google cloud function Client Background Client: A leading retail firm in the USA Industry Type: Retail Products & Services: Retail Business, e-commerce Organization Size: 300+ The Problem The client needs a consolidated KPI dashboard that aggregates data from various applications and SaaS products. Currently, the data is scattered across different platforms, making it difficult to track key performance indicators (KPIs) effectively. The client wants a dashboard that automatically updates with new data, eliminating the need for manual updates. The dashboard should contain separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Additionally, the client wants to use Google Cloud Functions to sync data regularly between the Podio data app and Google Sheets. Our Solution The proposed solution involves the creation of a KPI dashboard in Google Sheets, which will serve as a central hub for all the client’s data. This dashboard will be populated with data from various sources, including Google Sheets and the Podio data app. The data will be organized into separate tabs, each representing a different aspect of the business. The dashboard will be designed to automatically update with new data, removing the need for manual updates. The process begins with obtaining access to the data in Google Sheets. Once the data is accessed, a list of KPIs to be visualized will be prepared. The data from Google Sheets will then be connected to the Google Data Studio dashboard for visualization. The dashboard will be designed to align with the client’s goals, prioritizing the most important KPIs and positioning them at the top of the dashboard. The dashboard will also be protected to prevent further or accidental changes, ensuring that data can only be added or changed through designated data sheets. Collaborators will be invited via email, with specific roles assigned to ensure effective collaboration. The dashboard will be customized with brand-aligned colors and fonts to enhance its appearance and authority. In addition to the dashboard, webhooks will be created for the Podio data app deployed as a Google Cloud Function. This will enable regular data synchronization between the Podio data app and Google Sheets, ensuring that the dashboard is always up-to-date with the latest data. Solution Architecture Deliverables End-to-end data pipeline KPI Dashboard in Google Sheets with separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Automatic update functionality to eliminate the need for manual updates. Webhook for the Podio data app deployed as a Google Cloud Function to sync data regularly. Tech Stack Tools used Python Google Sheets Google Data Studio Google Cloud Functions Podio data app Language/techniques used Python Javascript Skills used Data Analysis Data Visualization Cloud Functions API Integration Databases used BigQuery What are the technical Challenges Faced during Project Execution One of the main challenges was ensuring that the dashboard could seamlessly integrate data from various sources and update automatically. Another challenge was designing the dashboard in a way that aligns with the client’s goals and presents the data in a clear and actionable manner. How the Technical Challenges were Solved The first challenge was addressed by connecting the data sources to Google Sheets and setting up the dashboard to automatically update with new data. This was achieved by using Google Data Studio and Google Cloud Functions. The second challenge was addressed by focusing on the design and organization of the dashboard, ensuring that it aligns with the client’s goals and presents the data in a clear and actionable manner. This was achieved by prioritizing the most important KPIs and positioning them at the top of the dashboard, and by presenting supporting data as charts and tables to help decision-makers make sense of the KPI Business Impact The implementation of the proposed solution has significantly improved the client’s ability to track and manage key performance indicators (KPIs). Prior to the solution, the client was struggling with data fragmentation across different SaaS products and applications, which made it difficult to compile comprehensive insights. The KPI dashboard, now consolidated in Google Sheets, has streamlined this process, providing a unified view of the business metrics. This solution has also automated the data update process, saving valuable time and resources that were previously spent on manual updates. The automatic update feature has allowed the client to focus on analyzing the data rather than spending hours updating it. Additionally, the integration of the Podio data app with Google Sheets via Google Cloud Functions has improved data synchronization efficiency. Regular data synchronization ensures that the KPI dashboard is always up-to-date, providing real-time insights into the business performance. These improvements have led to enhanced decision-making processes within the client’s organization. With accurate and timely data, managers can now set and achieve goals more effectively. The consolidation of data has also facilitated cross-departmental collaboration, as teams can now access and share data easily. Overall, the solution has resulted in significant business impact, leading to improved operational efficiency, informed decision-making, and strategic planning Project Snapshots Project website url https://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edit https://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboard Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",26.016260720411665,-13,46.14065180102916,10,1,27,0.07707129079562372,2.595197255574614,https://insights.blackcoffer.com/data-studio-dashboard-with-a-data-pipeline-tool-synced-with-podio-using-custom-webhooks-and-google-cloud-function/,583
67.0,67.0,6.3,33,"design and develop jenkins shared library Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem Create Jenkins shared library for the following: validate AWS AMI creation check if network rules exist in aws EC2 check if the security group in aws EC2 Our Solution We created a Jenkins shared library in which we are using AWS ec2 describe-images command with the help of aws cli if an ami don’t exist than describe-images throws error We created a Jenkins shared library in which we are using aws ec2 describe-network-acls for validating we were comparing input name with VPC We created a Jenkins shared library in which we are using aws ec2 describe-instances for validating we were checking input name with SecurityGroups group Deliverables Jenkins Libraries Tools used VS Code IDE Jenkins AWS Language/techniques used Grovvy Skills used Jenkins AWS Server Web Cloud Servers used AWS Project Snapshots Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",36.22857142857143,-4,23.57142857142857,13,1,5,0.07894736772853186,2.1,https://insights.blackcoffer.com/design-and-develop-jenkins-shared-library/,140
14.926470588235293,14.926470588235293,6.95968992248062,306,"etl discovery tool using llma, langchain, openai Client Background Client: A leading retail firm in the USA Industry Type: Retail Products & Services: Retail Business, e-commerce Organization Size: 100+ The Problem To develop an ETL discovery tool that can answer the queries related to ETL pipelines in conversational format. The areas of the concerned queries should include Environment Analysis, Workflow Analysis, Data Source and Target Mapping, Transformation Logic, Data Volume and Velocity, Error Handling and Logging and Security and Access Control. Our Solution In developing our solution, we began by aggregating Open-Source Generic ETL Tool Code from various repositories on GitHub and other relevant sources. Subsequently, we meticulously fine-tuned the collected ETL tool code, organizing and saving it into distinct folders, each containing different ETL pipelines. Following this, we implemented an OpenAI Assistant, integrating it with all the refined ETL pipelines. To facilitate communication with these pipelines, we employed the OpenAI Assistant ID within our Flask API. For the user interface, we opted for a Streamlit front-end, providing a seamless and user-friendly interaction with our OpenAI Assistant and the integrated ETL pipelines. Solution Architecture ETL Discovery Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools. Step 1 . Open-Source Generic ETL Tool Code: The Open-Source Generic ETL Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools. Step 2 . Data Cleaning : Data Cleaning is a critical stage that involves cleansing and pre-processing raw data to enhance its quality and integrity. In this step the ETL understands the expected data format that is organized and cleaned for uniformity of data. Step 3 . Files/DB Represents the storage or databases utilized for storing processed data. In this step, solutions for processed data the code files will be arranged and catalogued so that they are ready to be used by the OpenAI Assistants API. Step 4 . OpenAI Assistant Creation via API: This step involves creating an OpenAI Assistant using the OpenAI API. Configuring the OpenAI Assistant Configure .env file with OpenAI API Key We will upload the files to the Assistant for it to be added in context. Run assistant creator.py file for generating OpenAI Assistant ID After Generating OpenAI Assistant id look into terminal save the generated ID into .env file We will get the assistant ID that is to be used later. Step 5 . OpenAI Assistant: In this step, the Assistant that is created from previous step will be queried by the API with instructions for the context accommodation. Features and Capabilities: functionalities supported by the assistant OpenAI Assistant will read all our ETL pipeline which is provided when we are generating the OpenAI assistant ID Usage Guidelines/Instructions: – Guide users on interacting with the OpenAI Assistant We are providing Instructions to our OpenAI Assistant to communicate with user Step 6 . Django/Flask/FastAPI API: This step involves setting up an API using popular frameworks like Django, Flask, or FastAPI. Framework Selection: choice of the specific framework We are using Flask API to communicate with the OpenAI Assistant API Endpoints: available endpoints and their functionalities Configured the OpenAI Key in app1.py Configured the OpenAI Assistant ID in app1.py Store the Instruction file into variable we are using the variable below After the Configuration of Flask file run the app1.py file to start the Flask API Local Server Authentication: – Used for securing the API Handling Request and Response process Step 7 . Chat Frontend (Streamlit): Represents the user interface for interacting with the system, built using Streamlit. Configurations: Configurations of Streamlit frontend Set your OpenAI API key into .env file User Interaction: Users will be able to query based on training data. Integration with Backend: – Frontend will be connect to the backend API. In the main.py file Provide the Flask API url endpoint to communicate with OpenAI Assistant Handle Request and Response from the User Deliverables OpenAI Assistant Flask API Streamlit frontend Tech Stack Tools used Visual Studio Code Language/techniques used Python, Flask, OpenAI Models used OpenAI Assistant Skills used Python, RestAPI, OpenAI API What are the technical Challenges Faced during Project Execution Finding the ETL pipelines and fine tuning the ETL pipelines How the Technical Challenges were solved Our approach to overcoming technical challenges involved an extensive internet search focused on ETL pipelines. We scoured various online resources, eventually identifying the most effective ETL pipelines available on GitHub. To address each challenge systematically, we created individual files for each ETL pipeline. In the process, we meticulously fine-tuned and optimized each pipeline, documenting the specific tasks and functions within the respective files. This approach allowed us to provide detailed descriptions of the work performed for every ETL pipeline, ensuring a comprehensive understanding of the solutions implemented to tackle the technical hurdles encountered. Business Impact The business impact was substantial as the client efficiently analysed numerous ETL tool pipelines. Instant answers in a chat format replaced the time-consuming manual work that could take Data Engineers days or weeks. This streamlined process significantly enhanced productivity and responsiveness, reflecting a tangible improvement in operational efficiency for the client. Project Snapshots Assistant_creator.py Main.py Project Video Project Demo Video link:- https://www.loom.com/share/5ee7d0835412474ea4aa3383af5a0814?sid=999739fc-e91a-4cda-a30e-9cd02957205f Installation Walkthrough Video:- Part 1 (Backend):- https://www.loom.com/share/338c4e09c90e453e83b86050d469d98b?sid=03299e7a-0699-464e-be2c-689a409ec01e Part 2 (Frontend):- https://www.loom.com/share/8e7942f3a03e49889c6c70fba77f76b0?sid=eca0586f-b767-45fa-854d-853bca1890dc Project GitHub Repository GitHub Link:- https://github.com/AjayBidyarthy/Rob-Sandberg-ETL Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",24.94733242134063,-5,47.44186046511628,34,1,26,0.049284578617989544,2.5953488372093023,https://insights.blackcoffer.com/etl-discovery-tool-using-llma-langchain-openai/,645
13.529411764705882,13.529411764705882,6.603174603174603,66,"data management, etl, and data automation Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ Project Objective To extract the data for the given keywords from the listed websites https://www.ferguson.com/ , https://www.bakerdist.com/ , https://www.hajoca.com/ , https://www.carrier.com/residential/en/us/ , https://www.gemaire.com/ , https://www.fwwebb.com/ and store the count of each keywords for each website it in an Excel File. Project Description A list of websites is provided from which we were supposed to find out the mentioned keywords and store their respective counts for each website in an Excel sheet with different tabs for different set of keywords. Our Solution We used Selenium as well as Bs4(Beautiful Soup) to extract data from the given websites. To accomplish the given task, 2 tools were developed for each website. Search tool was developed to search the keyword in the website’s search bar and count displayed for keywords of each category was stored in separate files. Content tool was developed which scraped full text from each url obtained from the respective sitemaps. Along with the text visible on the page, data from meta keywords, meta description and title was also scrapped. Extracted content from all the websites was stored in their respective text files. After that number of keywords in the text were counted using substring and count method and stored the keyword and its corresponding count in an Ordered Dictionary and then the count was transferred to a list and Excel file was created for the same. Counts received from search tool and content tool were combined and final output file was created. Project Deliverables Python Scripts for each website to extract the count of keywords. Excel Sheet name HVAC_Report Test.xlsx having counts for each set of keywords for each website. Tools used Python Interpreter Language/techniques used Language Used: Python Libraries used: BeautifulSoup, collection.OrderedDict, pandas, requests, xlsxwriter, selenium.webdriver What are the technical Challenges Faced during Project Execution Some of the websites cannot be accessed using Indian IP address as it was having captchas. Also, we cannot go to each and every page by clicking the results and get the count. How the Technical Challenges were Solved To bypass the captcha and reach the website, we need to use VPN of Singapore. And to get access to each and every page of the website, we found out sitemap for each website which includes link to every page present in it. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",15.88795518207283,-1,26.190476190476193,16,1,12,0.057017543609572174,2.2301587301587302,https://insights.blackcoffer.com/data-management-etl-and-data-automation/,252
127.0,127.0,6.644628099173554,91,"design and develop powershell script Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem Create a PowerShell script for the following: check and enable auditing:- client wanted a PowerShell script that checks NTFS Rule is given to a folder or not and adds a rule to it configuring winrm for remote windows server:- this client wanted a PowerShell script which helps us to connect to another windows remote server check audit of windows/system32 folder and windows/inf folder of remote windows server:- this client wanted a PowerShell script which help us to connect to the remote server and check their NTFS Rule for windows/system32 and windows/inf folder also we can add rule for those folders Our Solution check and enable auditing for checking and enabling auditing of the file we used PowerShell NTFSSecurity module for checking the audit we used Get-NTFSAudit which is a submodule of NTFSSecurity for adding the audit we used Add-NTFSAudit which is a submodule of NTFSSecutiry configuring winrm for remote windows server For this we created 2 script: create script: this help us to create listener and open port 5986 for http as winrm uses port 5986 to connect with windows connect script: this help us to connect with remote windows server for this purpose we used Enter-PSSession check audit of windows/system32 folder and windows/inf folder of remote windows server for this, we created a script that connects to the remote windows server using the Enter-PSSession command and then checks the audit for windows/system32 and windows/inf folder also we can add audit rule to windows/system32 and windows/inf folder from remote servers Deliverables Powershell script Tools used VS Code IDE Powershell Virtual machine Language/techniques used powershell Skills used Powershell BuProject Snapshots Check audit Add audit Check audit Before running create script Create script for winrm listner List of listeners after running create script Connect with remote machine When rights are not applied When rights are applied Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",65.84132231404959,-2,37.60330578512397,15,1,5,0.032407407257373115,2.190082644628099,https://insights.blackcoffer.com/design-and-develop-powershell-script/,242
7.012987012987013,7.012987012987013,6.782918149466192,124,"crm (monday.com, make.com) to data warehouse to klipfolio dashboard Client Background Client: A leading marketing firm in the USA Industry Type: IT Services: Marketing, promotions, campaigns, consulting, business growth Organization Size: 100+ The Problem The client requires a dashboard for a ”week in review” and “human resources”. The dashboard should be dynamic whenever the client opens the dashboard, it should show the current week and should also have a dropdown choice option based on different time periods. So the client requires a meaningful KPI on the dashboard. Research Objective Taking the problem statement into consideration the following objectives are established. Objective 1: Getting access to the Monday.com site, Make.com, Google sheet, and Klipfolio. Objective 2: Connect Monday.com data to the Google sheet. Objective 3: Data Integration using make.com. Objective 4: Building KPIs using various calculations and formulas to get meaningful insights. Objective 5: Creating a dashboard from insight driven by KPIs. Solution Architecture 1. Data Integration Fig.3.4: Data Integration 2. Overall Architecture Fig.3.4.2 Overall Architecture Tools used Klipfolio make.com Language/techniques used Klip Formula Skills used Data Integration Data Processing Data Visualization Web Cloud Servers used Google Sheet What are the technical Challenges Faced during Project Execution During the project execution we faced following challenges: 1. Mapping the values in make.com from Monday.com 2. Whenever the update is generated on Monday.com, a new row is added to the Google sheet. 3. Extracting insights from the data How the Technical Challenges were Solved To solve the technical challenges, we provided following solutions as follow: 1. For mapping the values from Monday.com to make.com, we got access as admin to reach out the columns id on Monday.com. 2. On make.com, we created multiple models linking each other based on the row id in the google sheet. 3. After completing the data integration, we use calculations to extract meaningful insights from the data. Business Impact Using this dashboard, a client can keep track of the employee’s work process. So he can analyze employee workflow nature. Project Snapshots Project website url Google Sheet: https://docs.google.com/spreadsheets/d/15ADtNWh63O7DVbg-FRH0SmWb-TemqldOVK7dq16N7Xs/edit?usp=sharing Data Integration using make.com: https://us1.make.com/146703/scenarios?folder=all&tab=all Monday.com: https://primus-business-management.monday.com/ List Of Employees listed on Klipfolio: https://app.klipfolio.com/clients/index Klipfolio Dashboard: https://app.klipfolio.com/dashboard?tab=012f404bf82f8b4e331c4a0c48d32978#:~:text=https%3A//app.klipfolio.com/dashboard/add_tab/8ca9ae6808284b158f640834f3e2afd8%3Fparam%3AstartDate%3D1671926400%26param%3ADatepickerB%3D1671753600%26param%3ADatePickerA%3D1671408000%26param%3Adropdown%3DWorking%20on%20it%26param%3AendDate%3D1672444800%26param%3AKTdate%3DFY%20to%20Last%20month%26param%3ADatePeriodq%3DThis%20Week Project Video Todo Board Part 1: https://www.youtube.com/watch?v=qnTV64RhGWk Todo Board Part 2: https://www.youtube.com/watch?v=vDyaVkNv6bU Todo Board part 3: https://www.youtube.com/watch?v=FciSkP-uRkM Census Board Part 1: https://www.youtube.com/watch?v=jpgzakxdvZw Census Board Part 2: https://www.youtube.com/watch?v=3y6DmUGNmTE Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",20.456440356796232,-4,44.128113879003564,11,1,10,0.05204460947195313,2.4768683274021353,https://insights.blackcoffer.com/crm-monday-com-make-com-to-data-warehouse-to-klipfolio-dashboard/,281
19.1875,19.1875,7.05607476635514,288,"design and develop a product recommendation engine based on the features of products Client Background Client: A leading retail firm in the USA Industry Type: Retail Products & Services: Retail Business, e-commerce Organization Size: 100+ The Problem Design and develop a product recommendation engine based on the features of products Our Solution Content-based product recommendation system has been created using Machine Learning Algorithm and Python. Solution Architecture Recommendation engine have six cases which are mentioned below: Case 1: Description: Given an object name or PAR ID, inp_prodname recommends products with the same object type as the input and ranks them based on the number of specifications matched. Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3. Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, Specifications, userDef1, userDef2, userDef3. Case 2: Description: Given specifications and an object type, inp_custom_spec recommends products and ranks them based on the number of specifications matched. Input: JSON format with the following keys: Specifications, Object Type, userDef1, userDef2, userDef3. Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, userDef1, userDef2, userDef3. Case 3: Description: Based on compatible models, inp_prodname_comp recommends products and ranks them based on the number of compatible models matched. Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3. Output: JSON format with the following keys: Object Name, Object Type, PAR ID, Rank, Compatible Models, userDef1, userDef2, userDef3. Case 4: Description: Based on the number of specifications entered by the user, inp_spec_num creates clusters of products with the same number of specifications. Input: JSON format with the following keys: Number of Specifications, Object Type, userDef1, userDef2, userDef3. Output: JSON format with the following keys: Cluster ID, Object Name, Object Types, PAR ID, specifications_grped, userDef1, userDef2, userDef3. Case 5: Description: Given specification attributes and an object type, inp_spec_attr creates clusters of products with the same specifications. Input: JSON format with the following keys: Specification Attributes, Object Type, Debug Information, userDef1, userDef2, userDef3. Output: JSON format with the following keys: PAR ID, Object Name, Object Type, Cluster ID, Specifications, userDef1, userDef2, userDef3. Case 6: Description: Based on the object name or PAR ID entered by the user, inp_prodname_model creates clusters of products with similar specifications. Input: JSON format with the following keys: Object Name, PAR ID, Debug Information, userDef1, userDef2, userDef3. Output: JSON format with the following keys: Object Name, Object Types, PAR ID, Specifications, Rank, userDef1, userDef2, userDef3. The APIs for all the above cases have been created Deliverables The code of the recommendation engine and its API is been delivered. Tools used Python, Postman Language/techniques used Python, Machine Learning, Flask API, Pandas Models used Affinity Propagation is a clustering algorithm that does not require a predefined number of clusters. It is used to group products based on their similarities. Skills used Python, Logical Reasoning, Machine Learning, Data Engineering. What are the technical Challenges Faced during Project Execution Product has many features but there was one feature which Is a “product type” that needs to be handled differently because it was important to have the product in a cluster must have the same product type. Some cases can’t be solved with machine learning algorithms. How the Technical Challenges were Solved Handling Product Type as a Differentiating Feature: One of the challenges faced was dealing with the “product type” feature, which required special consideration. It was crucial to ensure that products with the same type were grouped together in the clustering or recommendation algorithm. This required developing a specific approach to address the uniqueness of the product type feature and incorporate it effectively into the recommendation system. Custom modifications and additional preprocessing steps were likely needed to accommodate this requirement and ensure accurate clustering based on product type. Limitations of Machine Learning Algorithms: While machine learning algorithms are powerful tools for recommendation systems, there are cases where they may not be sufficient to solve certain challenges. During the project, it was likely discovered that some complex scenarios couldn’t be adequately addressed using traditional machine learning algorithms alone. To overcome this, alternative techniques and approaches beyond the scope of standard algorithms needed to be explored. This might involve incorporating domain-specific rules, utilizing other data analysis methods, or considering hybrid models that combine machine learning with expert knowledge to overcome the limitations and improve the recommendation system’s performance. Business Impact This recommendation engine can significantly enhance customers’ shopping experience by increasing the likelihood of them discovering products that perfectly align with their preferences. This personalized approach not only saves them valuable time and effort in searching for relevant items but also ensures that their unique needs and desires are met. As a result, customers are more likely to make purchases, leading to increased sales and revenue for the business. Moreover, this recommendation engine plays a crucial role in improving customer satisfaction and fostering long-term loyalty. By suggesting products based on individual preferences and specific features, customers feel understood and valued. This tailored experience enhances their overall satisfaction, making them more inclined to return to the business for future purchases. Additionally, satisfied customers are more likely to spread positive word-of-mouth, attracting new customers and expanding their customer base. Project Snapshots Summarize Summarized: https://blackcoffer.com/ This project was done by the Blackcoffer Team, a Global IT Consulting firm. Contact Details This solution was designed and developed by Blackcoffer Team Here are my contact details: Firm Name: Blackcoffer Pvt. Ltd. Firm Website: www.blackcoffer.com Firm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043 Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy",25.61892523364486,-27,44.85981308411215,21,1,33,0.10033444799275176,2.526479750778816,https://insights.blackcoffer.com/design-and-develop-a-product-recommendation-engine-based-on-the-features-of-products/,642
29.529411764705884,29.529411764705884,6.358895705521473,124,"an etl tool to pull data from shiphero to google bigquery data warehouse Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem Shiphero company is an organization providing shipping solutions to vendors. The data created by shiphero for different product picking and packing time period doesn’t provide much insight into the efficiency of ship hero employees and other aspects that are needed and useful for vendors/brands to make better decisions for their business in order words the ‘key’ data is missing. Our Solution The solution is an effort to create the missing data by the existing data as we came to know that the ‘key’ data can be created by involving some deep methodologies and vast logical aspects linked to it. The incoming data from shiphero company is timestamp data therefore using this sequential data we can create the missing data we need to get the required KPI’s. The overall architecture included getting data from shiphero through api doing some preprocessing and creating our ‘key’ through this data and populating it on Google big query. This google big query is linked to Google data studio for insights visualisation. Solution Architecture The data coming from Shiphero is extracted every day using a cron job scheduler. Google app engine service is used to preprocess and apply a transformation to the data. Deliverables Ready-to-use Google data studio Dashboard. Google app engine service-based scheduler code. Tools used Google App engine Google big query Google data studio Google cloud platform Language/techniques used Python (for preprocessing) GraphQL (For data extraction) Skills used Python Programming GraphQL querying Statistics Data visualization Data Engineering Data Science Databases used Google big query Web Cloud Servers used Google Cloud platform What are the technical Challenges Faced during Project Execution Initially the approach client introduced that could be able to solve the problem directly failed to give proper results and because of that we need to come up with a solution that could be able to estimate our ‘key’ column to some extent.With the way around solution using statistics and data modelling there were a series of challenges coming that were creating a question mark for us but with keen solution building and delivering the desired results we came to solution for every challenge that arose. How the Technical Challenges were Solved Statistics was the only way around for the challenges we faced because it was the data which was missing and as the incoming data was in sequential format so we were able to figure out the patterns from that and the main problem of missing data for our KPI’s Business Impact Better insights into the business. Project Snapshots Dashboards aren’t finalised but yes giving desired solutions. Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",27.026488632262726,-8,38.036809815950924,19,1,12,0.07380073773505264,2.3251533742331287,https://insights.blackcoffer.com/an-etl-tool-to-pull-data-from-shiphero-to-google-bigquery-data-warehouse/,326
12.904761904761905,12.904761904761905,6.461538461538462,58,"data management – egeas Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ Project Objective To extract various Reports from the given input files. Reports to be extracted are: PRODUCTION COST – ANNUAL BY UNITS REPORT, SYSTEM EMISSIONS ANNUAL REPORT, RPS CONSTRAINT – ANNUAL REPORT, RELIABILITY – ANNUAL REPORT, RESERVE – ANNUAL REPORT and CAPACITY TOTALS ANNUAL REPORT. We had to extract above mentioned reports from the given .out files and store it in the respective .csv files. Project Description We were given a bunch of .out files in which various Reports were available in table format. We need to extract some of the required reports from the given files and store them in their respective .csv files. A tool had to be developed in python in order to accomplish this task. Our Solution From each .out file its content extracted and stored in a list. Using regular expression, we searched the required report in the content. Another regular expression is used to mark as end of the table content. Content between the two given regular expressions is stored in a dataframe which is then stored into respective .csv file. Project Deliverables Python Scripts for each report and a combined script which could extract all the required Reports. Respective .csv files of the Reports Tools used Python Interpreter Language/techniques used Language Used: Python Libraries Used: re, pandas, os Skills used Programming Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",17.909157509157506,-1,31.868131868131865,13,1,7,0.0506329110719436,2.17032967032967,https://insights.blackcoffer.com/data-management-egeas/,182
19.0,19.0,6.4765625,84,"ner task using bert with data in xml-format Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem The goal of this task is to create and implement a workflow that annotates People/Places/Organizations and assigns them a specific number (from a normdatabase). The NER-Task should be done by using Bert (NER-German https://huggingface.co/flair/ner-german or something similar). Our Solution The input to this first task is a text in XML-Format. It is important that the structuring text is not altered by the NER. This could be possible by tokenizing the XML-elements in a different/seperate way, to then run the NER with BERT and afterwards add the elements afterwards at the exact position where the initially were. The tags that were added by the NER than can be easily replaced with the required tags in the XML-format. Solution Architecture Input Data 🡪 XML Text Tokenization 🡪 NER Model 🡪 Replace NER Tags with XML Tags 🡪 Final Output Deliverables Python tool Documentation Installation Tools used VSCode For Python script Language/techniques used Python Programming Language Models used Named Entity Recognition (NER) FuzzyWuzzy tqdm Flair Pandas Skills used Data Loading Data Processing Data Restoring What are the technical Challenges Faced during Project Execution During the project execution, we faced the following challenges: Parsing of the input XML file. Predicting the Name, Place and Organization. Rearranging the XML file to its origin form with the predicted value. How the Technical Challenges were Solved To solve the technical challenges, we provided following solutions as follow: It was not possible by the beautiful soup library. So by using the logically function start index and end index we break the sentence. For predicting the NPO we used the flair ner-german model. To rearrange the file we used start index and end index function which can be split with a certain condition and we place the predicted value in it. Business Impact The client can know easily predict the Name, Place, and Organisation from XML containing file by using our python script model. Project Snapshots Fig. Input XML file Fig. Output XML file with predicted values. Project website url Github: https://github.com/AjayBidyarthy/Sven-Meier-XML-tool/tree/master Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",20.725,-6,32.8125,17,1,7,0.057017543609572174,2.234375,https://insights.blackcoffer.com/ner-task-using-bert-with-data-in-xml-format/,256
34.55555555555556,34.55555555555556,5.814345991561181,55,"design and develop a retool app that will show stock and crypto related information using iex api Client Background Client: A leading fintech firm in the USA Industry Type: Finance Services: Crypto, financial services, banking, trading, stock markets Organization Size: 100+ The Problem Create a retool app that will show stock and crypto related information using IEX API Our Solution Created a flask web application with following features and pages: Page 1 (Home page) – Show a Stock & Crypto Search Bar that will show the most relevant option in the IEX API via ticker search. Upon submit, user will be taken to the “Ticker Page” – List the 10 top trending stocks for each category (link click to ticker page) (logo, Stock ticker, company name, stock price, % change. Mega Cap Large Cap Mid Cap Small Cap Micro Cap Page 2 (Ticker Page) -Show Company Data – (Ticker, Company Name, Logo, Market Cap, and all the other corporate data (employees, CEO, HQ, Founded, Website) -Stock Price Chart – 1 year chart, daily. -Stock Price Volume – Weekly average 20 weeks -Recent News – list of 25 most recent articles Deliverables Deployed flask web application on AWS Tools used VS Code IDE Nginx Language/techniques used Python Skills used API Integration Python AWS Server Nginx Web Cloud Servers used AWS What are the technical Challenges Faced during Project Execution There was lots of pre-processing required to create application as per client requirement How the Technical Challenges were Solved We shifted the application from retool to python flask application as python programming language allow as to pre-process the data as per our requirement Project Snapshots Project website url www.stocks.bullish.studio Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",23.104922644163153,-3,23.20675105485232,6,1,6,0.04326923056120562,2.0126582278481013,https://insights.blackcoffer.com/design-and-develop-a-retool-app-that-will-show-stock-and-crypto-related-information-using-iex-api/,237
32.75,32.75,6.409356725146199,64,"design and develop retool app for wholecell.io and asana data using their api’s Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem Create retool app for wholecell.io and Asana data using their api’s Our Solution We have created two table one table contain data from wholecell.io platform and another table contain data from Assna. In that wholecell.io table we are providing: Order id Order status Order channel Organization Link of the Order In Assna Table we are providing following details: Id of the task Name of the task Resource type Resource_subtype Caller Po-id As client data from wholecell and Assna was linked client can search the order by PO-id in Assna table Deliverables App in retool Tools used Retool Language/techniques used JavaScript Skills used Retool API integration JavaScript What are the technical Challenges Faced during Project Execution Api was not providing all required details according to the client requirement and there were less options for data pre-processing as retool only javascript How the Technical Challenges were Solved We had fetched details from one api and provide id to the other api using JavaScript this was done by using javascript promise method We also had to do some string manipulation to get data according the client requirement Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",28.07076023391813,-3,37.42690058479532,12,1,6,0.06293706249694361,2.409356725146199,https://insights.blackcoffer.com/design-and-develop-retool-app-for-wholecell-io-and-asana-data-using-their-apis/,171
19.63157894736842,19.63157894736842,6.208163265306123,85,"data from crm via zapier to google sheets (dynamic) to powerbi Client Background Client: A leading solar panel firm in the USA Industry Type: Energy Services: Solar Panel Organization Size: 500+ The Problem Solar Panel organization from America wants to keep track of sales data. They want to see the leadership dashboard of their organization in terms of sales. They also want to keep track of their campaigns and leads generated from sources of those campaigns. They want to keep track of sales data from different sources. Our Solution First, we fetch the data from CRM to PowerBI. Clean the data of CRM using DAX and then perform calculations on the data. Using cleaned data, we build KPI on PowerBI. Solution Architecture To complete the project, we follow the following data flow pipeline: Data from CRM 🡪 Zapier 🡪 Google Sheet (Dynamic) 🡪PowerBI Language/techniques used PowerBI, DAX Language Skills used CRM, Zapier , PowerBI, Google Sheet What are the technical Challenges Faced during Project Execution Challenges Faced during the Project Execution : Fetching the data from CRM Unclean Data Merging the Data How the Technical Challenges were Solved Solution: To Fetch the data from CRM. We used Zapier. It is connector between two applications so that whenever a particular incident happen it will populate into another application. We use Zapier to connect CRM and Google sheets so that whenever a new lead will change or modified data will be stored into google sheets. Data in google sheets was uncleaned. First, we connect the Google sheet with PowerbI then perform EDA to clean the data using DAX language. Using merging of two tables by ONE-ON-ONE schema we solve duplicate entries of a particular lead in PowerBI. Business Impact Using this Dashboard client can make important decisions like from which campaign they are getting a greater number of leads and out of those leads how many are actually a Sale. They can keep track of their sales leadership of employee of the month in term of sales. Project Snapshots CRM Zapier Dashboard Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",21.730182599355533,-3,34.69387755102041,21,1,16,0.094999999525,2.2612244897959184,https://insights.blackcoffer.com/data-from-crm-via-zapier-to-google-sheets-dynamic-to-powerbi/,245
30.571428571428573,30.571428571428573,6.862559241706161,174,"recommendation engine for insurance sector to expand business in the rural area Client Background Client: A leading insurance firm in the globe Industry Type: Insurance Services: SaaS, Products, Insurance Organization Size: 10000+ Project Objective Develop the recommendation engine Item-based collaborative filtering based on the use case of the project Work on Streaming data platform i.e BangDb Data Generation for Testing the platform Project Description BangDB is the platform that manages the static data stored on the cluster and also works with live streaming data as Hadoop does. Wherever the bangdb is able to manage machine learning model deployment with their inbuilt parameter and hyper tuning parameters for each model. Streaming data from the client which relates to the customer details and the numbers of products offered by the client on their platform, such as Insurance, loans (Business Loans and Personal Loans), Mobile recharge, UPI transactions done by their platform, etc. They wanted the recommendation of other services provided by them to each of their customers who are using their platform. Our Solution This Project Module develops according to the Clients Requirements which involves item-based collaborative filtering based on customer behaviour, Firstly classify the customers into various segments on the basis of age, location, gender, and product usage. On the basis of RFM (marketing tactics to classify the customer on the basis of their purchase history, amount spend, and frequency of usage of product) classify them and recommend them the other services based on item-based collaborative filtering. We generated the synthetic data (90 Million events) for the testing of the recommendation model and its accuracy for recommending the other products to customers. Project Deliverables – KPI of the Customers – Recommendation model – Graph databased model – Data Generation code based on python (using copula-based on PyTorch) Tools used BangDb Tool (ML, AI, NoSQL database supported) Graph Databased Google Colab (Data file generation) Tableau for data visualization Language/techniques used Linux cloud machine Python Graph Database Data visualization tools Models used -K means model for clustering -Recommendation Engine model -Collaborative based filtering model Skills used – Machine learning – NoSQL Database – Graph database – Data Generation using python – Linux – Data Visualization Databases used – BangDB – Graph Database – Microsoft MYSQL server Web Cloud Servers used AWS cloud service What are the technical Challenges Faced during Project Execution Decide the Recommendation Engine based on the use case Finding the RFM score and classifying the customers into clusters Graph Model to define the relations of customers with each service which they are using Synthetic data generation(90 Million events) and around 1.5 Gb structured data. How the Technical Challenges were Solved Item-based collaborative filtering solves the issue of recommendation because we are dealing with almost 14- 15 services. Clustering of customers based on their similarities Measure the RFM score, and group and classify them based on their scores. Graph database provides to reduce complexity and increase the processing speed. Data generation is one of the difficult tasks and generating relational data across 29 different streams using copula and UUID python library function which is based on PyTorch. Business Impact It is Qualitative and Quantitative impact on economically where customers are a direct impact of these projects in their life. It is suggesting to the customers what services they have to utilize from the provider and this is a direct impact of the product on the customers. Product is providing the action statement of the usage of services by the customers and impacts them economically as well. The scope impact of product service is Nationwide or statewide. To provide these impact-full services, there is a tech team of Blackcoffer behind it Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",28.721462423832094,-7,41.23222748815166,28,1,17,0.061381074011813114,2.4668246445497632,https://insights.blackcoffer.com/recommendation-engine-for-insurance-sector-to-expand-business-in-the-rural-area/,422
10.608695652173912,10.608695652173912,6.242214532871972,112,"monday.com to kpi dashboard to manage, view, and generate insights from the crm data Client Background Client: A leading energy firm in the USA Industry Type: Energy Services: Solar panel Organization Size: 200+ Project Objective Setup a dashboard on Monday.com Fetch client CRM data onto Monday.com dashboard. Project Description Mohsin has CRM for his business where he has all data regarding leads of his clients. He wanted to see all his client appointments at one place. Client took subscription of Monday.com. It is an CRM where you can manage your work more easily in neat and clean user-friendly environment. We can easily track our task on Monday.com. Pipeline for Monday.com is very easy to use and also customized according to our needs. Our Solution The challenging part of this project was to get CRM data on Monday.com dashboard. Client also has subscription of Zapier. Zapier is a connector which connect two apps to transfer data from each other. Zapier also has limitation to fetch limited type of data from CRM. Like for Mohsin CRM we can only fetch hot lead comes on CRM. But in his CRM there are also other functions like if a customer lead comes on CRM. They manually book appointment for that client. So there is no way to get that data from CRM. Issue for client was he has attached integrated four google calendar account with CRM so whenever he confirms appointment on CRM that data fetched on google calendar. But he has check manually one by one on each calendar which was bit hard task for him. So, we advised Monday.com where he can track all his task at one place. Tools used Monday.com Zapier Google Calendar Databases used Google Calendar What are the technical Challenges Faced during Project Execution The challenging part of this project was to get CRM data on Monday.com dashboard.There is no direct integration of CRM and Monday.com to fetch data. How the Technical Challenges were Solved To solve challenges, we used Zapier to get CRM data to dashboard of Monday.com. We used google calendar of client which were integrated with CRM. All the appointment confirmed leads were present on google calendar. Pipeline of Data: CRM 🡪 Google Calendar 🡪 Zapier 🡪 Monday.com Business Impact Using the Monday.com dashboard client can easily track all appointments of customers. He can track data of his team members and connect with them at one place. He will not miss any of his meeting with customer. Monday.com also has timeline and calendar view using that client can see all activity of his work. Project Snapshots CRM Calendar view Monday.com Google Calendar Zapier Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",19.745208364675793,-8,38.75432525951557,33,1,20,0.11155378441612038,2.1833910034602075,https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-from-the-crm-data/,289
17.6,17.6,6.415730337078652,52,"qualtrics api integration using python Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem API Integration to read/write data in SQL tables from an online application. Our Solution To write the api between qualtrics and sql server using python programming language. Solution Architecture Fig. System Architecture Deliverables Python Software Documentation Tools used Python Qualtrics Models used Pandas Requests numpy Zipfile io pyodbc Skills used Extract Transfer Load Databases used SQL Server What are the technical Challenges Faced during Project Execution During the project execution, we faced the following challenges: After data integration, the content of the file was not readable. Mapping the values with the required columns. How the Technical Challenges were Solved To solve the technical challenges, we provided the following solutions as follow: To get the content into the CSV format after integration we used the Io module to get the text content. To get the mapping values we created the CSV file and store the record in it and fetch that record to the SQl. Business Impact Using this script the client can now fetch the Qualtrics data into the SQL server automatically after every 1 hour. Project Snapshots Fig. Data in CSV Format Fig. Data in Table form Fig. SQL data Project website url Github: https://github.com/AjayBidyarthy/Richi-S-api Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",18.725393258426966,-2,29.213483146067414,10,1,6,0.04938271574455114,2.191011235955056,https://insights.blackcoffer.com/qualtrics-api-integration-using-python/,178
35.714285714285715,35.714285714285715,6.733333333333333,51,"nlp-based approach for data transformation Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem Performing Readability and Quality testing on the text corpus from text files Our Solution The intention was to create a tool/system that can consume text files through a given csv file having a path for all the text files through this csv file our tool should be able to read all files one by one and could perform some tests and analysis on that text data and output the results in a csv format presenting all the metrics. In order to achieve this goal we created a Python-based ready-to-use code that will read all text files presented in the given csv files and perform 14 different evaluations on that text data and save the results in a excel and csv based format. Solution Architecture Deliverables The final deliverable was the tool/system/code for processing and evaluation text. Language/techniques used Python Natural Language processing technique used for text evaluation Skills used Python Programming What are the technical Challenges Faced during Project Execution The architecture of the solution for this project problem statement was simple, no challenges were faced during the execution of the project. Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",27.88571428571429,-3,34.0,7,1,6,0.0666666661728395,2.3466666666666667,https://insights.blackcoffer.com/nlp-based-approach-for-data-transformation/,150
27.923076923076923,27.923076923076923,6.924107142857143,94,"plaid financial analytics – a data-driven dashboard to generate insights Client Background Client: A leading financial firm in the USA Industry Type: Finance Services: Financial Services Organization Size: 100+ The Problem Applying automation to Financial data coming from the Plaid platform that needs to be visualized in order to get better insights and metrics from data. Our Solution The intention was to create an automation tool that could consume the financial csv format data and perform preprocessing on that data and could directly present the insights on visually appealing dashboard. Initially the step was to create a tool/website that could consume the data and preprocess it and send it either directly to dashboard or into a database so the data could be safe and through the database the dashboard could be linked and updates accordingly. The data source for the tool was to be a manual entry therefore we created a website and hosted it on a cloud platform(Heroku) to make it available all the time for all the desired users. The processed data from this tool will be send to the Google big query database and our GBQ will be linked to the Google Data Studio for the insights presentation. Therefore as the data will keep on updating in the google big query accordingly the dashboard in our google data studio will gets updated. Solution Architecture Deliverables The final deliverable was the ready-to-use dashboard and website where the preprocessing of the data happens. Tools used Google Cloud platform – Google Big Query (Database) Google Data studio(Visualisation/Dashboard) Heroku Cloud(Hosting the web application) Language/techniques used Python Skills used Python programming Data analytics/Visualisation Google Big Query Databases used Google Big Query Web Cloud Servers used Heroku Cloud What are the technical Challenges Faced during Project Execution The project was easy to implement and the architecture was simple therefore no major challenges were encountered. Project Snapshots Project website url https://plaid-conversion.herokuapp.com/ Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",27.954945054945057,-7,41.964285714285715,11,1,10,0.08762886552768626,2.5892857142857144,https://insights.blackcoffer.com/plaid-financial-analytics-a-data-driven-dashboard-to-generate-insights/,224
28.46153846153846,28.46153846153846,6.617647058823529,83,"analytical solution for a tech firm Client Background Client: A leading tech firm in the USA Industry Type: IT Services: Consulting Organization Size: 100+ The Problem The client’s organization had a project that matches URLs up using TF-IDF algorithm. The script threw some errors and resolving these errors was the immediate ask. The client also required us to adjust the script for better accuracy and faster computation. Our Solution R&D on the code developed Find & List bugs Solve the Bugs Find and get the best matching algorithm implemented. Check and compare the existing matching algorithm implemented for accuracy. if not check of other solution – ngrams or fuzzy logic Meet the expected output Deliverables Fully functional code Solution & Documentation Support Tools used Google spreadsheets Microsoft Excel Google Colaboratory Language/techniques Python Models used TF-IDF BERT Ngrams Flair Embeddings Rapid Fuzz Skills used Problem-solving Communication Data Modelling Data Pipelining Python Coding Databases used Google spreadsheets What are the technical Challenges Faced during Project Execution Bugs on the model used by the client was fairly competent using pretrained libraries The accuracy for the bug free code on the models used by the client was shaen once the model ran on a different set of data input How the Technical Challenges were Solved A vanilla code to execute the same logic while fine tuning the matching algorithm was written in order to over come the shortcomings of the pretrained model bugs The data pre-processing was done manually in order to transform every instance of an input into better readable format to be able to go into the model and get best matching accuracy possible in the given timeframe of execution of the code Business Impact Helped the client to perform the matching process with maximum accuracy and lowest cost on code, by implementing manually written vanilla code from scratch to utilise the matching algorithm. Project Snapshots Project website url https://colab.research.google.com/github/AjayBidyarthy/Daniel-Emery/blob/main/vanilla.ipynb#scrollTo=vPp14xj020RL Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",25.334195216548157,-13,34.87394957983193,5,1,18,0.14903846082193048,2.2815126050420167,https://insights.blackcoffer.com/analytical-solution-for-a-tech-firm/,238
50.84615384615385,50.84615384615385,6.7703862660944205,199,"ad networks marketing campaign data dashboard in looker (google data studio) Client Background Client: A leading financial firm in Dubai Industry Type: Financial Services Services: Banking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech Organization Size: 200+ The Problem Build dashboards unifying all the platforms in use: Google Ads, FB ads, Appsflyer, Mixpanel, etc, in order to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel Our Solution Track the app data analytics using various platforms Prepare the data sources – find and build data connectors for Google Data Studio. Developed 14 pages of Dashboard reports- creating templates to importing data sources and perform various visualisations. Maintained and tracked dashboard reports and helped the client with intelligence from these reports. Deliverables 1 Updating the iOS datasheet 2 Fixing the incoming data for androids 3 Correcting a calculation error 4 Finding an alternative to provide automated data update directly to google data studio for iOS. 5 Updates done to all the dashboards 6 Created new dashboards 7 Created a consolidated dashboard 8 Added required visualizations and conected to data sources 9 Created new data sources 10 Managing the consolidated dashboard with daily data monitoring 11 Funnel Report for consolidated dahboard 12 Google analytics installed on website through tag manager 13 Resolving errors 14 work on automation for ad acccounts 15 Developed a new dashboard 16 Ad accounts data Automated 17 work towards android data automation 18 altering of blended data joins as per gds updates 19 Personalisation of dashboards 20 Current dashboard updated with google events and widget changes 21 Added Apple search ads dashboard 22 Firebase funnel report dashboard developed 23 Card topups Funnel report dashboard developed 24 Porter metrics custom dashboards for trial 25 Registration firebase funnel and percentage added 26 Updates for all the dashboards running until now and addition of kpi to the new firebase dashboards 27 User info for firebase dashboard and retention report 28 Registration Funnel, Cardtopups, KYC funnel Dashboard 29 Fixing and Updating user info firebase dashboard and began working on the tiktok dashboard 30 Tiktok Dashboard Developed and populated with data from porter metrics Tools used Google Data Studio Google Analytics- GA4 and universal analytics Google Tag Manager Big Query Firebase Appsflyer Mixpanel Google spreadsheets Language/techniques used Google Standard SQL dialect- bigquery Apps script Skills used Analytical aptitude Problem-solving Communication Knowledge about SQL Knowledge in digital marketing and strategies Google cloud services Creating data pipelines. Databases used Bigquery Google spreadsheets Firebase Web Cloud Servers used Google Cloud Platform What are the technical Challenges Faced during Project Execution Community/in-built Connectors for Appstore connect didn’t exist Connector for apple search ads couldn’t be found Data tracking from google play console, due to the timezone lag in data updation. Facebook connector issues How the Technical Challenges were Solved Worked towards building the custom connector by using apple api for Appstore connect and search ads Utilised big query to call and store 100% accurate data from google play console and be used as a connector in GDS Made use of inhouse built facebook connector and google sheet add-on to track and keep connector inaccuracy check. Business Impact Helped the client to view a consolidated report of all their ad campaigns Calculated and executed analytics metrics which helped to track various app events and helped the business to take decisions on UX Consulted the client and collaborated with them in marketing and ad campaign strategies- helped them cut their marketing expenses over less efficient marketing platforms Created funnel reports and suggested insights on app traffic to take decisions on important landing pages. Project Snapshots Project website URL https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",37.42000660283923,-10,42.70386266094421,8,1,17,0.059080962671595266,2.4184549356223175,https://insights.blackcoffer.com/ad-networks-marketing-campaign-data-dashboard-in-looker-google-data-studio/,466
42.8,42.8,6.53074433656958,127,"design and develop mlops framework for data-centric ai Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem The task involves finding models and tools for several different tasks across various domains. The tasks include video and image capturing, working with documents such as PDF and Excel files, converting text to audio, audio capturing and transcription, translation to major languages, utilizing language models with a focus on Jina finetuner and its limitations, creative AI for generating pictures and designs, synthesizing language texts, creating Kibana dashboards and data storytelling, code creation for specific platforms like Editorjs and Nextjs, integrating Jina API inference into function blocks in Editorjs/Nextjs, UX/UI creation for the front end of Editorjs and Nextjs, transfer learning and reinforcement learning, utilizing Wikipedia for general knowledge, and utilizing an epistemic model called EPINET. To fulfill this task, you will need to search for relevant models, tools, and resources specific to each task mentioned above. Our Solution Jina AI Hub to deliver an ecosystem of: Core transformer model Distilled & Fine tuned models OKR:s/KPI:s +domain data = “Book of knowledge” + model = AI agents Ensembled models = AI teams Also delivered functions in the marketplace Voice interface, OpenAI Whisper transformer Multiple data types capturing of information (DocArray) CLIP model to mesh multiple data types into vectors Neural Search function and Generative AI function Automatic data labelling Used weight watcher to fine tune the model quality without CPU/GPU cost Solution Architecture Automatic selection a model for fine tuning with data corpus (book of knowledge), given the best performance. Add the model to an API inference Unlike ChatGPT the model can specify when they don’t know and acknowledge it instead of making stuff up with its creative ability. When the model knows what it doesn’t know, it can ask to go back and consult other models for joint predictions. Add a function to select ensembled models for joint prediction when step 4 occurs. Deliverables Identify core transformer models “Clean” and stabilize selected core models Set up the process in Jina Hub Integrate FastAPI/Jina with our Jina Hub Integrate FastAPI/Argilla/Kibana into our Jina Hub Tools used Jina Hub/AI, Python, Hugging Face, Argilla, Redis stack, Kibana Language/techniques used Python Models used Epistemic Neural Nets, weight watcher, OpenAI Whisper transformer, Epinet Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",33.56012944983819,-3,41.10032362459547,14,1,15,0.06716417885386501,2.4174757281553396,https://insights.blackcoffer.com/design-and-develop-mlops-framework-for-data-centric-ai/,309
21.060606060606062,21.060606060606062,6.657534246575342,142,"ai and nlp-based solutions to automate data discovery for venture capital and private equity principals Client Background Client: A leading Venture Capital and Private Equity Principals in the Globe Industry Type: Venture Capital and Private Equity Principals Services: Private Equity, Venture Capital, Data Analysis, Fund Performance, Alternative Assets, Competitive Intelligence, Limited Partners, Customized Benchmarks, Service Providers, Fund of Funds, M&A, and Financial Services Organization Size: 100+ The Problem Extract funding-related data from news articles (from 1000+ websites) such as company name, funded amount, participated investors, and other details. create a web app to manage the extraction of funding data Our Solution There were 1000+ websites from funding-related articles so we couldn’t make a crawler for each website. So we used an inbuilt web crawler provided by elasticsearch. When we have extracted articles then we need to extract funding related information company name, fund amount and investors participated etc. Then we decided to use NLP’s question-answering method in which we need to train transformers to extract funding-related information. First we have created some keywords based approaches to create labels for each field we need to extract to train models. After that we have trained distil bert model on labelled data on AWS EC2’s GPU server. We applied this approach for all the fields we need to extract. We got 90%+ accuracy for the company name field and for other fields we got 80%+ accuracy. To manage and view all the fields of extracted funding data we created a web app using python flask. In this we created several pages to show extracted raw data by crawler, cleaned data after applying some cleaning functions and final output which have all the fields. We also created admin dashboard pages to show daily crawling status, how many articles processed in one day, total final output etc. Solution Architecture Deliverables Flask Web app Elasticsearch crawler Tools used Flask, Spacy, NLTK, pandas, numpy, transformers, elasticsearch etc. Language/techniques used Question answering in NLP, web scraping, web application Flask, Python Models used Distil-bert model, en-core-web-sm (pre trained model of spacy) Skills used NLP, Data Analysis, Flask web app, Pandas, Numpy, transformers, fastapi, elasticsearch etc. Databases used Elasticsearch database Web Cloud Servers used AWS What are the technical Challenges Faced during Project Execution The client wanted to extract data from 1000+ different websites and if we make any crawler it only works for one website so it was not possible to create a 1000+ web crawler. How to extract funding information from an article. It is very difficult to extract that type of information from normal python code by defining keywords because every website has different types of articles. How the Technical Challenges were Solved To solve web crawler-related issues we used elasticsearch web crawler which is very fast and can extract multiple websites at a time. In this we need to create an engine and add websites that we want to scrape. After that we added some keywords to extract only funding-related articles. We set up this crawler to run every hour so we can get new articles every hour. To extract funding-related information we collected articles from different websites and created labels for each field we wanted to extract. After that we have fine-tuned the transformer’s Distil-bert model on our labeled data. We used these models to extract funding-related information. We also created an automated python script that uses these model on every extracted article and extracts funding-related information. Business Impact This funding-related data would be used in two ways. From this project, companies can find suitable investors for their startups. Companies can search for investors based on industry, verticals, etc., and find investors to help their startups. Investors can use it to find a startup in which they want to invest based on their preferences like industry, verticals, etc. Project Snapshots (Minimum 10 Pictures) Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",21.39227895392279,-6,32.42009132420091,41,1,11,0.04415584404115365,2.221461187214612,https://insights.blackcoffer.com/ai-and-nlp-based-solutions-to-automate-data-discovery-for-venture-capital-and-private-equity-principals/,438
14.533333333333333,14.533333333333333,6.707865168539326,113,"equity waterfalls model-based saas application for real estate sector Client Background Client: A leading real estate firm in the USA Industry Type: real estate Services: Property business, investment, real estate Organization Size: 100+ Project Objective The objective is to create software that will calculate the equity waterfalls for different cases. And there should be 3 users admin, sponsor and investor. We need to create the equity waterfall calculation according to the csv file that is shared by the client. All users have their own UI portal. Project Description The project is created using python language, working on django rest framework and for frontend we use reactjs and the code deployed on google cloud app engine service. We need to create a software that will calculate the equity waterfalls. And there should be 3 users admin, sponsor and investor. We need to create the calculation according to the csv file that is shared by the client. All users should have their own UI portal. Sponsors can create deals and send deal invitations to all investors or specific investors. Investors can see all the deals that are offered by the sponsor’s. After that Investors can subscribe that deal after subscription it is depending on sponsor that he will accept the investor subscription or not. Our Solution We have created api’s that will calculate the equity waterfall calculation according to the selection of the waterfall tiers. Project Deliverables Django rest framework api’s with frontend. Github source code. Working UI. Tools used Views. Routers. Serializers. Serializer relations. Settings. Language/techniques used Python Django rest framework ReactJS JWT SMTP Skills used SMTP JWT Databases used Sqlite3 Database Web Cloud Servers used Google cloud platform What are the technical Challenges Faced during Project Execution The technical issues faced during the project is how to calculate the equity waterfall calculation for different tiers and different cases. And also invite the sponsors by admin or sponsors invite their investors. How the Technical Challenges were Solved We have used conditional statements in code and write different codes for different calculations. so that it will check which case we need to run and it will run accordingly. Added the functionality in which admin can invite the sponsors to the website and sponsors can invite their investor through sending the invitation link to their email. Project Snapshots Project website url https://stackshares.io/dashboard/add-new-deal Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",22.742172284644194,-5,42.32209737827715,20,1,5,0.04329004310264051,2.400749063670412,https://insights.blackcoffer.com/equity-waterfalls-model-based-saas-application-for-real-estate-sector/,267
26.1875,26.1875,6.6286764705882355,87,"ai solutions for foreign exchange – an automated algo trading tool Client Background Client: A leading tech firm in the USA Industry Type: Financial Services Services: Trading, consulting, financial serivices Organization Size: 100+ The Problem Our main objective in this project was to help with setting up with given Broker API using MT4 and extracting historical data from it, and solving different tasks which are related to extracting important values from the data. And tasks assigned by the client were related to working around the data, i.e. formatting, connecting with the IG trade broker, automating the Python script and scheduling the script accordingly. Our Solution During the initial phase, we were assigned to set up an MT4 with given Broker API access to extract historical prices, which was delivered to the client. In the second phase, the client requested to implement Profit/Loss, Spread Direction and Time in Trade. There were minute tasks related to the R script, which was duly completed. In the third phase, the client was assigned a task related to distinguishing the tickers according to cluster types which he provided and implemented code to distinguish the sell and buy spread for the given STD. In the fourth phase, I implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code and worked on retrieving Historical prices from another Broker API and retrieving Watchlist given attributes by the client. Automated the Python script to retrieve yesterday’s market price of the given list Deliverables Successfully delivered set-up in MT4 for retrieving historical prices, Created logic for automating the profit and loss, Implemented code to distinguish the tickers according to the cluster type, Implemented code for distinguish the sell and buy spread for the given STD, Implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code. Automated the Python script to retrieve yesterday’s market price. Tools used MT4, Jupyter Notebook, Excel, IG trade, Remote Desktop setup Language/techniques used MQL, Python, R Skills used Critical thinking, Logical Thinking What are the technical Challenges Faced during Project Execution? While setting up MT4 platform and its configurations How the Technical Challenges were Solved The above-mentioned challenges were resolved after many hours of effort and understanding. Project Snapshots Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",23.269117647058824,-5,31.985294117647058,11,1,10,0.06097560950822923,2.1911764705882355,https://insights.blackcoffer.com/ai-solutions-for-foreign-exchange-an-automated-algo-trading-tool/,272
29.75,29.75,6.813664596273292,60,"ai agent development and deployment in jina ai Client Background Client: A leading tech firm in Europe Industry Type: IT Services: IT and Consulting Organization Size: 100+ The Problem The client’s object was to create AI agents for his website, which the end-users will utilize for many tasks. The client had some recommendations on the models are utilized. Our Solution Created a feasible models list that complements the client’s requirement and when ahead and executed the Executor code for every model for compatibility with JinaAI deployment. After implementing Executor codes, I created a Flow to connect every executor and deployed it successfully. Deliverables Successfully delivered executable deployed models in Jina Ai Tools used Jina AI, VSCode, HuggingFace Language/techniques used Python Models used Whisper, Stable Diffusion, GPT3, Codex, YOLO, CoquiAI, PDF Segmentor Skills used Python, Model APIs Databases used JinaAI Cloud What are the technical Challenges Faced during Project Execution There were minute challenges, such as deployment issues and Execution issues How the Technical Challenges were Solved I resolved the issues effectively after long hours of understanding the concept because JinaAI is a new growing technology that does not have many forums to solve errors and issues. Project Snapshots Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",26.806832298136648,-9,37.267080745341616,10,1,12,0.15441176357050174,2.4285714285714284,https://insights.blackcoffer.com/ai-agent-development-and-deployment-in-jina-ai/,161
13.225806451612904,13.225806451612904,6.630081300813008,98,"data warehouse to google data studio (looker) dashboard Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products, healthcare, government, energy Organization Size: 100+ The Problem Our client needed a Google Data Studio dashboard for different sectors such as Oil and Gas, Government, Healthcare, and Sales analysis. They want to see an analysis of data from which they can provide insights in different domains. They want us to create visual KPIs of meaningful insights. Our Solution They provided us with data for different sectors. Using those data first we analyze the data and perform EDA on data for cleaning the data. After cleaning the data, we performed calculations to extract insights for KPIs. Using those KPIs we build a dashboard on Oil and Gas, Government, Healthcare, and Sales analysis. Solution Architecture To build the dashboard we follow the pipeline as follows: Data 🡪 EDA(Cleaning data )🡪 Connection(GDS) 🡪 Building KPIs(Visuals) Tools used Google Data Studio Skills used EDA, Google data studio What are the technical Challenges Faced during Project Execution During the project execution, we faced the following challenges: The data client provided was not cleaned. Data was of four different sector which we have to analyse and visualize. Extracting insights from data. How the Technical Challenges were Solved To solve the technical challenges, we provided following solutions as follow: Performed EDA on data to clean it and find the missing values. As data was from different domains, we have analysed each sector and understand the culture of each domain. We understand the pipeline and flow of work process. After completing the case study, we use calculations to extract the meaningful insights from data. Business Impact Using these dashboards client can visualize the sales insights and understand the workflow. They can take crucial decisions based on these insights which will help them to make an impact on their sales. Project Snapshots Sales Dashboard: Government Dashboard: Oil and Gas Dashboard: Hospital Analysis: Project website url Dashboards on Google Data Studio: 1.Government:- https://datastudio.google.com/reporting/dda94ce8-5b77-46aa-a1e0-1a57ccaef5f9 2.Oil:- https://datastudio.google.com/reporting/47c6529e-1355-4072-babf-1a96f9f842cf 3.Healthcare:- https://datastudio.google.com/reporting/b1e95a11-4380-465c-ad45-2d1995c799fb 4.Sales:- https://datastudio.google.com/reporting/36ec0e42-6b77-4fbb-9dea-760cccaa741f Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",21.22528193023866,-2,39.83739837398374,24,1,9,0.05069124400603114,2.3780487804878048,https://insights.blackcoffer.com/data-warehouse-to-google-data-studio-looker-dashboard/,246
22.363636363636363,22.363636363636363,6.410774410774411,96,"golden record – a knowledge graph database approach to unfold discovery using neo4j Client Background Client: A leading retail firm in the USA Industry Type: Retail Services: Retail business, consumer services Organization Size: 100+ The Problem To use data ingested into Neo4j and use the nodes and relationships with its properties to determine which nodes are actually the same person. For eg: we have Person nodes in the data, now people might enter their names in different ways. Our main aim is to identify Person nodes that may have similar data and are actually the same person. This will be represented as a perfect match between the nodes. This single-person view is referred to as the Golden Record Our Solution Till date, we have loaded data into Neo4j and created relationships with score property which defines match strength. We have created some criterias by which we can determine what constitutes two nodes being the same and then based on them created ‘perfect match’ and ‘probable match’. We have considered four properties for our criteria – full name, address, driver’s license, and passport number. We have relationships between nodes for these properties with scores, we use these in our perfect match and probable match creation. We have also configured Graphlytics (a viz software) in the virtual machine which connects to the neo4j database and helps vizualize the nodes and relationships. We have also worked on some algorithms using the GDS library in neo4j to produce more information on the graph, the common neighbors algorithm was used to produce scores based on node similarity and the higher the score the higher the similarity. Other algorithms were tried as well but since all the properties are of String format it did not work on it. We have Resolved issues neo4j is facing when deleting a Large set of data and Provided steps to recover neo4j if it fails by going OutofMemory. We have figured out the issues with the probable and perfect match cypher queries not working as intended and proposed a solution. Solution Architecture Deliverables Created Perfect match and probable match queries. Created queries that return the nodes (even if it does not have associated relationship) and it’s associated relationship. A cypher query that return the result as a json object that can be mapped into a java oject. A cypher query that will create the relationship if two node’s properties have same value. A cypher query that will delete one relationship from bidirectional relationship. A python code for a sample neo4j query Adjust the perfect and probable match queries so it would work for current data. Tools used Neo4j Language/techniques used Cypher Query Language Models used The common neighbors algorithm Skills used CQL Databases used Neo4j Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",21.87474747474748,-6,32.323232323232325,27,1,18,0.095999999616,2.2154882154882154,https://insights.blackcoffer.com/golden-record-a-knowledge-graph-database-approach-to-unfold-discovery-using-neo4j/,297
24.333333333333332,24.333333333333332,6.708074534161491,138,"google lsa ads (google local service ads) – etl tools and dashboards Client Background Client: A leading marketing firm in the USA Industry Type: Marketing Services: Ads, Marketing, Campaign, Consulting Organization Size: 200+ The Problem The client has a Google LSA Ads Manager Account with about 100+ accounts and wishes to collect data available through the Google LSA API daily. The client wishes to set up a private Databases that is automatically created for newly added accounts and stores all of the collected data (Lead and Phone Call data). Finally, all collected data must be presented through the Google Looker Studio Dashboards, with the design layouts as suggested by the client. Our Solution The solution involves a number of Python-based ETL tools that are responsible for fetching the data from Google’s LSA API daily and updating the same in the Google BigQuery Databases. Two different tools run are: MCC Data Fetching tool. Lead Record data fetching tool. The fetched data is stored in BigQuery Databases on the client-provided (Google)manager account. Carefully curated Google Looker Studio dashboards implemented with client-suggested theme layout which are updated upon client request, represent a number of KPIs and graphs indicating major data trends. The designed dashboards have a number of data-controlling filters that filter the data account-wise and date-wise. Solution Architecture Deliverables Heroku deployed Python tools Google Looker Studio Dashboards BigQuery Database Maintenance service Tools used Python Google BigQuery Heroku Google Looker Studio Git Heroku CLI Language/techniques used Python GoogleSQL (BigQuery supported SQL) Looker Modeling Language (Looker ML) Git Commands Skills used Data Engineering skill to fetch data as per client needs. Data Processing to make it suitable for dashboards, databases Dashboard designing and data presentation skills Tool Deployment Database manipulation Data piplining Databases used Google BigQuery Web Cloud Servers used Heroku: Cloud Application Platform What are the technical Challenges Faced during Project Execution Google LSA API is slow, high data fetching timelines. BigQuery jobs fail, causing inconsistencies. How the Technical Challenges were Solved Entire data fetching operation requires 1-2 hrs daily, 2 separate tools run in asynchronously and populate two different databases, the data is grouped in the dashboards Regular weekly and monthly data refreshes update any inconsistent data. Business Impact Business clients are able to access important KPI’s without the need to understand the complexities involved behind the scenes. Allows clients to track their performances, responsiveness. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",26.876190476190477,-9,42.857142857142854,6,1,12,0.0714285711856171,2.3850931677018634,https://insights.blackcoffer.com/google-lsa-ads-google-local-service-ads-etl-tools-and-dashboards/,322
20.545454545454547,20.545454545454547,6.9423076923076925,69,"create a knowledge graph to provide real-time analytics, recommendations, and a single source of truth Client Background Client: A leading tech firm in the USA Industry Type: Retail Services: Retail Business Organization Size: 100+ The Problem The Client was using NoSql Database which was slow and did not provide real-time response for complex queries. The data had many Connections and it was difficult to represent them in NoSQL or Relational Databases. Our Solution Create a Knowledge Graph and Provide Real-time Analytics and Recommendations using Machine Learning. Solution Architecture Neo4j was Installed on a Cloud VM based on Linodes. Deliverables Knowledge graphs and Data Pipelines are used to Populate the Graph. API’s to Perform CRUD operations in real-time. Tools used Neo4j Postman Language/techniques used Python JSON Models used Node-Relationship model Skills used Programming Data Engineering Data Analytics Databases used Neo4j Web Cloud Servers used Linode What are the technical Challenges Faced during Project Execution Integration of Firestore with Neo4j without any native integration method or driver. How the Technical Challenges were Solved The challenge was solved by using api to retrieve data from Firestore. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",25.910489510489512,-7,44.230769230769226,6,1,7,0.102189780275987,2.5128205128205128,https://insights.blackcoffer.com/create-a-knowledge-graph-to-provide-real-time-analytics-recommendations-and-a-single-source-of-truth/,156
15.144736842105264,15.144736842105264,6.62982689747004,268,"data management for a political saas application Client Background Client: A leading tech firm in the USA Industry Type: IT Services: SaaS, Products Organization Size: 100+ The Problem As per the guidelines and discussion. Political Research Automated Data Acquisition (PRADA) in the following phases which included. 1. Get pics for existing EOs (Elected Officials) 2. Get new EOs and Pictures. 3. Run QA checks regularly on EOs 4. Get data from government Facebook pages. 5. Geospatial project: Create a new version of provided KML without using google earth. Creating a nested directories which contained description and Map-URL at the designated location. 6. Get data of US States and Counties(Including Boroughs and Parishes) By building an automated generated structured data that allows a non – programmer to create a config for each page allowing a bot to scrap and update the data. Our Solution We created an automated python scripts for designated phases with respective requirements. Solutions to various type of problems varied such as most of data scrapping automation was done through python developed scripts including the geospatial KML task. In addition to this different ranges of data was scrapped generated directed output for the respective tasks in the form of CSV format. So the user’s main aim requirement was achieved i.e. a non programmer could create a con-fig and initiate a bot to scrap the required data. Solution Architecture The majority task of project consisted of web data scraping automation so a high- level overview, and specific implementation details of project shall will be as follows: Web Scraping Framework : Python as a coding language was used in almost all of the tasks and the framework used for data scraping included Beautiful-Soup, Selenium and Web drivers. These libraries provide tools and functionalities to navigate web pages, extract data, and handle various HTML elements. Data Extraction and Parsing: Use the selected web scraping library to extract the desired data from the web pages provided either in the data sheet or within the websites of URLs given in the sheet. This involves locating HTML elements, applying filters or selectors, and parsing the extracted data. Data Processing: Followed by data extraction it was cleansed, transformed and aggregated to a structured form such as pandas’ Data Frame followed by a CSV file. In the case of geospatial task it resulted to generation of nested folders in a kml file. Data Storage: The how and where to store the scrapped data was determined which is local file system in the form of CSV (Comma Separated Values). As it was the appropriate data storage solution according to need of the project. In addition to this the geospatial task had the output in the form of kml file as polygons inside directories of nested folders. Deliverables Tasks Outputs (CSV/KML/XLSX) Python Scripts Canada EOs mydata.csv Script1.py Script2.py Geospatial Task Electoral Districts.kml – Facebook Scrapping of EOs EO_OUTPUT_O.csv final_eo_scrapping.py Facebook Scrapping of 429 Cities Output_DRAFT_429_CITIES.csv Facebook_image_scrapping.py USA States Website URLs ScreenScrapingt.csv final_50_states_scrapping.py USA Counties Website URLs US Website_final_write.xlsx county_scrapping.py Tools used Python ( Programming Language ) Beautiful Soup Selenium Pandas Numpy Simplekml re (regular expressions) Language/techniques used Python ( Programming Language ) – It is an interpreted language, which allows quick prototyping and interactive coding. Its versatility can be is one of the reasons for its major applications. Different libraries and tools were used in this project for various data solutions. Beautiful Soup – A python library used for web scraping and parsing HTML and XML documents. It provides a convenient way to extract from the said files. It eases out the work flow from parsing to data extraction and encoding handling as well. Selenium – A python library used for web browser automation like Chrome, Firefox, Safari and others. It interacts with elements such as clicking buttons, filling out forms and selecting drop down options. In this project we used it in Chrome. Selenium web driver was used for web automation. It acted as bridge between Python code and the Web browser. Pandas – It is Python’s versatile library that provides high performance data structure tools and it is built on top of Numpy. Data Frame is one of its key feature due to which this library was used. This key feature allows efficient manipulation, slicing, and filtering of structured data Numpy – It is also a python library aka Numerical Python as it is a fundamental library for scientific computing in Python. Simplekml – It is a python package which enables you to generate KML with as little effort as possible. re ( Regular Expressions): It is a powerful tool in python sued for pattern matching and manipulations of strings. Skills used Python Programming Web Fundamentals Web Scrapping using libraries such as BS, Selenium. Data Cleaning and Processing Problem- Solving and debugging. KML structure and handling using Python’s programming. Databases used None. All the structured data was in the form of either python Data Frames, CSV or Excel Sheets. What are the technical Challenges Faced during Project Execution Firstly some of the web URLs were not accessible because they were restricted to particular range of IPs of that region Couldn’t fetch whole data through Beautiful Soup as it couldn’t parse whole tags. List of US Counties wasn’t provided in the given resource links How the Technical Challenges were Solved Used VPN for accessing Official sites which were not generally accessible. Used Selenium Web driver to automate the direction at URLs which fetched complete html tags of the desired webpages. Performed a search and created structure data of list of counties of each state which was used as input to gain web URLs of counties of US. Business Impact Enhanced Analysis : Web scraping allows businesses to gather valuable data from various websites. This information can provide insights to desired aim and objectives enabling businesses to make informed. Real-Time Monitoring and Upgradation: Web scraping can enable business to monitor changes or updates on website in real-time. This can be useful for tracking regulatory changes. It keeps the business and it’s data updated. Increased Efficiency: Automation eliminated the need for manual data collection, saving time and resources. With automated web scraping, business can extract large amount data quickly, accurately, improving overall operational efficiency. Project Snapshots Chrome driver initiated Chrome driver visiting the directed links and accessing the image URLs Directed to next link KML task Facebook Data extraction Data of State Governments of US Accessing links through wiki directing to counties Nesting within the list of counties of a particular state Finding and Extracting link of the website of County Project website url The GitHub repository link:- https://github.com/AjayBidyarthy/Paul-Andr-Savoie/tree/main Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",20.332195668932652,-9,35.685752330226364,31,1,28,0.056661561934668356,2.307589880159787,https://insights.blackcoffer.com/data-management-for-a-political-saas-application/,751
29.454545454545453,29.454545454545453,6.454166666666667,92,"ai solution for a technology, information and internet firm Client Background Client: A leading Technology, Information and Internet firm in India Industry Type: IT Services: Emerging Technologies, 2030, and 2050 Organization Size: 10+ The Problem The objective was to analyze, research, and propose data science solutions in the product based on the product design, use cases, and services. Our Solution Analyze each use case Analyze product design Analyze user type, controls per use cases For each use case and available product design, provide solution or scope of the data science capabilities List attributes needed in each of the product design screens List use cases are driven by the data For each data-driven use cases a. Research and design the data science solution b. List needed data c. List process d. List models e. List solution Help product design team with data science use cases Help product design team with data science solutions for each use case Deliverables Statement of Work (SoW) with a solution documentation Data science use cases document Data science solution for each use cases document Data Science methodology, algorithms needed, models, recommended and more in a good documentation Tools used Google docs Microsoft word Draw.io Excel Google Draw Language/techniques Python- Flask Models used K-Nearest Neighbours K-Means Clustering NLTK DeepAvlov Spacy Texttiling Eclat LSTM Skills used Aptitude for functionalities Problem-solving Communication Data Modelling Data Pipelining MLOps NLP Recommender systems Databases used Amazon S3 Web Cloud Servers used AWS EC2 Business Impact Collaboration with the client to identify the scope and use cases for the platform Cost Effective approach taken to document solutions Regressive R&D to find and document third-party solutions for certain use cases- saving cost and time. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",27.115151515151513,-4,38.333333333333336,5,1,11,0.07109004705644527,2.3625,https://insights.blackcoffer.com/ai-solution-for-a-technology-information-and-internet-firm/,240
15.074074074074074,15.074074074074074,6.17948717948718,77,"crm, monday.com via zapier to power bi dashboard Client Background Client: A leading solar panel firm in the USA Industry Type: Energy Services: Solar Panel Organization Size: 200+ Project Description Mohsin has Solar Panel Company. He has setup CRMs for that. He wanted to use CRMs data and want to visualize the leads in PowerBI Our Solution First, we check CRMs thoroughly and understand the work culture of his company. It was not easy to fetch data into PowerBI using API key. To fetch new leads from CRMs we used Zapier. The limitation of Zapier is it cannot fetch historical data into spreadsheet. So we download data from CRMs and fetch it into spreadsheet. For new leads we created zaps for every instance. After that we connect the spreadsheet with PowerBI and clean the data accordingly. Using that data, we build KPIs according to client need. Tools used API , Zapier , Spreadsheet , PowerBI Language/techniques used M language , DAX Skills used API , M language , DAX , PowerBI What are the technical Challenges Faced during Project Execution? First challenge was to fetch data from CRMs using API key. Data we were getting was uncleaned and were not able to fetch all data. If there were multiple pages in the CRMs we will not be able to fetch all data from the pages. How the Technical Challenges were Solved Technical challenge in this project was to extract data from CRMs. So for that we used Zapier connector from CRMs to spreadsheet. But there was some limitation with Zapier that it will not fetch the historical data of our CRMs. So to solve that we download all historical data from CRMs and append it to the spreadsheet we were using. We fetch new leads to our spreadsheet using Zapier. By doing this now we have all the data historical and new lead which will be pushed by Zapier. Then we fetch the data to our PowerBI and do some cleaning in data. By using cleaned data, we build the KPIs for our client according to there requirements. Business Impact Client will be able keep track on his company data on PowerBI and it helps them to make decisions accordingly. Project Snapshots CRMs Zapier PowerBI Dashboard Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",19.192022792022794,-4,32.9059829059829,34,1,13,0.08095238056689343,2.1666666666666665,https://insights.blackcoffer.com/crm-monday-com-via-zapier-to-power-bi-dashboard/,234
23.555555555555557,23.555555555555557,6.9375,53,"advanced ai for pedestrian crossing safety Client Background Client: A leading tech firm in the Middle East Industry Type: Security Services: Security services Organization Size: 100+ The Problem Traffic Signals are inefficient because even if there are no cars or no pedestrians on the road it still works on a timer and stops the traffic or pedestrian unnecessarily. Our Solution We provide a Computer vision-logic to Manipulate the traffic signal to work such that it turns red only when x number of pedestrians are waiting to cross the signal. Solution Architecture Yolov7 pose estimation Opencv Deliverables The program Detects Pedestrians and Gives alerts to traffic Signals to turn Red or stay Green. Yolov7 pose model weights Tools used Yolov7 Opencv Language/techniques used Python Computer Vision Models used Yolov7 Pose Estimation Skills used Programming Computer Vision Deep Learning What are the technical Challenges Faced during Project Execution There was no existing solution and we had to create the logic from scratch. How the Technical Challenges were Solved Researching Computer Vision. Learning new Techniques and Experimentation. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",24.144444444444446,-5,36.80555555555556,8,1,8,0.10236220391840785,2.4166666666666665,https://insights.blackcoffer.com/advanced-ai-for-pedestrian-crossing-safety/,144
23.5,23.5,6.080779944289693,104,"an etl solution for an internet publishing firm Client Background Client: A leading internet publishing firm in Singapore and Australia Industry Type: Internet Publishing Services: peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value Organization Size: 100+ Project Objective Fetch all call logs using zendesk api from drivelah server Analyse call logs and number of calls made by a particular phone number to company and fetch recent call timing Project Description We need to fetch last month’s call details (from user, to user, call_time, call_status ) using zendesk api. Then we need to analyse all call logs and need to identify the number of calls made by a particular user to the company and the most recent call timing from the company server. Our Solution To fetch all call logs using zendesk api we used python language in programming. When we checked call details in the zendesk api, the details were in json format which is very tough to understand the calls details. So first we have fetched only needed details (call made from person, to person and call timing) converted into tabular format. In tabular format it was easy to identify call details. After that we need to identify the number of calls made by the user to the company in the last month. We used the python pandas module here which is very fast and effective to handle tabular data. First we separated the user who made a call to the company last month and then counted each unique user’s call records. For recent dates we used python’s datetime module which can easily identify recent date time. Project Deliverables 2 python scripts for fetching call details and converting into table format for identifying number of calls made and recent call timing Tools used VS Code, Google Drive, and MS Excel. Language/techniques used Python programming language, Data Analytics with numpy and pandas, python datetime. Skills used Data Analytics,, Python, Mathematics Databases used local data from MS Excel Sheet What are the technical Challenges Faced during Project Execution First one was the api data in json format with other unwanted data so it was a little difficult for us to identify the number of calls and other information from direct json data. The date format in the api data is not appropriate for us to handle. Because the date is stored in string format, it was difficult to compare dates with one another and identify recent ones. How the Technical Challenges were Solved For the first technical challenge we first took only useful details from api’s json format and converted these details in tabular format. In python we can easily handle tables with pandas dataframe and can apply whatever operation we want to collect details. For the second one we know that it would be difficult to handle dates in string format. So we first converted dates to a proper datetime format using python’s datetime module. It has a lot of built in functionalities which can easily compare dates with one another. So from comparison we have identified recent dates of calls. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",20.98774373259053,-5,28.969359331476323,25,1,16,0.07266435961015792,2.0891364902506964,https://insights.blackcoffer.com/an-etl-solution-for-an-internet-publishing-firm/,359
16.0,16.0,7.089171974522293,72,"using graph technology to create single customer view. Client Background Client: A leading retail firm in Newzealand Industry Type: Retail Services: Retail business Organization Size: 100+ The Problem Companies face issue of having a Single customer under various rows with slightly different information in the same database. This causes unwanted duplication and inaccurate statistics. It also results in inaccurate ad targeting and financial loss. Our Solution We leverage graph technology to create a single customer view by using Complex cypher queries and Graph Algorithms. Solution Architecture We have an Azure VM on which we have installed the Neo4j Database. Deployment architecture is a single Instance because of using the Community version of the software. Deliverables Populated Neo4j Database. Required Cypher Queries. Tools used Neo4j Graphlytics Language/techniques used Java Cypher Query Models used Node-Relationship model Skills used Data Analytics Data Engineering Data Science Databases used Neo4j Web Cloud Servers used AZURE What are the technical Challenges Faced during Project Execution Only 1 Difficulty was faced in this Project and that was to migrate data from Elasticsearch to Neo4j. How the Technical Challenges were Solved Research and Experimentation. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",24.74394904458599,-10,45.85987261146497,8,-1,6,0.11347517650017605,2.56687898089172,https://insights.blackcoffer.com/using-graph-technology-to-create-single-customer-view/,157
18.4375,18.4375,5.959568733153639,97,"ai-based algorithmic trading bot for forex Client Background Client: A leading trading firm in the USA Industry Type: Finance Services: Trading, Banking, Investment Organization Size: 100+ The Problem Build ML/AI Model to predict next 15 min EMA cross on historical and live data by using indicators such as EMA, MACD, RSI etc. Create a web app to show predicted EMA cross and other indicators movement Our Solution In stock market indicators such as EMA, MACD, RSI etc helps us to find cross by using historical price data. If we accurately predict cross earlier then it will help us in investment. So we have used 12data api to collect historical and live EUR/USD price data. We calculated EMA(12), EMA(26), MACD and RSI indicators based on price data. After that we created labels of ema cross in historical data. When we have training data we used different classifier models for training. We predicted accuracy with different models and the Logistic regression model gave 91% accuracy. This logistic regression is predicting the cross only for the next step. It means we will know only 15 minutes before that the cross will happen in the next 15 min but we need to know more earlier. For that we predicted the next 45 minutes price values using the LSTM model from historical price data. Based on these price values we have calculated EMA, MACD and RSI and after that cross using logistic regression. So now we can predict the cross 1 hour earlier based on these 2 models. To show cross and other indicators movement we created a python flask web app and hosted it on AWS EC2 server. The process runs every 15 minutes and checks the cross. If there is any cross in 1 hour it sends a telegram notification. Deliverables Flask web app All the python code and machine learning models Tools used Pandas, numpy, scikit-learn, tensorflow, flask etc. Language/techniques used Data Analysis, Data Visualization, Machine learning, Deep learning, flask web app etc. Models used Logistic Regression, LSTM model Skills used Data Analysis, Data Visualization, Machine learning, Deep learning, flask, python etc. Databases used MongoDB Web Cloud Servers used AWS Ec2 What are the technical Challenges Faced during Project Execution Main challenge in this project is to find the best model. Because we have time series data so we cannot change the orders to get better accuracy. One machine learning model is only predicting the next 15 min cross but we need the ema cross 1 hour before. How the Technical Challenges were Solved We were using time series data so we cannot change the order to find better accuracy in every model. So we have tried different models with the same order and evaluated the model. Only the logistic regression model worked best for the data it gave 91% accuracy on test data. To get the next 1 hour prediction we first tried the same logistic regression to predict the next 3 steps but we failed because of poor accuracy. So we trained the LSTM model on price data and predicted the next 3 steps using the LSTM model. After that we used logistic regression to predict ema cross. Business Impact It will help traders to predict the stock market earlier and get better returns from this project. Project Snapshots Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",17.833221024258762,-12,26.1455525606469,33,1,12,0.07973421900420526,2.0673854447439353,https://insights.blackcoffer.com/ai-based-algorithmic-trading-bot-for-forex/,371
17.347826086956523,17.347826086956523,6.654008438818566,79,"car detection in satellite images Client Background Client: A leading retail firm in the USA Industry Type: Retail Services: Retail business Organization Size: 100+ Project Objective The objective of this project was to detect cars in satellite images and highlight them using a bounding box. Project Description The client, Steffen Schneider, approached us with a requirement to develop a Python project that dealt in the field of computer vision. The main aim of the project was to detect cars present in a satellite image and highlight them using a bounding box. To achieve this, we decided to use the Darknet model and train it on Yolov4 dataset of cars in satellite images. Our Solution We used Google Colab for coding and training the Darknet model. Kaggle was used to download the Yolov4 dataset of cars in satellite images. We preprocessed the dataset and trained the model on it. Once the model was trained, we tested it on sample satellite images and it worked perfectly fine. Finally, we created a script that detected the cars in an image and highlighted them using a bounding box. Project Deliverables The final deliverable was a ipython Notebook presented on Google Colab. Tools used Google Colab, Kaggle, Slack(For Communication) Language/techniques used Python Models used Darknet(CV Model) Skills used Python programming, AI/ML. What are the technical Challenges Faced during Project Execution The main challenge we faced was related to the pre-processing of the Yolov4 dataset of cars in satellite images. The dataset was large and had to be cleaned and formatted before it could be used for training the model. How the Technical Challenges were Solved We used Python programming skills and developed a script that automated the pre-processing of the dataset. This saved us a lot of time and allowed us to focus on training the model. Business Impact The project was a success and the client was very happy with the final product. The car detection model worked perfectly fine on sample satellite images and could be used for further development of an application that could detect cars in real-time. Project website url https://colab.research.google.com/drive/1AoeHdZdpi0lWLf3X2G800J0VT_7wJtnE Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",20.27246376811594,-2,33.33333333333333,19,1,13,0.07177033458483094,2.3122362869198314,https://insights.blackcoffer.com/car-detection-in-satellite-images/,237
18.214285714285715,18.214285714285715,6.357859531772576,85,"data transformation Client Background Client: A leading tech firm in the USA Industry Type: Retail Services: Retail business Organization Size: 100+ Project Objective The objective of this project was to convert dirty JSON data present in a CSV file to a readable CSV file. The CSV file contained data in JSON format, which was split into columns in an Excel file, making it hard to read. The client wanted the data to be extracted and converted into a readable format to perform further analysis on it. Project Description Our client had provided us with a CSV file that contained data in JSON format, which was split into columns in an Excel file. The data was hard to read and understand, making it difficult to perform any analysis on it. Our objective was to extract the data, convert it to a readable format, and validate the JSON file to ensure that it was in a correct format. Finally, we had to convert the JSON data into a CSV file that could be easily read and analyzed. Our Solution To extract the data, we used Python programming language and Pandas library. We extracted every piece of text present in the Excel sheet using Pandas and converted it into a readable text format. We then validated the JSON file with a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas again to convert the JSON data into a CSV file that could be easily read and analyzed. To perform the conversion, we used Jupyter Notebook, Json Validator, and Microsoft Excel. Project Deliverables The final deliverable was a readable CSV file that contained the converted data from the original JSON format. Tools used Jupyter Notebook, Json Validator, and Microsoft Excel. Language/techniques used Python programming language and Pandas library. Skills used Python programming and Pandas data manipulation. What are the technical Challenges Faced during Project Execution The main technical challenge we faced during the project was dealing with dirty JSON data present in a CSV file that was split into columns in an Excel file. This made it hard to read and understand, and required extra effort to extract the data and convert it into a readable format. How the Technical Challenges were Solved We solved the technical challenges by using Python programming language and Pandas library to extract and manipulate the data. We validated the JSON data using a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas to convert the JSON data into a readable CSV file that could be easily analyzed. Business Impact The business impact of this project was that the client was able to perform further analysis on the extracted data in a readable format, which was previously hard to read and understand. Project website url https://colab.research.google.com/drive/1yWDj8_HXu6hOYatrzWQ3ezqBxsUON3JY Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, or Skype or Whatsapp? Please recommend, what would work best for you.",18.656951743908262,-13,28.428093645484946,27,1,22,0.12681159374343626,2.210702341137124,https://insights.blackcoffer.com/data-transformation/,299
20.285714285714285,20.285714285714285,7.196261682242991,44,"advanced ai for handgun detection Client Background Client: A leading tech firm in the Middle East Industry Type: Security Services: Security services Organization Size: 100+ The Problem Detecting Handguns in images and videos. Our Solution We use Yolov7 instance segmentation model to detect and provide coordinates for handguns. Solution Architecture Linux 22.04 Yolo Deliverables Trained model of yolov7 instance segmentation Tools used Openimages Roboflow Yolov7 Language/techniques used Python Models used Yolov7_mask Skills used Deeplearning Programming What are the technical Challenges Faced during Project Execution Retrieving handgun images in bulk from opensource. How the Technical Challenges were Solved Found Openimages dataset with good amount of required images Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",24.562883845126834,-2,41.1214953271028,5,1,7,0.09278350419810821,2.542056074766355,https://insights.blackcoffer.com/handgun-detection-using-yolo/,107
18.428571428571427,18.428571428571427,6.726315789473684,35,"advanced ai for thermal person detection Client Background Client: A leading tech firm in the Middle East Industry Type: Security Services: Security services Organization Size: 100+ The Problem Detect a Person from thermal image and videos. Why this model was created was not told to us by the client. Our Solution Use Deeplearning Computer Vision to train the model on custom dataset and get the results. Solution Architecture Linux 22.04 Nvidiva RTX 3080 Deliverables Trained model Tools used Labelimg Yolov7 COCO2JSON Language/techniques used Python Models used Yolov7 Skills used Deeplearning Computer vision Programming Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",22.10827067669173,-2,36.84210526315789,4,1,6,0.09638554100740312,2.389473684210526,https://insights.blackcoffer.com/advanced-ai-for-thermal-person-detection/,95
16.90909090909091,16.90909090909091,7.0,49,"advanced ai for road cam threat detection Client Background Client: A leading tech firm in the Middle East Industry Type: Security Services: Security services Organization Size: 100+ The Problem Detect the threat level of accidents between a Pedestrian and a Car. Our Solution Use Deeplearning Computer vision and logic to detect the threat level as defined by the Client. Solution Architecture Linux 22.04 Deliverables Program which detects the threat level. Pretrained model. Tools used Yolov7 DEEPSORT Opencv Language/techniques used Python Models used Yolov7 Skills used Programming Computer Vision Deep learning What are the technical Challenges Faced during Project Execution Integration of Object tracking algorithm with Object detection algorithm. Writing of logic to detect the threat level. How the Technical Challenges were Solved The technical challenge was sorted by testing, experimenting and later on finding and modifying an already existing repository to use as a baseline for our code for integration. Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",21.840559440559442,-9,37.69230769230769,5,-1,6,0.12711864299051998,2.4692307692307693,https://insights.blackcoffer.com/advanced-ai-for-road-cam-threat-detection/,130
17.347826086956523,17.347826086956523,6.587755102040816,88,"advanced ai for trading automation Client Background Client: A leading tech firm in Europe Industry Type: Banking & Finance Services: Trading, and financial services Organization Size: 100+ The Problem Create an automated trading application with fully automated trading capabilities from selecting pair of assets to buying/selling assets. This application uses AI to decide what action to take while trading. Our Solution We have integrated coin_api with the application from which data is extracted. We have created the homepage for this application. We have changed the code structure of the front end to make it more fast and efficient. Solution Architecture An application, where the first automated top asset pair selection happens. If the coins are co-integrated, then only one indicator must be executed else trading starts based on 2 indicators. The AI agent will take specific action to trade based on the algorithm. Deliverables We have removed the old API and integrated the new api with the application. We have altered the code structure of the front end to make the code faster and more efficient. Tools used Visual studio code Language/techniques used Python Skills used Django Databases used SQlite Web Cloud Servers used Digital Ocean What are the technical Challenges Faced during Project Execution We faced an issue while integrating coin api with the application while retrieving the data. To retrieve the data using the coin api, we need to input a symbol id. This symbol id is a combination of exchange_name, symbol_type, currency_we_want_to_trade, and quote_currency. There are N coins that can be retrieved using coin api. There are more than multiple exchanges, multiple symbol types, and multiple quote currencies for ONE SINGLE COIN. This makes there is a huge no. Of combinations for one single coin. This made the execution of the api integration very slow. How the Technical Challenges were Solved We created one drop-down for exchange selection, one drop-down for symbol type selection, one drop for coin, and one drop-down for quote currency selection. The user selects these, and in the backend, a combination is created and is sent as input to the coin api code and the data is retrieved without slowing down the process. Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",21.30647737355812,-5,35.91836734693877,13,1,13,0.08450704185677445,2.4571428571428573,https://insights.blackcoffer.com/advanced-ai-for-trading-automation/,245
25.333333333333332,25.333333333333332,5.6,30,"trading bot for forex Client Background Client: A Leading Trading Firm in the USA Industry Type: Finance Services: Trading, Consulting Organization Size: 100+ The Problem Automate trading on the MT4 terminal for forex when certain conditions are met, and end trade at the best exit point. Save mt4 forex data for a instrument live for every tick. Our Solution Use PyTrader to log into trading system (mt4) for 2 brokers. Use live prices to identify when prices diverge. Buy one currency on broker 1, sell currency on broker 2. Hold until prices come back together. Coded a MQL4 script that will save tick data (bid, ask, open, high, low, close) for any instrument when active Solution Architecture Deliverables Python Script to Automate the two Meta Trader 4 terminals, and trade when some conditions are true and break the trade at a exit point. A MQL4 Sript that will Save the Live tick data (Bid, Ask, Spread, Open, High, Low, Close) in a CSV file. Tools used PyTrader numpy pandas Language/techniques used Python (Automation) Mql4 (To save tick data) Business Impact Client requirements were to automate his forex trading strategy on Meta Trader4 terminal, so that he doesn’t have to bother trading anymore, the Python script we designed to not only do it, plus it offers a safe exit point for Ongoing Trades, that saved the client’s money and time.",17.8752688172043,-3,19.35483870967742,6,1,3,0.04195804166462908,1.967741935483871,https://insights.blackcoffer.com/trading-bot-for-forex/,155
20.555555555555557,20.555555555555557,6.90204081632653,97,"algorithmic trading for multiple commodities markets, like forex, metals, energy, etc. Client Background Client: A Leading Trading Firm in the USA Industry Type: Finance Services: Trading, Consulting, Software Organization Size: 100+ The Problem A Trading site will have all the required features, allowing users to trade in multiple commodities markets, like Forex, Agriculture, Metals, Energy etc. Our Solution Designed the website with technical indicators, and the ability to trade in live market, plus allows the user to create his/her own strategy to backtest. Functionalities like all types of technical indicators: Trend following mean reversion relative strength volume momentum. Strategies are specific scripts, which are able to send, modify, execute, and cancel buy or sell orders and simulate real trading right on your chart. Backtesting is the process of recreating the work of your strategies on historical data, essentially all of your past strategic work. Forward testing allows for the recreation of your strategy work in real time, all while your charts refresh their data. Solution Architecture Deliverables A Fully functional trading platform that lets you customize technical indicators, create charts, and analyse financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators: Trend following mean reversion relative strength volume momentum. Tools used Numpy pandas Language/techniques used Python Business Impact Clients want a social media network, analysis platform, and mobile app for traders and investors. So we designed a website with all the client’s requirements, where traders, investors, educators, and market enthusiasts can connect to share ideas and talk about the market. By actively participating in community engagement and conversation, you can accelerate your growth as a trader, and your ability to trade in the live market, plus allows the user to create his/her own strategy to backtest. A Fully functional trading platform that lets you customize technical indicators, create charts and analyze financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators Project Snapshots",24.058956916099774,-1,39.59183673469388,17,1,9,0.04672897174425714,2.416326530612245,https://insights.blackcoffer.com/algorithmic-trading-for-multiple-commodities-markets-like-forex-metals-energy-etc/,245
24.142857142857142,24.142857142857142,6.926605504587156,87,"kpi dashboard for accountants Client Background Client: A leading accounting firm in the USA Industry Type: Finance and Accouting Services: Accounting and financial services Organization Size: 100+ Project Objective The objective of the project was to create a simple and easy-to-use dashboard for the accounting firm Tech 4 Accountants to track their highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Project Description Our client, Andrew Lassise, wanted a KPI dashboard for Tech 4 Accountants that would help them track their business performance easily. The dashboard needed to have various charts and tables that would display important KPIs in a visually appealing manner. Our Solution To achieve our client’s objectives, we used Google Data Studio and Google Sheets to create a visually appealing and easy-to-use KPI dashboard. We created various charts and tables that displayed the KPIs that our client wanted to track. We used Google Sheets to store the data and created visualizations using Data Studio. Project Deliverables We delivered a KPI dashboard for Tech 4 Accountants that included charts and tables for tracking the highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Tools used Google Data Studio and Google Sheets Skills used Data Analytics What are the technical Challenges Faced during Project Execution There were no major technical challenges faced during the project execution as the data was stored in Google Sheets, and Data Studio allowed us to easily create visualizations using the data. How the Technical Challenges were Solved No major technical challenges were encountered, and the project was completed smoothly. Business Impact The KPI dashboard that we created for Tech 4 Accountants allowed them to track their business performance easily and make informed decisions. The dashboard helped them identify areas where they needed to improve and make changes to their business strategy accordingly. Project Snapshots Project website url https://lookerstudio.google.com/u/1/reporting/fbf7879a-be79-4cb9-b7d4-783bf7447902/page/Hmg2C Project Video",25.620445609436434,0,39.908256880733944,17,0.9999999090909174,11,0.0547263678869335,2.4220183486238533,https://insights.blackcoffer.com/kpi-dashboard-for-accountants/,218
21.325,21.325,7.035593220338983,264,"return on advertising spend dashboard: marketing automation and analytics using etl and dashboard Client Background Client: A leading ad firm in India Industry Type: Ads Services: Ads, Marketing, and Promotions Organization Size: 100+ The Problem The main problem that was addressed in this project was the manual calculation of Return on Advertising Spend (ROAS) due to the lack of a centralized platform for running ads. The client’s ads were spread across multiple revenue generating platforms, including Google Adsense, Adx, and Ezoic, while the spending was managed through the Google Ads Platform. At that time, the client lacked a centralized dashboard or website that could effectively calculate ROAS by integrating revenue and cost streams. This fragmentation made it challenging for the client to track and evaluate the effectiveness of their advertising campaigns. Therefore, a comprehensive solution was developed and implemented, providing a centralized platform for calculating ROAS, aligning revenue and cost data from various sources, and enabling informed decision-making regarding advertising investments. Our Solution We developed a comprehensive solution to address the challenges faced by the client in calculating Return on Advertising Spend (ROAS) and centralizing their advertising data. The solution involved collecting data from four different APIs: Google Ads API for spending data, Google Adsense API, Ad Manager API, and Ezoic data for revenue data. To ensure compatibility, we utilized an Extract, Transform, Load (ETL) tool to convert the data received from each API, which was in different formats, into a standardized format storing them Pandas Dataframe for both revenue and spending data. The transformed data was then stored in a Postgres database for easy access and management. To automate the data extraction process, we implemented an ETL script that runs twice daily via cronjob on a Digital Ocean VM, ensuring the latest data is always available. Moreover, we designed a backend API using the Flask framework. This API fetched the required data from the Postgres DB, allowing users to retrieve relevant information efficiently. Finally, we implemented a ROAS Dashboard frontend to display the calculated ROAS using the fetched values. The dashboard provided a visually appealing and intuitive interface for users to track and monitor their advertising performance. With our solution in place, the client could now easily monitor ROAS over time, access consolidated data, and make informed decisions regarding their advertising investments. Solution Architecture The solution architecture involved a multi-step process to address the challenges faced by the client in calculating ROAS and centralizing their advertising data. Data was collected from various APIs, including Google Ads API, Google Adsense API, Ad Manager API, and Ezoic data, and transformed into a standardized format using an ETL tool. The transformed data was stored in a Postgres database, and a backend API was developed using the Flask framework to fetch the required data. The calculated ROAS was then displayed on a Next Js Dashboard, providing users with an intuitive interface to track and analyze their advertising performance. Deliverables ETL Tool Deployment on Digital Ocean Backend API Next js backend/ frontend ROAS Dashboard Tools used Google Ads API Google AdSense API Adx API Ezoic API Python 3.9 Jupyter Notebook Flask Digital Ocean Droplet Next Js frontend/backend Stack Vuexy Template for ROAS Dashboard Language/techniques used Python 3.9 Flask API DigitalOcean Droplet Functional Programming in Python ETL Tool Skills used Python Git Deployment Data Engineering Web Development using Next js Databases used We used PostgreSQL database for the project. Web Cloud Servers used Digital Ocean Droplet What are the technical Challenges Faced during Project Execution Some of the technical challenges encountered were: Ensuring data integrity during the transformation process. Deployment of Docker image on VM Setting up an automated ETL pipeline. Adding SSL certificate to backend API. How the Technical Challenges were Solved 1. Ensuring data integrity: Implemented checks, cleansing, and validation to maintain the accuracy and reliability of the data. 2. Docker image deployment on VM: Configured VM to support Docker Image for ETL and deployed the image for seamless execution. 3. Setting up automated ETL pipeline: Automated data extraction, transformation, and loading processes for efficient data management via cronjob. 4. Adding SSL certificate to backend API: Secured backend API with SSL certificate, enabling encrypted communication for enhanced data protection. Business Impact The implemented solution had a significant positive impact on the client’s business. By providing a centralized platform for calculating ROAS and integrating data from multiple revenue-generating platforms, the client gained valuable insights into the effectiveness of their advertising campaigns. The availability of real-time, consolidated data enabled informed decision-making regarding advertising investments. The user-friendly interface of the RAOS Dashboard allowed the client to easily track and monitor their advertising performance, leading to improved campaign optimization and potentially higher returns on advertising spend. Overall, the solution streamlined the client’s advertising operations, resulting in increased efficiency and improved business outcomes. Project Snapshots Here are the project snapshots: Login Screen Landing page with first selected campaign in the list: Using Date Picker Search Functionality Revenue Breakdown by Platform Show/Hide Left Sidebar Switching Site’s theme to Light Mode Settings/Log Out Menu Change Email/Password Project Website URL: https://roasing.com/ Project Video",26.428305084745762,-7,44.74576271186441,18,1,26,0.059999999890909095,2.5559322033898306,https://insights.blackcoffer.com/return-on-advertising-spend-dashboard-marketing-automation-and-analytics-using-etl-and-dashboard/,590
26.0,26.0,6.734939759036145,94,"python model for the analysis of sector-specific stock etfs for investment purposes Client Background Client: A Leading Investment Firm in the USA Industry Type: Finance Services: Investment, Consulting Organization Size: 100+ The Problem Have an existing Python model that has been built for the analysis of sector-specific stock ETFs for investment purposes. Need to update the existing selection criteria to adjust the selection filter and add a screening criterion that drops off one or more of the proposed holdings, and to have the ability to adjust the parameters of the selection criteria to test different variables. Our Solution The 2 in 4 Fundamental model screens a fundamental ranking of stock market sectors, picks the top ranked holding and continues to hold that sector as long as it remains in the top four rankings. The model holds two positions at a time. The sector ranking data is in the wcm5.xlxs file. We input data from the PRICES.CSV file to pull up monthly returns. When I go to run the program, I use the 2_in_4_New.py and that give me the current rankings for both the fundamental and technical rankings. Sometimes a sector is ranked as being fundamentally attractive because it has become cheaper because of problems going on within an industry. What I would like to do is to test out a way of screening out a sector based upon poor performance over a lookback period. Here is what the new model would do. Screen for a the specific number of sectors, probably between three and five, based upon the fundamental ranking over an average time period (currently 3 weeks) Choose either three, four, or five holdings Exclude the holding that has the weakest performance over a specify lookback period, let’s start with 52 weeks, but I would like to be able to adjust this variable compare the performance of various combinations, seeing the return on an annual basis if possible, as well as showing the maximum drawdown Solution Architecture Deliverables An Updated, Optimised Python script that will filter and return Technical and Financial holdings, with a Price filter that will do price analysis on a certain lookback period. Tools used Numpy pandas itertools, combinations permutations Language/techniques used Python Business Impact The client now can get more than 2 Financial and technical holdings , up to maximum 5 holdings for both Technical and Financial, plus the holdings were more accurate because of the new added Price Filter that will Exclude the holding that has the weakest performance over a specify lookback period, default 52 weeks. It boosted the Client’s profit because of the more accurate and optimised functional filters. Project Snapshots",25.500401606425704,-3,37.75100401606426,10,1,10,0.061611374115585905,2.3775100401606424,https://insights.blackcoffer.com/python-model-for-the-analysis-of-sector-specific-stock-etfs-for-investment-purposes%ef%bf%bc/,249
17.76,17.76,6.989285714285714,115,"building a physics-informed neural network for circuit evaluation Client Background Client: A leading tech firm in the USA Industry Type: Retail Services: Consulting Organization Size: 100+ Project Objective The objective of this project was to build a Physics Informed Neural Network (PINN) using TensorFlow, which could evaluate circuits based on the parameters provided through a MATLAB simulation. Project Description Mohamed provided us with a dataset generated from a MATLAB simulation of a circuit, consisting of various input parameters and the corresponding circuit performance outputs. We were tasked with developing a machine learning model that could accurately predict circuit performance based on the input parameters, while also incorporating the underlying physics principles that govern circuit behavior. Our Solution Our team utilized Jupyter Notebook, Google Colab, Octave, and MATLAB to build the PINN. We used TensorFlow models to build the neural network and Microsoft Excel to clean and preprocess the data. Our team employed Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN. We did not use any databases for this project, nor did we use any web/cloud servers. Project Deliverables The final deliverable was a functional PINN capable of evaluating circuits based on the provided parameters. Tools used Our team used Jupyter Notebook, Google Colab, Octave, MATLAB, and Microsoft Excel. Language/techniques used The primary languages and techniques we used were Python programming, TensorFlow, and MATLAB. Models used We used TensorFlow models to build the neural network for the PINN. Skills used Our team utilized Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN. Databases used We did not use any databases for this project. Web Cloud Servers used We did not use any web/cloud servers for this project. What are the technical Challenges Faced during Project Execution The project was very challenging since our team did not have a background in electrical engineering. It was difficult to understand the physics behind the circuit evaluation, and we faced issues when using MATLAB to provide data for the project. How the Technical Challenges were Solved We worked with the client to gain a better understanding of the physics behind the circuit evaluation. We also worked with MATLAB experts to help us better understand how to provide data for the project. Business Impact The PINN we built for Mohamed Zamil allowed for efficient circuit evaluation and improved the overall accuracy of the evaluation process. Project website url https://colab.research.google.com/drive/1HX37MP4Jcb39SWJgkE_5z5n1gQwqWmV9 Project Video Contact Details Here are my contact details: Email: ajay@blackcoffer.com Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.",23.53257142857143,-5,41.07142857142857,22,1,17,0.09053497905129639,2.4642857142857144,https://insights.blackcoffer.com/building-a-physics-informed-neural-network-for-circuit-evaluation/,280
20.894736842105264,20.894736842105264,6.263358778625954,91,"an etl solution for currency data to google big query Client Background Client: A Leading Tech Firm in the USA Industry Type: IT Consulting Services: Software, Consulting Organization Size: 100+ Project Objective Fetch currency data from Pure-clear API and store it to Google cloud BigQuery. Create a Google cloud function to automate the above process. Project Description We have given a pure-clear API and a google cloud account. We need to fetch currency data from that pure-clear API using python and need to store fetched data in Google Cloud Bigquery. We also need to automate the above process like the process runs on a daily basis and update the currency data on Bigquery. Our Solution We have created a python program that can fetch pure-clear API data. The API data was in JSON format but we needed table format so we used python package pandas. We converted json data to tabular format using pandas. After that, we connected python code to google cloud using google’s authentication module and then stored data frame (table) directly to BigQuery using the “.to_gbq” method. We also need to run the above process daily to update new data in BigQuery. For this Google cloud provides a “Cloud function” tool. In this, we can create a function and set up their running process. So we created a function and attached the above code to that function and set up a cloud function to run daily. Project Deliverables A Google cloud function that runs daily and updates data on Google BigQuery Tools used Cloud function, BigQuery of Google Cloud, Google Colab notebook, Python programming, Pandas Language/techniques used Python language and pandas module Skills used Python programming, Data handling, Google Cloud Databases used Google Cloud BigQuery Web Cloud Servers used Google Cloud Server What are the technical Challenges Faced during Project Execution Connecting google cloud to python code is challenging because Its credentials should be in a specified format otherwise it shows an authentication error. How the Technical Challenges were Solved To tackle this challenge we created a dictionary format (key-value pair) and stored all the authentication variables in the dictionary as a key value pair. Then we used google’s authentication library “google.auth” and passed a dictionary to the service_account method and stored it in different variables so we can store data from pandas dataframe to Google BigQuery. Project Snapshots",22.251024507834472,-18,34.73282442748092,21,-1,2,0.09433962219651122,2.301526717557252,https://insights.blackcoffer.com/an-etl-solution-for-currency-data-to-google-big-query/,262
22.153846153846153,22.153846153846153,7.152631578947369,91,"design and develop solution to anomaly detection classification problems Client Background Client: A Leading Tech Firm in the USA Industry Type: IT Consulting Services: Software, Consulting Organization Size: 100+ Project Description We need to create a notebook with solutions to binary classification-related anomaly detection problems. We need to use machine learning and deep learning models which have greater than 90% accuracy. Our Solution We created a notebook for anomaly detection. We used 10 to 15 machine learning and deep learning models but only 3 different types of auto encoder models that were giving greater than 90% accuracy. We trained all 3 models on one classification data which have anomalies and evaluated trained models on test data. Project Deliverables A notebook that has solutions for anomaly detection related classification problems and accuracy should be above 90%. Tools used Google colab notebooks, Tensorflow, Google drive Language/techniques used Python programming language, Machine learning, Deep learning, Data analysis and Data visualization. Models used Auto Encoder and Variational Auto Encoder Skills used Python, Data Analysis, Data visualization, Machine learning, Deep learning. Databases used MS Excel What are the technical Challenges Faced during Project Execution Most of the anomaly detection models work with regression type data and this problem was classification problem so we need to deal with classification data. Getting high accuracy is also a tough challenge for us because there are only a few models which work well on anomaly detection related classification problems. How the Technical Challenges were Solved So we have limited models for this problem so we used only classification models like Autoencoders, Isolation forest and one class svm. Only Autoencoder was giving high accuracy so we worked with different types of autoencoders like variational autoencoder and normal autoencoder. Project Snapshots",28.01943319838057,-16,47.89473684210526,11,-1,9,0.14534883636425094,2.7421052631578946,https://insights.blackcoffer.com/design-and-develop-solution-to-anomaly-detection-classification-problems/,190
22.666666666666668,22.666666666666668,6.368932038834951,124,"connecting mongodb database to power bi dashboard: dashboard automation Client Background Client: A leading tech firm in Newzealand Industry Type: Retail Services: Retail business Organization Size: 100+ Project Objective Brodie Johnco had a MongoDB Database that he wanted to connect to a Power BI Dashboard. However, ODBC connectors were not working for his level of subscription, so he needed a cheaper workaround. Project Description Brodie Johnco had a MongoDB Database containing a large amount of data that he wanted to visualize in a Power BI Dashboard. He initially tried to use ODBC connectors to connect his database to Power BI, but ran into issues due to his level of subscription. We were brought in to help find a cheaper workaround. Our solution involved using Python to extract the relevant data from Brodie’s MongoDB Database. We used the Pandas library to create Dataframes, which we then uploaded to Azure Blob Storage as tables. We set up an Azure pipeline that ran a Python script every 30 minutes to update the tables with new data from the database. Our Solution We used Brodie’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added them as tables to Azure Blob Storage and set up a Python script to an Azure pipeline that refreshed every 30 minutes. This allowed us to keep the data in sync and provide Brodie with up-to-date information for his Power BI Dashboard. Project Deliverables The final deliverable was a readable CSV file that contained the converted data from the original JSON format. Tools used Jupyter Notebook, Google Colab, Power BI, MongoDB Compass, Microsoft Excel, Azure Blob Storage Language/techniques used Python, Pandas, Azure Cloud Storage Skills used Python programming, Azure Cloud Storage, data extraction and manipulation Databases used MongoDB Database Web Cloud Servers used Azure Blob Storage What are the technical Challenges Faced during Project Execution The main challenge we faced was finding a way to connect Brodie’s MongoDB Database to his Power BI Dashboard without using ODBC connectors. We overcame this challenge by using Python and Azure Blob Storage to extract and store the relevant data. How the Technical Challenges were Solved We solved the issue by using the client’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added these dataframes as tables to Azure Blob Storage and set the Python script to an Azure pipeline that refreshed every 30 minutes. This allowed the client to access the data in Power BI without the need for ODBC connectors. Business Impact Our solution allowed Brodie to visualize his data in a Power BI Dashboard without having to pay for expensive ODBC connectors. The Azure Blob Storage solution we implemented was much more cost-effective and provided him with up-to-date information every 30 minutes. Project website url https://github.com/AjayBidyarthy/Brodie-Johnco",25.11844660194175,-7,40.1294498381877,26,1,8,0.05703422031545924,2.3268608414239482,https://insights.blackcoffer.com/connecting-mongodb-database-to-power-bi-dashboard-dashboard-automation/,309
24.045454545454547,24.045454545454547,6.451428571428571,116,"etl and mlops infrastructure for blockchain analytics Client Background Client: A Leading Blockchain Tech Firm in the USA Industry Type: AR/VR Services: Metaverse, NFT, Digital Currency Organization Size: 100+ Project Objective Code for extraction of the price of cryptocurrency Required real-time data of cryptocurrency and this is extracted from the cryptocurrency URL Forecast code for prediction of the price Built FastApi to reduce interaction complexity for the user Project Description ETL and MLOps Infrastructure for Blockchain Analytics this entire project completes in 4 outlines and stages. In the first segment data scraping for the price of the cryptocurrency. The second stage is, Loading the data into the Microsoft MYSQL server and Transforming data into the required shape for the automated process data Load into the Amazon RDS tool management service which knows as the Amazon relational database service, and creating DB instances (DB instance class – db.t3.small ) . In the fourth stage, built the FastAPI for the get data to the fingertips and easily accessible for the client because it reduces the time to fetch the price of a particular cryptocurrency with a single click, and increases the efficiency of understanding. Our Solution This Project Module develops according to the Client’s Requirements which involves Data extraction of Cryptocurrency data from a given URL by the Client, it also changes the data format, and attributes nomenclature according to the requirements. After extracting the data its loads into Microsoft MYSQL Server for the transformation of data and for full automation process, used Amazon RDS and built the FastAPI. Project Deliverables – Data Scraping code using Python – ETL code for extracting, Transform and Loading into Microsoft MYSQL server – AWS RDS (db.t3.samll) instances for storing data and for deployment – Built FastAPI for getting the price of cryptocurrency Tools used – VC code and Google Collab – Microsoft MYSQL server – AWS RDS services Language/techniques used Data Scraping using Python ETL process to extract, load, and transform the data FastAPI using Python Amazon Cloud services Skills used – Data scraping using python – ETL setup – Aws web services – FastAPI using Python Databases used – Microsoft MYSQL server – Aws RDS (Amazon Relational Database services) Web Cloud Servers used -AWS RDS services What are the technical Challenges Faced during Project Execution Data scraping speed does not meet the expected speed (events/sec) API calls have their own limitation in requesting calls per sec Storing the huge amount of data How the Technical Challenges were Solved Get the Premium service of API calls (20 calls/sec) Used the AWS RDS for storing the data and for faster execution Business Impact This Project impact is directly responsible to the investors of the cryptocurrency. To get the prices of cryptocurrency on fingers tips and use it for buying and investing money in the right corner of the cap market of finance. It clearly impacts financially to the investors and helps them for investing purposes. The scope impact of product service is worldwide for purchasing any cryptocurrency in the world. To provide these impactful services, there is a tech team of Blackcoffer behind it. Project Snapshots Project website URL 127.0.0.1:62190 Project Video https://www.youtube.com/watch?v=xDeL5YggxDw&ab_channel=Blackcoffer",22.875324675324677,-3,33.14285714285714,9,1,5,0.025316455616087166,2.125714285714286,https://insights.blackcoffer.com/etl-and-mlops-infrastructure-for-blockchain-analytics/,350
23.307692307692307,23.307692307692307,6.319838056680162,71,"a web-based dashboard for the filtered data retrieval of land records Client Background Client: A Leading Real Estate Firm in the USA Industry Type: Real Estate Services: Land, Infrastructure, Real Estate, Investment Organization Size: 100+ Project Description The client’s own raw database needed to be converted into a dynamic web application with modern features like user management and subscription where users could explore land records as per their wish. Our Solution Created the web application as per client needs. Added user functionality to handle signup/logins and added authorization middlewares to protect routes from unwanted access. Transformed raw data into a meaningful NoSQL-based database with a proper schema being served as an instance on a cloud service named ‘ MongoDB Atlas ‘. Project Deliverables Pushed code to the required GitHub repository. Tools used – Vanilla javascript – Javascript Frameworks ( Nodejs, express , cors ) – Postman Language/techniques used – JavsScript – Backend Service setup ( express, cors , js ) – Fronted logic setup ( HTML , CSS , JavaScript , Jquery ) Models used Backend: An API service created to handle land records database and queries made by users. Frontend: A frontend client is available as a web application where users can signup and access land records. Skills used JavaScript Programming, APIs, JavaScript Frameworks ( NodeJS, Express , cors ) , Web Design, NoSQL querying in MongoDB. Databases used MongoDB (NoSQL) Web Cloud Servers used MongoDB Atlas What are the technical Challenges Faced during Project Execution – UI component creation – User authorization middleware creation – Querying data in NoSQL How the Technical Challenges were Solved – Created and extended UI components to handle filters like owners, date fields, and area ranges on land records. – API and Frontend are separately built for easier team management of tasks. – Using a cloud-based MongoDB instance provided support for teams to work without any problems with accessibility. Business Impact – Created a platform for clients’ business. – Transformed his raw data into meaningful business applications.",20.82105263157895,-4,28.74493927125506,3,1,13,0.07203389799985636,2.1538461538461537,https://insights.blackcoffer.com/a-web-based-dashboard-for-the-filtered-data-retrieval-of-land-records/,247
119.0,119.0,6.243243243243243,50,"design & develop an app in retool which shows the progress of the added video Client Background Client: A Leading Tech Firm in the USA Industry Type: IT & Consulting Services: Software, Business Solutions, Consulting Organization Size: 200+ Project Description The objective was to develop a progress bar that can help costumes to estimate the analytics of the video. Our Solution The client wanted a progress bar with the following filters: Date filter: – Update the progress bar and count of the videos according to the date selected Category filter: – Update the progress bar and the count of the videos according to the selected category We have created a SQL query for getting a count of the videos from the full video table according to the filter selected in the app In added video table some columns were missing to solve this we created a SQL query for joining the added video table to the other tables and return the count of the video according to the filter selected Project Deliverables App in retool Tools used Retool Language/techniques used SQL Skills used SQL Databases used SQL Database What are the technical Challenges Faced during Project Execution Client wanted date filter and a video category filter but this data was not there in added video table How the Technical Challenges were Solved We had to join multiple data so that we can get category column and date column for applying filter Project Snapshots Project Video",61.11351351351351,0,33.78378378378378,6,0.9999998333333611,6,0.043478260554505356,2.1621621621621623,https://insights.blackcoffer.com/design-develop-an-app-in-retool-which-shows-the-progress-of-the-added-video/,148
23.529411764705884,23.529411764705884,6.53932584269663,101,"medical classification Client Background Client: A Leading Tech Firm in the USA Industry Type: IT Consulting Services: Software, Consulting Organization Size: 100+ Project Objective Classify the medical research paper into 0 if the medical research paper cannot be used in future medical research and 1 if the medical research paper can be used in research based on some research-related phrases. Train an ML/DL model on classified data. Project Description We have given an excel sheet of medical research paper text and provided some phrases to identify research papers that can be used for future medical research. If the phrase is not present in a research paper then it will not be used for research. After annotation, we need to find the best ML/DL model to train research data and evaluate the model on test data. Our Solution We have created a python script that can compare all medical research paper text to research phrases and annot 0 if research phrases are not present in a medical research paper and 1 if research phrases present in medical research paper. After annotation we have trained different machine learning and deep learning models like Bert base uncased using Tensorflow, bert large, XGBoost Classifier, Random Forest Classifier and Logistic Regression. Among these models we have chosen the best accuracy parameters model. In our case the bert-base model performed good and gave 95% test accuracy. Project Deliverables ML/DL model which is trained on medical research classification data to classify other medical research papers. Tools used Google Colab notebooks, Tensorflow, PyTorch, Transformers, MS Excel Language/techniques used Python, Machine learning, Deep learning, Data Science, Natural Language Processing (NLP). Models used Tensorflow-Bert model, PyTorch LSTM model, Random Forest Classifier, XGBoost Classifier, Logistic Regression. Skills used Machine Learning, Deep learning, NLP, Python programming. Databases used used ms excel data What are the technical Challenges Faced during Project Execution There are various technical challenges faced during project execution: The research paper has a huge amount of text data so the model was giving space errors in colab notebooks. Find the best threshold value which gives best test accuracy. How the Technical Challenges were Solved To solve space error we have trained the model with lower batch size so this solved the error. To find the best threshold value we created the ROC AUC curve and Precision Recall curve and checked best points where accuracy will be higher.",24.542850848204452,-5,37.82771535580524,11,1,12,0.07112970681535688,2.2808988764044944,https://insights.blackcoffer.com/medical-classification/,267
26.53846153846154,26.53846153846154,6.518987341772152,75,"design & develop bert question answering model explanations with visualization Client Background Client: A Leading Tech Firm in the USA Industry Type: IT Consulting Services: Software, Consulting Organization Size: 100+ Project Description We need to use a pre-trained bert question answering model and create a notebook that has explanations of model’s working with some visuals of bertviz, allennlp and gradient values. Our Solution We created a notebook first and explained the model with model view and head view visuals of bertviz library. It gives similarity between words so we can easily find related words. We used the allennlp library and created bar charts and heatmaps to show higher and lower attention words. It means when it finds question related words in the context it gives higher value to those words and if words are not related it gives lower values. We used a gradient based method to show higher and lower gradient values word according to question text and created bar charts and text color charts to show higher gradient values. Project Deliverables A notebook which has an explanation of the bert question answering model using some visualization. Tools used Google colab notebooks, Tensorflow, Bertviz, Allennlp, Transformers Language/techniques used Python programming language, Deep learning, NLP, Data Visualization Models used Pretrained bert-base-uncased model and distilbert model (both trained on squad2 dataset) Skills used Data visualization, Deep learning, NLP, python What are the technical Challenges Faced during Project Execution We need to use the best pre-trained model which can give good results on different questions and answers. We were working on text data so we need to use charts which can clearly show differences between higher attention and lower attention value words. How the Technical Challenges were Solved For best pretrained we tried different Bert’s pretrained models like distilbert(trained on squad dataset), distilbert(trained on squad2), bert base uncased, bert large and roberta base. Among these models we kept the best one. For solving charts related issues we used heatmap chart, bar chart with dark and light colors and text coloring method. Project Snapshots",23.273612463485883,-2,31.645569620253166,18,1,7,0.0473684208033241,2.2447257383966246,https://insights.blackcoffer.com/design-develop-bert-question-answering-model-explanations-with-visualization/,237
30.066666666666666,30.066666666666666,6.3238434163701065,84,"ranking customer behaviours for business strategy Client Background Client: A Leading Retail Firm in the USA Industry Type: Retail Services: Retail Business Organization Size: 100+ The Problem Create an API service that will parse text, include comments, analyse the remarks, assign a score based on sentiment or other criteria, etc. Feed it comments, and it should analyse the syntax and sentiment of the comments as well as extract key terms to add to the extended meta data of that model. In order for us to know a user’s behaviour, personal information, and more meta data about their interests Our Solution Created a flask API, that will take comments as input and will textual analysis as follows: Spell and Grammar Check : We have used language tool python for this , LanguageTool is an open-source grammar tool, also known as the spellchecker for OpenOffice. This library allows you to detect grammar errors and spelling mistakes through a Python script or through a command-line interface. Sentimental Analysis : For Sentimental Analysis we used FLAIR, Flair is a pre-trained embedding-based model. This means that each word is represented inside a vector space. Words with vector representations most similar to another word are often used in the same context. This allows us, to, therefore, determine the sentiment of any given vector, and therefore, any given sentence. Keywords Extraction : For keywords extraction we used SPACY which is newer than NLTK or Scikit-Learn, is aimed at making deep learning for text data analysis as simple as possible. The following are the procedures involved in extracting keywords from a text using spacy. Split the input text content by tokens Extract the hot words from the token list. Set the hot words as the words with pos tag “ PROPN “, “ ADJ “, or “ NOUN “. (POS tag list is customizable) Find the most common T number of hot words from the list Solution Architecture Deliverables CommentScoringAPI that will take comments/reviews as input, and do the textual analysis on the given comment and will return the Comment Score based on counts of spell and grammar errors, sentiments, hot keywords. Tools used Numpy , pandas , flask , NLTK , Spacy (Keyword Extraction), language tool python (spell and grammar check), flair (Sentimental Analysis) Language/techniques used Python Business Impact Client have a user schema that contain all the information of users that have visited there platform, and he/she want to build a Script that will take all the reviews of a certain User as input and than will do textual analysis on all the comments of the user , by textual analysis we mean Spell and Grammar Check, Sentimental Analysis, and Keywords extraction. Based on these factors our Script scored each user and helped Client to understand his/her users well.",23.98396204033215,-8,29.8932384341637,14,1,9,0.06719367562374041,2.170818505338078,https://insights.blackcoffer.com/ranking-customer-behaviours-for-business-strategy/,281
14.9,14.9,6.700996677740863,102,"data integration and big data performance using elasticsearch Client Background Client: A Leading Tech Firm in the USA Industry Type: IT & Consulting Services: Software, Business Solutions, Consulting Organization Size: 200+ Project Objective Migrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database. Project Description The client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database. Our Solution Setup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance. Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data. Identify the code in the backend that needs to be changed. Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this. Testing Postgres and Elasticsearch performance. Project Deliverables Setup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance. Pipeline i.e; logstash file New working backend code for elasticsearch Commands to check elastic data. Customizable logstash pipeline Tools used Elasticsearch Postman Kibana Logstash Python Javascript Amazon Web Services Postgres Docker Git Bucket Github Language/techniques used Javascript Json Domain-Specific Language for elasticsearch bash Skills used Elasticsearch query knowledge Postgres query knowledge Networking Javascript Backend web stack Databases used Postgres Elasticsearch Web Cloud Servers used Amazon Web Services (AWS) What are the technical Challenges Faced during Project Execution Sometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs. How the Technical Challenges were Solved To solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times. Business Impact Earlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience. Project Snapshots",19.514817275747507,-3,33.88704318936877,11,1,5,0.030769230650887575,2.4086378737541527,https://insights.blackcoffer.com/data-integration-and-big-data-performance-using-elk-stack/,301
101.66666666666667,101.66666666666667,5.7772277227722775,76,"an app for updating the email id of the user and stripe refund tool using retool Client Background Client: A Leading Healthcare Tech Firm in the USA Industry Type: Healthcare Services: Healthcare Solutions Organization Size: 200+ Project Description The client needed two apps in retool Update the email id of the customer Stripe refund app with two options full payment and partial payment Our Solution We create the following two apps in retool Takes the old email id of the user and new email id of the user when the update email id is clicked then the old email id is updated with the new email id. For updating email id we have used stripe API The user has to select the email id of the user and payment id of the user from the table the user get two options for a refund Full payment: – This option refunds the whole amount to the customer Partial payment: – This option refunds the partial amount entered by the user Project Deliverables Apps in retool Tools used Retool Stripe Language/techniques used JavaScript Models used We have not used any models Skills used API Databases used Stripe database What are the technical Challenges Faced during Project Execution The main challenge was creating a full payment option using stripe API. If the customer has already received a partial amount then while performing a full refund the refund amount was always greater than the balance amount How the Technical Challenges were Solved To solve the full payment option issue, we calculate the balance amount and provided that amount to the full payment event in retool Business Impact Using this apps it’s easy for the client to update the email id of the customer and refund the customers client can refund into two option full payment and partial payment Project Snapshots Project website url",55.71617161716172,-1,37.62376237623762,6,1,9,0.05649717482204986,2.1881188118811883,https://insights.blackcoffer.com/an-app-for-updating-the-email-id-of-the-user-and-stripe-refund-tool-using-retool/,202
59.0,59.0,6.3584070796460175,75,"an ai ml-based web application that detects the correctness of text in a given video Client Background Client: A Design & Media firm in the USA Industry Type: Marketing Services: Consulting, Software, Marketing Solutions Organization Size: 100+ Project Objective Create a python web application that detects the text and checks the spelling of written text in the videos and prints the count of wrong spelling in the end Project Description Developing a dockerized Django web application for detecting the text and checking the spelling of written text in the video and printing the count of wrong spelling in the end and deploying the application on google cloud Our Solution We have created a python web application with Django framework when user uploads the video the application run keras-ocr model on each frame of the video and keep the count of the wrong words at the end it provides the video with the bounding box around the words. For correct words it creates green bounding box and for wrong words it creates red bounding box and also it provides the summation of count of wrong words. Project Deliverables Deployed dockerized web application on google cloud which generate video with bounding box around texts Tools used Docker Redis Server Django Celery Nginx Opencv NLTK Moviepy Language/techniques used Python Html CSS JavaScript Models used We have used keras-ocr model for detecting the text form the video and creating the bounding box around the words Skills used Natural language processing, Machine learning, Image processing, Web development, Python programming Databases used Django Sqlite3, Redis Server Web Cloud Servers used Google cloud What are the technical Challenges Faced during Project Execution Running model on each frame of the video Show progress bar for the progress of the work How the Technical Challenges were Solved For running the model on each frame of the video we have used celery it runs the model in the backend of the application We have used celery backend progressrecorder and updated it every time when model has detected the text from the frame of the video Project Snapshots Project website url http://34.68.134.64/",36.87433628318584,-9,33.1858407079646,11,-1,4,0.06701030893293655,2.1814159292035398,https://insights.blackcoffer.com/an-ai-ml-based-web-application-that-detects-the-correctness-of-text-in-a-given-video/,226
20.037735849056602,20.037735849056602,6.7165605095541405,304,"website tracking and insights using google analytics, & google tag manager Client Background Client: A leading marketing firm in the USA Industry Type: Marketing Services: Consulting, Software, Marketing Solutions Organization Size: 400+ Project Objective The project objectives are as follows: Assisting the businesses with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website. Setup pixels of Social Media platforms like LinkedIn and Facebook which assists users in tracking conversions. Providing monthly insights on their website performance to analyse the businesses’ strengths and opportunities for growth. Project Description This project includes assisting businesses with digital analysis for their marketing.Digital analytics allows you to stand back, get the big picture, and see what is working and what isn’t in your overall strategy so you can adjust. The importance of digital analytics is that it allows for a data-driven approach to marketing, and as such it can produce better results. The primary objective of the project is to help the businesses in knowing their target audience, understanding the trends in digital marketing, and providing insights on the analytics part of their website performance. Use the digital analytical data to determine if your business’ aims are in line with the customer’s wants and needs. As the picture of the customer’s needs unfolds, adjust the objectives accordingly. Our Solution The main aim of this project is to assist the businesses to improve their website performance with the use of technologies like Google Analytics, Google Tag Manager and dashboards built on Whatagraph. Google Analytics: Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process. Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics. Google Tag Manager: Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface. When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags. Whatagraph Dashboards: The whatgraph dashboards previews the important metrics related to the website including conversions, events, number of users and performance about ads and campaigns by the website. This dashboard helps in drawing some of the useful insights for the website notifying the strengths,gains and areas of improvement. Project Deliverables Main deliverables for the project are: Setup the Google Analytics and Google Tag Manager for the website. Tracking events on Google Analytics using Tags created in Google Tag Manager. Monthly Reporting of Analytics for businesses on Whatagraph dashboards or via presentations. LinkedIn and Facebook Pixel setup and validation for the website. Setup Goal Conversions for the website to track the important and valuable metrics from the website. Tools used Google Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics. Whatagraph: To visually represent important metrics like impressions, clicks, goal completions and many more related to Ads management and Google Analytics. Clickup: This tool is used to manage tasks given. Skills used Digital Analysis Data Analysis Digital Marketing Google Analytics What are the technical Challenges Faced during Project Execution The main technical challenge faced was that any changes in Google Analytics are operational after 24 hrs. Thus, we can’t judge if the setup works as per required. How the Technical Challenges were Solved We had to wait for 24 hours to check the setup. We could use real-time report as well to check the setup on-the spot. Business Impact This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users. Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest. Some of the common metrics that are important in digital analytics include: Dashboard metrics : Some examples are pages per visit, bounce rate, and average duration of each visit. Most exited pages: Pages with an exit rate of 75–100% show that you need to examine the problem with the content and improve upon it. Most visited pages: These pages will make the customers either exit or explore the website further. Referring websites: These are other websites that link to your website. Conversion rate: This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter. Frequency of visitors: This tells you about the loyalty of the customers. Days to the last transaction: This refers to the time lapse between the first visit and the sale. The shorter the time taken, the better it is for your business. Project Snapshots Figure 1: Google Tag Manager Domains Figure 2: Google Tags Figure 3: Google Analytics Figure 4: Google Analytics Figure 5: Tracking Facebook Pixels for a website Figure 6: Whatagraph dashboard Figure 7: Whatagraph Dashboard(Conversions) Project website url https://unite.ca/ https://livelike.com/ http://essencelle.ca/ https://www.decorium.com/ https://www.everafterfest.com/2022-tickets/ https://winagetaway.com/",27.378151664463406,-2,48.40764331210191,42,1,31,0.05749128909844723,2.43312101910828,https://insights.blackcoffer.com/website-tracking-and-insights-using-google-analytics-google-tag-manager/,628
23.0,23.0,6.718367346938775,94,"an agent-based model of a virtual power plant (vpp) Client Background Client: A Leading Energy Firm in the USA Industry Type: Energy Services: Power, Energy, Distribution Organization Size: 5000+ Project Objective To create an agent based model of a virtual power plant in Netlogo. To see the function of multiple such power plants that worked simultaneously. These power plants created and supplied energy based on a demand parameter that can be controlled by the observer Project Description The client defined specific requirements as to how he wanted the model to be. The requirements were divided into 4 parts. Each successive part increased in complexity and required the model to be adjusted or configured to fit that part into it The entire model when completed contained all the four parts defined by the client in the Statement of work. Our Solution Created the model according to requirements. The clustering of multiple agents and their position is decided mathematically based on the total number of agents and the sum of their energies. The agents form a cluster based on the condition that the sum of their power is a figure that is above a certain threshold amount, the threshold amount is also decided by the observer. Project Deliverables https://github.com/AjayBidyarthy/Shingi-Samudzi-Build-Netlogo-ABM-for-simulating-Virtual-Power-Grid-economics Above is the github link to every state of the model that was delivered to the client. The uploads start from a basic model with only clustering of the agents The final upload is a model that contains the full representation of a VPP for simulation. Tools used -Netlogo – python Language/techniques used Netlogo uses a specific language that resembles the logo language but has it’s unique syntax and variations in the way variables are stored and how a list is parsed Models used Clustering Skills used Netlogo programming What are the technical Challenges Faced during Project Execution The major challenge was controlling the behavior of each agent in the model. The lack of understanding of the language and the available resources about it made it challenging to figure out the actual behavior of the agents and the overall model. The decision to decide where exactly each agent will cluster on the grid was difficult primarily because each agent spawned on a random patch of the screen. This meant that each agent would have to be given a spot to land on and form a cluster with other agents. The next challenge was deciding the condition on which the agents will cluster as their relative distance to each other couldn’t be used as a parameter as it wasn’t relevant to the model’s purpose. How the Technical Challenges were Solved The technical challenges were solved by extensive research and referring to several forums over the span of 2 months. Project Snapshots Project Video https://www.youtube.com/watch?v=1fzCUzZ0q0Q&ab_channel=Blackcoffer",24.546938775510206,-3,38.36734693877551,11,1,4,0.032407407257373115,2.330612244897959,https://insights.blackcoffer.com/an-agent-based-model-of-a-virtual-power-plant-vpp/,245
16.375,16.375,6.576530612244898,80,"power bi dashboard on operations, transactions, and marketing data, embedding the dashboard to web app Client Background Client: A leading tech firm in the USA Industry Type: IT Services Services: Consulting, Software, Marketing Solutions Organization Size: 100+ Project Objective Create a dashboard with Assets Performance With react App. So users can evaluate with Key metrics from data analytics and forecasting. Project Description The client requires two pages: Screening Asset Performance Portfolio Investing according to criteria and sector-based. Our Solution By using Power BI We can achieve this requirement without any additional stack. It requires a subscription to enhance the report. Using Page Navigation and bookmarks to create reports like Web Application with React App. Project Deliverables Asset Report Page Investor Page Tools used Power BI Azure AAD Mongo DB BI Connector ODBC Connector DAX Studio Language/techniques used STAR SCHEMA Skills used DATA MODELLING. Performance Analyser. Vertipaq Analyser. Databases used Mongo DB Web Cloud Servers used AZURE What are the technical Challenges Faced during Project Execution Time for loading pages is increased due to raw data. Cold start of Report taking more time than usual How the Technical Challenges were Solved From Snowflake to Star Schema achieved performance of Report By using Performance Analyser debugging resolved many glitches and where it is happening. Extraction, Transformation makes data less complex and removing unwanted data from a website perspective makes data shrink and achieved 75% of Data Reduction. Business Impact Less coding with Power BI speeds the development process and achieves Best UX with less time. Project Snapshots Project website url https://digital.bctriangle.com Project Video",22.8765306122449,-5,40.816326530612244,5,-1,4,0.05294117615916955,2.377551020408163,https://insights.blackcoffer.com/power-bi-dashboard-on-operations-transactions-and-marketing-embedding-the-dashboard-to-web-app/,196
19.285714285714285,19.285714285714285,6.16734693877551,73,"nft data automation (looksrare), and etl tool Client Background Client: A leading tech firm in the USA Industry Type: IT Services Services: Blockchain, NFT Organization Size: 10+ Project Objective To scrape all the desired information regarding the NFTs from a website and store them in a database to be accessed later on. Project Description Matthew Brown – extract all events, all time from this https://looksrare.org/explore/activity . We can then pay you weekly to keep them up to date. You can choose any technology you like, as long as it’s updated into an SQL database. Additional tasks may be to make an alert or dashboard from data, later access API when it becomes available. Our Solution We provided a robust solution which returned the NFT data every 8 hours into the google big query database. To do this we used selenium web driver to scrape all events as the website was dynamic and did not have a format data structure to scrape data using AJAX POST calls. After automating the scarper the data was manipulated and constructed into a desired format into pandas dataframe, which was later used to push the dataframe into the google big query database using Google cloud api and credentials. The data was getting collected every day and about 50M distinct rows were created. Project Deliverables Webcrawler and database Tools used Python Selenium GBQ Language/techniques used Python Selenium web scraper Pandas Google big query Parallel processing. Databases used SQL Google BigQuery Web Cloud Servers used Google BigQuery What are the technical Challenges Faced during Project Execution The only technical challenge faced during this project was that the website used to keep changing the elements on their webpage and used to cause error. Though it did not use to happen regularly, it happened 3 times in 5 weeks. Also AJAX calls were not proper. How the Technical Challenges were Solved Identifying the elements solved the issue. Also remote access to a better desktop enabled me to keep working as well as keep the code running all the time. Business Impact Supplied upto 50 million rows data regarding NFTs. Provided a python solution with optimal functions and code to be used and automate them to save the data into a database on a daily basis. Caused a huge influx of data which can be used to make many insightful decisions regarding the nft market. Project Snapshots Project website url https://looksrare.org/explore/activity",19.63265306122449,-4,29.795918367346943,17,1,10,0.06730769198409764,2.2122448979591836,https://insights.blackcoffer.com/nft-data-automation-looksrare-and-etl-tool/,245
33.857142857142854,33.857142857142854,6.825174825174825,46,"transform api into sdk library and widget Client Background Client: A Leading Tech Firm in the USA Industry Type: IT Services: Consulting, Marketing, Healthtech Organization Size: 500+ Project Objective Convert API documentation into SDK library and widget. Expected deliverables are SDK library and widgets for Web apps iOS apps Android Apps Project Description API documentation is available for a tool that allows customers to type in their medication and find the cheapest price near them. For partners who want to have it on their own site, currently using the API documentation but would like to ultimately be able to send them an embeddable widget that incorporates the tool on their site Our Solution We created a flutter widget that uses SDK libraries that allows the customer to type their medication and find the cheapest price near them. This widget can be embedded in their web, android and IOS applications Project Deliverables 1)SDK Library/Widget 2)Sample flutter application Tools used Flutter Language/techniques used Dart Skills used 1)Knowledge of dart language 2)flutter app developing What are the technical Challenges Faced during Project Execution 1 )Problems while fetching details of drugs and pharmacies 2) Showing details of drugs and pharmacies in the widget How the Technical Challenges were Solved All technical challenges are solved by proper communication with the client and by logical analyzing of data Project Snapshots Project Video https://www.youtube.com/watch?v=MyNK_DPtsKA&ab_channel=Blackcoffer",26.40999000999001,-1,32.16783216783217,12,1,11,0.09160305273585456,2.3566433566433567,https://insights.blackcoffer.com/transform-api-into-sdk-library-and-widget/,143
19.6,19.6,6.378600823045267,71,"making a robust way to sync data from airtables to mongodb using python – etl solution Client Background Client: A leading tech firm in the USA Industry Type: IT Services Services: SAAS services, Marketing services, Business consultant Organization Size: 100+ Project Description Equilo is a social impact start-up focused on gender equality and social inclusion. We need to link data in Airtable (1 million+ records spread across 20+ bases) to MongoDB (v3.x.x). Most of the data is backend data for our app, in which case the flow is only AT to MDB. Need to create a code that can calculate a scores by pulling from indicators in many different bases and putting result in new database. Our Solution Used Python and MongoDB module along with Airtable API to fetch all the data from airtables and push them to the database. Stayed in touch with the client through slack and asana completing daily tasks and applying a cronjob for the program to run on a scheduled time. Project Deliverables Python code for sync into their staging server and then to production. Tools used VScode MongoDB Airtable API Slack Asana Github Language/techniques used Python MongoDb SQL Skills used Data extraction Data handling Data storage Computational data queries Databases used Airtables MongoDB Web Cloud Servers used Airtable What are the technical Challenges Faced during Project Execution Main challenge faced was regarding the new concept of Airtables and syncing up the data into mongodb in a very complex schema as proposed by the client. Dissimilar columns in mongoDB and Airtables for 100s of tables took lot of time. Also insufficient information provided by client while coding and the previous versions codes that had been written only to discover them on a later stage caused a lot of problem. Not proper code management which could help next coders like me to complete the remaining stuff quickly. How the Technical Challenges were Solved These issues were solved by lot of self study and evaluation and then asking the exact question to client which they would then answer. For eg: whereabouts of the previous codes and people who run that code. Business Impact Helped them immensely making their backend to frontend integration seamless. Sped up their product development by 20% to calculate various different scores and visualize them on the frontend. Project Snapshots Project website url https://www.equilo.io/",19.52724279835391,-7,29.218106995884774,13,-1,6,0.06132075442773229,2.213991769547325,https://insights.blackcoffer.com/making-a-robust-way-to-sync-data-from-airtables-to-mongodb-using-python-etl-solution/,243
15.65,15.65,6.472727272727273,66,"integration of a product to a cloud-based crm platform Client Background Client: A Leading Logistics Firm Worldwide Industry Type: Logistics Services: Import, Export, Supply Chain, Logistics, Trades Organization Size: 500+ Project Description The main challenge faced by the team was the integration of the two systems themselves. Since one-by-one entering of records into each module is a mundane task and a waste of valuable time we proposed the automation using APIs. Our Solution The challenge was divided into two milestones and sub-tasks for each. 1. First was the ingestion of existing data into the cloud-based CRM platform. 2. Second was the question of automating the process of adding newer records to the cloud platform. Project Deliverables The client has been provided with python scripting handling bulk data ingestion to CRM and also the script to handle daily synchronization of data. Tools used – Python – MySQL Database – Postman – TeamViewer Language/techniques used – Automation – 3 rd party APIs – Authentication methods – Multi-Threading of function calls – bat Scripts for easier running of scripts for the client Models used Python Frameworks like requests to build own custom client for consumption of APIs. Skills used Python Programming, Mult-threading, APIs Databases used The client provided a MySQL instance. Web Cloud Servers used Zoho What are the technical Challenges Faced during Project Execution? – Writing own client-side API-consumption code handling API calls from Authentication and Other Operations as per task requirements. – Debugging of API responses was messy. How the Technical Challenges were Solved – Multiple alternatives were discussed and implemented in python like conditional refreshing of API tokens. – Automation of daily synchronization handled by use of time deltas. – Logging of all operations to efficiently handle errors in the future. Business Impact – Automated workflow of the client – No need for dull tasks like data entry to CRM modules everything is taken care of using logic. URL https://www.exportgenius.in/",18.26,-7,30.0,3,1,8,0.07537688404333225,2.2045454545454546,https://insights.blackcoffer.com/integration-of-a-product-to-a-cloud-based-crm-platform/,220
20.083333333333332,20.083333333333332,6.928571428571429,78,"incident duration prediction – infrastructure and real estate Client Background Client: A leading research institution in the middle east Industry Type: Research Services: R&D Organization Size: 1000+ Project Objective To complete a Research Paper draft by training various Machine Learning models which can predict the Incident Duration based on various parameters given in the dataset and summarising the results. Project Description Given a set of researches, need to analyse and compare various machine learning and deep learning models to predict the Incident Duration for the given dataset. The dataset contained Short durations as well as Long durations. Build models for each set of durations, compare and get the best out of all. Our Solution Here, we had to predict the traffic incident duration with some machine learning tools and techniques i.e. XGBoost, SVR and Deep Learning algorithm using tensor flow. First two models were run on Python Interpreter whereas Deep learning model was run on R studio, all the three with the same dataset and then we had compared these models based on their MAE (mean absolute error). Initially, we had done a preliminary analysis of the collected incident duration data, to collect the statistical characteristics of all the variables used in our research. Project Deliverables Python Script for each model. Documentation for Research Work. Tools used Python Interpreter Language/techniques used Language Used: Python Libraries Used: pandas, sklearn, numpy, keras, pickle Models used XGBRegressor SVR SGDRegressor Sequential DecisionTreeRegressor Skills used Programming, Statistical Analysis Project Snapshots",26.604761904761904,-1,46.42857142857143,7,1,4,0.034965034720524237,2.511904761904762,https://insights.blackcoffer.com/incident-duration-prediction-infrastructure-and-real-estate/,168
23.272727272727273,23.272727272727273,6.266666666666667,64,"integration of video-conferencing data to the existing web app Client Background Client: A Leading Tech Firm in the USA Industry Type: IT & Consulting Services: Software, Business Solutions, Consulting Organization Size: 200+ Project Description Integration of 3 rd party APIs to client’s platform.Client required meeting/conference data from sites like gotomeeting/zoom. Our Solution Using APIs fetched data from different platform and rendered data into client’s application. Modifed web application with a UI to handle form data accepting dates as a timeframe – which then makes a request to the API being handled at server end and returns the meeting data from the required source. Project Deliverables Pushed code to client’s github repository. Tools used – Python – Postman Language/techniques used – Automation – 3 rd party APIs – Authenication methods – Multi-Threading of function calls ( authentication of api client ) – UI component design to get dates from user-end Models used Python Framework- Django , requests Skills used Python Programming, APIs , Multi-threading , Web Developement Databases used Default project postgreSQL Web Cloud Servers used Heroku What are the technical Challenges Faced during Project Execution – UI creation for handling form data – Managing and Validating form data to process request at server end How the Technical Challenges were Solved – Created autmated functions as views in django to handle requests made to video-conferencing platform. – Which then returns meeting data as per user’s wish. Business Impact – Instead of extracting meeting data and adding it to all users any authorized user can get meeting data as his wish. Project website url https://www.codanalytics.net/",22.437296037296036,-1,32.82051282051282,4,1,2,0.016393442533369166,2.230769230769231,https://insights.blackcoffer.com/integration-of-video-conferencing-data-to-the-existing-web-app/,195
14.1875,14.1875,6.326388888888889,94,"web data connector Client Background Client: A Leading Marketing Tech Firm in Australia Industry Type: Marketing Services: Marketing Solutions Organization Size: 50+ Project Objective To make a software code that takes data from a source and ingests it into a database present on a server. The scripts should automatically execute after regular intervals of time. Project Description The client had several data sources that were updated with new data regularly. The client wanted software that triggers itself automatically and takes data from those data sources and ingests it into a database that is hosted on a Linode server. Also, the date parameters in the query should be changed dynamically using the current date. Further, we had to assist in setting up the Tableau BI tool on the client’s PC and connect the Postgres database to the tableau. Our Solution We setup a linux server on linode. Install Postgres on this linux server. Create a database and create a new user. Grant this new user all privileges on the database. Create a table within the database. This table has columns with datatypes as specified by the client. Write a python script that makes GET request to the client specified data source and store the response in json format. Inside the python script itself, establish a connection to our postgres database using the pscopg2 module and user credentials. Ingest the data into postgres using INSERT query in python script. Write code to get the today’s date using the datetime module. Using this, calculate yesterday’s date. Now we can use these as parameters inside our query to the data source. Move these python files to our server. Install and setup Cron on our server. Add the task to run specified python files at regular intervals to Cron. Repeat steps 4 to 11 for every new data source. Project Deliverables Python Script Working linode server with cron installed Tableau installation and connection to postgres Project Documentation Tools used Linode server VS Code Language/techniques used Python Bash PSQL. Skills used Python programming Postgres SQL Linux scripting Databases used Postgres Web Cloud Servers used Linode What are the technical Challenges Faced during Project Execution Avoiding duplicates was a challenge. Since Client was living in Australia all the timezone (on server and in code) were changed to AEDT. How the Technical Challenges were Solved Used uniqueid Column to check for duplicates. Used pytz module to change timezones. Business Impact This solution helps in maintaining a copy of all data sources inside our Postgres database. Also, the data is 24/7 available. Since data inside the Postgres is updated regularly, graphs in the tableau are also up to date. Project Snapshots Project website url https://github.com/X360pro/Web-connector-for-tableu",18.730555555555558,-2,32.63888888888889,13,1,2,0.015384615325443787,2.232638888888889,https://insights.blackcoffer.com/web-data-connector/,288
16.761904761904763,16.761904761904763,6.618834080717488,87,"auvik, connectwise integration in grafana Client Background Client: A Leading Tech Firm in the USA Industry Type: IT & Consulting Services: Software, Business Solutions, Consulting Organization Size: 200+ Project Objective Get statistics such as uptime, availability, cpu throughput etc. from Auvik and Connectwise and make a dashboard from it in Grafana. Project Description Unlike many technologies for which plugins are readily available in Grafana, there are none for auvik and Connectwise. So our task was to device a solution through which all the data from Auvik and Connectwise can be fed to Grafana. This data then would be used to plot graphs in Grafana. Our Solution Setup Postgres on linux Create appropriate databases, tables and users in it. Use python to get data from Auvik and Connectwise and perform necessary preprocesing. In the same python file, Connect to our postgres database. Ingest this data into postgres database. Setup Grafana. Connect Grafana to postgres using the postgres plugin. Query our postgres database in Grafana to get desired results. Plot multiple graphs according to client’s requirement and make a dashboard from it Project Deliverables Setup Postgres Setup Postrges in Grafana Write Python code to get data from Auvik and Connectwise into Postrges Plot graphs into Grafana according to client’s requirement Make dashboards for all the graphs Tools used Grafana Postgres Vs Code AWS Postman Language/techniques used Python bash Skills used Python networking Data visualisation Databases used Postgres Web Cloud Servers used Amazon Web Services (AWS) What are the technical Challenges Faced during Project Execution Since, the data received from Auvik was in Json fromat, our first approach was to use Grafana’s built-in Json plugin. But this wasn’t working since, the data received from Auvik was multi-dimensional when the Json plugin required One dimensional data. How the Technical Challenges were Solved The above challenge was addressed by transforming the multi- dimensional data into one dimensional when it was store in a python variable. This transformed data was then inserted into Postgres. Project Snapshots Project website url https://github.com/AjayBidyarthy/Henry-Pardo Project Video https://www.youtube.com/watch?v=7CcbdfjkBzc&ab_channel=Blackcoffer",22.31014307068119,-5,39.01345291479821,10,-1,4,0.04568527895591229,2.327354260089686,https://insights.blackcoffer.com/auvik-connectwise-integration-in-grafana/,223
14.411764705882353,14.411764705882353,6.607954545454546,73,"database normalization & segmentation with google data studio dashboard insights Client Background Client: A leading marketing firm in the USA Industry Type: Market Research Services: Marketing, Consultancy Organization Size: 60+ Project Objective To combine the different datasets. To make dashboards for each and every dataset individually. Project Description Phase – 1: In this project first of all we have to combine different datasets individually to make single file for each source. Phase – 2: Make Good looking reports for each file individually. Our Solution We used pandas dataframe to combine different files to make single file for each source. We used Google Data Studio to make good looking and better reports with good UI. Project Deliverables We have provided a Google Data Studio report file as deliverable for the project. Tools used Python, Google Data Studio, Google Chrome Language/techniques used Python Programming and SQL queries editor. Models used SDLC model used in this project. We have used the SDLC model as analysis, design, implementation, testing and maintenance. Skills used Data cleaning, Data Pre-processing, Data Visualisation are used in this project. Databases used We have used the traditional file systems as database storage. What are the technical Challenges Faced during Project Execution Combining Data sets into single file. Making good looking UI dashboards. How the Technical Challenges were Solved I used pandas dataframe to combine different datasets and made a single file of every individual source. I used Google Data Studio to make dashboard for the project. Project Snapshots Project Video",22.355614973262036,0,41.47727272727273,9,0.9999998333333611,6,0.039735099074601994,2.471590909090909,https://insights.blackcoffer.com/database-normalization-segmentation-with-google-data-studio-dashboard-insights/,176
17.418181818181818,17.418181818181818,6.475247524752476,223,"google local service ads (lsa) leads dashboard Client Background Client: A leading law firm in USA Industry Type: Law Services: Law practice Organization Size: 40+ Project Objective: For a better understanding, provide visualisations of the data on the LSA Dashboard. Learn how to enhance Rank and push the Ad to potential consumers by gaining data insights. Project Description Local Service Ads is a newer program by Google that allows advertisers to achieve a “Google Guaranteed” status in search engines when a visitor makes a search. Advertisers who participate in Google Local Service Ads will receive a larger ad space with their competitor’s local services ads and they will be able to feature their local businesses throughout organic search queries. There are various aspects that firms must concentrate on in order to win the Google services ad and so raise their ranking. These enhancements may be implemented if companies obtain current data about their leads and analyse it in order to take appropriate actions in the future. This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur. Our Solution The solution for the project includes data insights through visualisations which will help businesses to better analyse the available data. This solution will help the businesses in improvising the factors to increase their potential customers and raise their respective ranks. It is divided into two parts: databases and data dashboard. The databases will store the important data retrieved from the LSA dashboard and use them to calculate some important metrics. The data dashboard will represent those metrics in form of graphs and data in form of tables. Project Deliverables The project deliverables can be divided into two parts: Data in databases: The data is divided into three parts: Historical Account Data, Historical Phone Lead and Historical Message Lead. Using these three data, we calculate and store other important metrics like Cost per Acquisition, Conversion Rate, number of booked leads, number disputed leads, pending leads and approved leads. Google data studio dashboard: The dashboard will show the count of important metrics like total number of records, total interactions and different types of leads. It will represent different types of graphs portraying different kinds of information and tables containing major data like Lead data combined and Net monthly spent on Ads. Tools used For extracting the data from the LSA Dashboard, we have made our own tool by python scripts. The automation tool will store data in the excel sheets and google bigquery for respective businesses on a day to day basis. PyCharm for compiling and running the code. JsonViewer for processing Language/techniques used We have used the LSA API to extract data from the LSA Dashboard. Google Sheets API to store data in excel sheets. Bigquery API for storing data in google bigquery. The scripts for the automation tool were written in the Python programming language. Models used Software Model: RAD(Rapid Application Development model) Model In the RAD paradigm, less emphasis is placed on planning and more emphasis is placed on development activities. It aims to create software in a short period of time. Advantages of RAD Model: Changing needs can be addressed. Progress may be quantified. Increases component reusability. Encourages responses from consumers. Integration from the start solves a lot of integration concerns. Skills used API Data Abstraction Data Visualisation Automation of tools Exception Handling from Python Data Preprocessing Data Wrangling Databases used Two types of databases: Google excel sheets and google bigquery. Web Cloud Servers used Google BigQuery Cloud Database with up to 1 TB of free storage is being used. What are the technical Challenges Faced during Project Execution Some minor technical challenges were faced for clients with minimum data. For those, plotting graphs became difficult. How the Technical Challenges were Solved We tried to process the data, remove the blank data spaces and plotted the graph with available data. Business Impact It’s undeniable that Google’s Local Services ads (LSA) have changed the way home service businesses advertise online. The pay per lead system designed to provide the end-user with a quick, clean and trusted experience, gives small and medium-sized businesses a better shot at competing with national brands and massive budget operations. To win with the Local Services the businesses need to take care of some factors where data comes to help. Dialling in your service area, Profile and Budget: The data from the message and phone leads help to know whether they are potential customers. If they are potential customers, their location and profile can help you in charging them or not charging the leads. Mark your JOBS as Booked: The dashboard will display the number of archived leads and booked leads. This count can help you analyse your performance and how you can work to increase your potential customers. Deal with disputes: The dashboard will also represent the disputed disputes and approved disputes which will help you to deal with the disputes. Net Monthly Ad Spend: This is an important metric which helps the firms to make better decisions for their expenditure. They can have an efficient control over their expenditure once they have proper data available. Other metrics related to finances include Cost per lead, Cost per Acquisition and Conversion rate. Project Snapshots Fig.1: Data Dashboard for individual businesses-1 Fig.2: Data Dashboard for individual businesses-2 Fig.3: Consolidated Dashboard Fig.4: Historical Account Data Fig.5: CPA and CPL datasheet Fig.6: Lead Dispute Status",21.686744674467448,-7,36.7986798679868,37,1,52,0.11478599199457978,2.3514851485148514,https://insights.blackcoffer.com/google-local-service-ads-lsa-leads-dashboard/,606
22.183673469387756,22.183673469387756,6.704992435703479,293,"dashboard to track the analytics of the website using google analytics and google tag manager Client Background Client: A Automobile firm in India Industry Type: Automobile Services: Retail, Automobile Organization Size: 1000+ Project Objective The project objectives are as follows: Assisting the client with the setup for Google Analytics, Google Tag Manager which helps them in tracking the analytics of the website. Dashboards on website analysis presenting the important metrics and analysis related to websites. Project Description This project includes assisting the client to study the user flow and behaviour flow of the users on the websites. It had one main website and three other sub websites to analyse the button clicks, impressions and understanding the user’s behaviour on the website. Many events were to be tracked and converted to a dashboard in Google Data Studio to make it simpler to understand. This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur. Our Solution The main aim of this project is to display the event flow, user flow and behaviour flow through dashboards and analyse them to work on the areas of improvements. Google Analytics: Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process. Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics. Google Tag Manager: Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface. When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags. Google Data Studio Dashboards: The dashboards preview the important metrics related to the websites using graphs, tables to understand the trends, patterns in the users. The following steps were carried out for the project: Get the important metrics for website performance like the number of users visiting the websites, the average session duration, graphs related to the user acquisition like number of new users vs the returning users. This is related to the main website. For the sub websites, track the number of users clicking on specific buttons. Through this I understand the user flow. Compare between the number of users entering the website and those clicking on buttons. Track the metrics related to goal conversion like goal completions, goal conversion rate, goal completion rate and different goals and present it using visualisations. Provide data insights in the end providing scope of improvements and recommendations. Project Deliverables The main deliverable for this project were dashboards on Google Data Studio depicting important metrics related to website performance. There were three sub websites for which there were two types of views each. Each of the views had several buttons related to the product. The project was about finding the user flow and event flow on the views. Tools used Google Analytics: To track events, goal conversions and analyse the traffic sources/medium, the top viewed pages and the top cities and countries. Google Tag Manager: To set up the tags and triggers of button clicks, page visits as events in Google Analytics. Google Data Studio: To visually represent important metrics like impressions, clicks, goal completions using Google Analytics. Skills used Digital Analysis Data Analysis Data Visualisations Google Analytics What are the technical Challenges Faced during Project Execution The main technical challenge faced was that there were multiple events setup in Google Analytics for one event and thus identifying a particular one was difficult. How the Technical Challenges were Solved We had to communicate with the client to clarify about the event names. Although this took some time but it was necessary since accurateness of data is very essential for the project. Business Impact This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users. Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest. Some of the common metrics that are important in digital analytics include: Dashboard metrics : Some examples are pages per visit, bounce rate, and average duration of each visit. Conversion rate: This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter. Source/Medium Analysis: This analysis helps in understanding the traffic sources and medium on the website. This helps the businesses to work on strengthening the traffic sources to get better reach to the target audience. Traffic Analysis: The overall traffic analysis for the website provides information regarding the important metrics like users,avg. session duration and goal completions according to different source/medium. This will help the business to analyse different traffic channels performances. Project Snapshots Figure 1: Tracking of Buttons for Triber Virtual Studio Figure 2: Triber Goal Conversions Figure 3: Kiger 360 Experience Website Tracking Figure 4: Traffic Medium Analysis Figure 5: Overview of Dashboard Metrics Figure 6: Kiger Studio Experience Website Project website url Website URL: https://www.renault.co.in/ Dashboard URL:",26.604180431628027,-1,44.3267776096823,31,1,33,0.05841924388587759,2.4689863842662634,https://insights.blackcoffer.com/dashboard-to-track-the-analytics-of-the-website-using-google-analytics-and-google-tag-manager/,661
18.875,18.875,6.297777777777778,87,"metabridges api decentraland integration – ar, vr Client Background Client: A leading tech firm in the USA Industry Type: IT Services: Consulting, Software, Blockchain, Metaverse Organization Size: 20+ Project Objective To integrate with Metaverse environments with the help of EC2, S3 bucket and the Decentraland SDK. Project Description Move 3D model files from EC2 instance to S3 bucked using aws-sdk. Our Solution Configure s3 bucket in aws account, create an user for s3 bucket api keys, and api secret. Put the api key, aapi secret, bucket name and bucket region in environment variable to use them in app. Install aws-sdk to implement s3 bucket. Create a function to send file from nodejs server to s3 bucket. Project Deliverables Aws ec2 instance credentials, s3 bucket credentials. Code used in the project Tools used vs code editor, git bash terminal, google chrome web browser. Metamask wallet, cryptocurrency, blockchain, bitcoin, metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality Language/techniques used Javascript language is used. Metamask wallet, cryptocurrency, blockchain, bitcoin, metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality Models used dcl SDK (Decentraland sdk for nodejs), aws-sdk, awscli. Skills used Node js project setup, Dcl sdk setup, Aws ec2 instance setup with aws cli, S3 bucket connection with aws-sdk. cryptocurrency, blockchain, bitcoin, metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality Databases used No database is used Web Cloud Servers used AWS cloud server is used What are the technical Challenges Faced during Project Execution Making the application port in ec2 instance available globaly. How the Technical Challenges were Solved Search few blogs and videos for the solution. And make it done by doing some change in Security group in ec2 instance. Business Impact As Decentraland is a platform based of NFT so main part of business is related to NFT and cryptocurrency. Project Snapshots Project Video",23.016666666666666,-3,38.666666666666664,4,-1,2,0.02336448587212857,2.1955555555555555,https://insights.blackcoffer.com/metabridges-api-decentraland-integration/,225
42.666666666666664,42.666666666666664,6.396907216494846,77,"microsoft azure chatbot with luis (language understanding) Client Background Client: A leading retail firm in the USA Industry Type: Retail Services: e-commerce, retail business Organization Size: 100+ Project Objective To create an advanced chatbot using Microsoft Azure cognitive service to take orders from customer on behalf of a pizza restaurant and give order summary as end result to the user. Project Description The project uses MS Azure LUIS service for language understanding to receive order details from a customer and provide an order summary. Also display various menu options to the customer in a dynamic method. Our Solution Our solution is to create a chatbot on MS Azure platform using their LUIS service in bot-framework composer environment. Use dynamic hero cards to display menu so that user can get a better experience. Project Deliverables Chatbot Tools used Bot Framework composer Bot emulator MS Azure LUIS services Language/techniques used Bot framework composer Natural language processing Models used MS Azure LUIS MS Azure QnA MS Azure speed SDK Skills used Deep learning Web development Cloud tech Web Cloud Servers used Microsoft Azure web platform What are the technical Challenges Faced during Project Execution Monthly quota for LUIS authoring service was reached Tracking multiple items ordered by user Accessing relevant images for each menu item How the Technical Challenges were Solved Switching to a more suitable pricing tier which would have to eventually switch to when move onto production phase Creating custom functions and intents for different trackers Using open license images from internet Project Snapshots Project website url Demo",32.942955326460485,-2,39.69072164948454,3,1,7,0.05421686714327188,2.365979381443299,https://insights.blackcoffer.com/microsoft-azure-chatbot-with-luis-language-understanding/,194
19.23076923076923,19.23076923076923,6.662650602409639,50,"optimize the data scraper program to easily accommodate large files and solve oom errors Client Background Client: A leading tech firm in India Industry Type: IT Services Services: SAAS services, Marketing services, Business consultant Organization Size: 100+ Project Description Building a large data warehouse that houses projects and tenders data from all over the world that is to be collected from official government websites, multilateral banks, state and local government agencies, data aggregating websites, etc. Our Solution We had tried multiple solutions to prevent the program from running out of memory. We used python pandas techniques to control the use of memory which worked for some files and did not work for others. Provided more solutions using vaex ,dask module and datatables. Project Deliverables Desired changes to the code and committing them to github. Tools used Vscode Python Github Slack Language/techniques used Chunking dask Dataframe vaex datatable python. Skills used Cloud Python Time complexity What are the technical Challenges Faced during Project Execution System specs requirement was the main issue during this project because the RAM available was too less and got used up quickly. How the Technical Challenges were Solved Team viewer to use remote desktop which had higher specs would be sufficient enough to solve the problem. Business Impact Provided various techniques to solve memory issues. Suggested parallel programming to decrease the execution time by 12% making getting the tender data at a much faster rate. Project Snapshots Project website url https://github.com/Taiyo-ai/opentenders-eu https://opentender.eu",19.74050046339203,-6,30.120481927710845,5,1,8,0.09655172347205708,2.2951807228915664,https://insights.blackcoffer.com/optimize-the-data-scraper-program-to-easily-accommodate-large-files-and-solve-oom-errors/,166
31.25,31.25,7.108359133126935,140,"statistical data analysis of reinforced concrete Client Background Client: A leading research institution in the middle east Industry Type: Research Services: R&D Organization Size: 1000+ Project Objective Conducting statistical data analysis on the data provided for different types of reinforced concrete (using 3 different fibers – Steel, Date Palm and Polypropylene fibers) and also helping in preparing good research paper based on laboratory data. Project Description The project had two phase: Phase 1: In this phase, we had to do a comprehensive analysis on the data given and finally build statistical models for the variables present. The main motive was to understand the behaviour of concrete based on various parameters – Compressive strength, Flexural strength, water absorption capabilities of the concrete and many more. The analysis should include, but was not limited to: Comparison of Mo (control mix) with all mixes at 28 days for each parameter test Comparing all parameters for all specimens (all concrete mixes) with 28 days and also 6 months heat-cool and wet-dry all other expected analysis we could see you and do Phase 2: In this phase, we had to develop a structure for the research paper based on the results and analysis. The paper included sections – Abstract, Introduction ( literature, background and objective), Experimental program ( materials and methods), Results and discussion ( analysis and interpretation) and Conclusion ( summary, insights and remarks). Our Solution Providing a Comprehensive analysis for the concrete data – showcasing the key insights from it based on the parameters (compressive strength, etc). On the basis of results from the analysis, research paper was drafted which included all the deliverable. Project Deliverable A manuscript (drafted article) with the following: Abstract Introduction ( literature, background and objective) Experimental program ( materials and methods) Results and discussion ( analysis and interpretation) Conclusion ( summary, insights and remarks) References Tools used Tools used: Jupyter – Notbebook (Python) Numpy Pandas Sklearn Matplotlib Seaborn MS Excel Google spreadsheets Language/techniques used Python Statistical Modelling Statistical Inference Models used Statistical models – linear, polynomial, exponential and logarithmic models build for showcasing behavior of concrete mixes due to mixing of different fiber content and its effect on different parameters specified above. Skills used Coding – Python Performing statistical analysis – extracting inferences Building statistical models – through python or through Excel and its counterparts. Databases used No database was used. Web Cloud Servers used No Cloud server was used. What are the technical Challenges Faced during Project Execution The Challenges faced during project execution are: Getting statistical models from seaborn libraries, there is no direct way to get the models from the graphs created from data. Building models in excel and validating it (didn’t know how, had to learn it before applying it). How the Technical Challenges were Solved I had to use different libraries for building the models, later on turned to MS excel and spreadsheet because they were building models and were also able to showcase it on the data itself. For this, I learned how to build models on the aforementioned software through YouTube and blogs. Project Snapshots Project Video",29.8374613003096,-3,43.343653250774,16,1,9,0.0384615383382643,2.476780185758514,https://insights.blackcoffer.com/statistical-data-analysis-of-reinforced-concrete/,323
26.476190476190474,26.476190476190474,6.027777777777778,120,"google data studio dashboard for marketing, ads and traction data Client Background Overview Bankiom – the super banking app for MENA on a mission to make managing your finances easier. ☞ Open an account on your phone and get a virtual card in 3 minutes or less ☞ Manage all your bank accounts from one app and one control panel ☞ Save money and grow your wealth Website http://www.bankiom.com Company size 2-10 employees Founded 2019 Specialties Banking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech Project Objective Build a dashboard unifying all the platforms that we use: Google Ads, FB ads, Appsflyer, Mixpanel Project Description We want to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel): – App settings in Appsflyer – SDK Installation, test it (+ instruction for devs) – Ad sources setup in ad accounts (Facebook, Google Ads, etc) – Ad sources setup in Appsflyer – In-app conversions mapping – Conversion set up in ads sources – One link, smart script, and deep link setup – SKAD Network for IOS app Our Solution Built dashboard for each data source like Google Ads, Facebook Ads for tracking installs, channel spend, cost per install for both Android and IOS. Then, we made a dashboard for tracking the retention rates of customers and other events that they execute on the app like transfer money, user registration, connect banks. The data for these events was fetched from MixPanel. These dashboards were made using Google Data Studio. Project Deliverables We need to deliver dashboards for tracking the ads data from Google and Facebook and to track the events which the users perform on their app and for this data was collected from MixPanel. Tools used Following Tools were used for successful execution of the project Google Data Studio Adveronix Mixpanel Api BigQuery GCP Language/techniques used Code was written to create the pipeline to fetch MixPanel data through mixpanel Api and store it in bigquery. So, the code was written in Python. Skills used Following Skills were used to complete the project Data Preparation Data Visualization Python API BigQuery Google Cloud Platform Databases used For storing the data of the project Google Sheets and Google BigQuery were used. Web Cloud Servers used Web Cloud server used in this project was Google Cloud Platform. What are the technical Challenges Faced during Project Execution? Technical Challenges faced during the execution of the project was to understand how the api of the mixpanel works and how to connect it to Google BiqQuery. Another technical challenge that we faced was to find a free resource to connect the facebook ads data to data studio. How the Technical Challenges were Solved To solve the technical challenges we went through the documentation of the mixpanel api to get a understanding of how the things work. Based on that we built the pipeline to connect the mixpanel data to big query. The other technical challenge of finding a free resource to connect the facebook ads to datastudio for free was solved by researching for the various connectors available and we found an add on named ‘Adveronix’ which could connect the facebook ads data to google sheets which can eaily be connected to data studio. Project Snapshots Project website url https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f Project Video",23.923809523809524,-4,33.33333333333333,18,1,13,0.05295950139264953,2.175,https://insights.blackcoffer.com/google-data-studio-dashboard-for-marketing-ads-and-traction-data/,360
19.105263157894736,19.105263157894736,6.234513274336283,69,"power bi dashboard to drive insights from complex data to generate business insights Client Background Client: A leading marketing firm in the USA Industry Type: Market Research Services: Marketing, Consultancy Organization Size: 100+ Project Description Phase – 1: In this project first of all we have made heatmap between two columns named Author and Data Source. Then after two combining two tables named NY_data and nodeid_views made the report of all of the data. Phase – 2: Success of story was given by if pageviews is more than 35000, if pageviews lies between 3500-35000 the story was labelled as needs improvement and if it was below 3500 the story was labelled as failure. Phase – 3: The powerbi report was made to find different insights in the data like different tables were drawn between different attributes of data like pie chart, time series chart, comparison charts. The data is updated every week and the report is generated automatically. Our Solution We provided them Phase 1 in the powerbi sql editor by combining two tables using sql queries. For phase 2 we just used the power bi program tool and written a script in Python to calculate the success of story. For Phase 3 we used the internal features of Power BI to find insights of the data. Project Deliverables We have provided a PowerBI report file as deliverable for the project. Tools used Python, PowerBI, Google Chrome Language/techniques used Python Programming and SQL queries editor. Models used Waterfall model used in this project. Skills used Data cleaning, Data Pre-processing, Data Visualisation are used in this project. Databases used We have used the traditional file systems as database storage. What are the technical Challenges Faced during Project Execution Drawing heatmap in the PowerBI. Combining two tables on the basis of the pageviews. Converting the time series to data to 5 minute format. How the Technical Challenges were Solved We installed a new add on in the PowerBI to draw heatmap for the project and used the SQL editor to combine the tables on the basis of page views. We used python programming to convert the time series data to 5 minute time gap format. Project Snapshots Project Video",19.854494643688867,-3,30.53097345132743,11,1,6,0.044776119180218316,2.15929203539823,https://insights.blackcoffer.com/power-bi-dashboard-to-drive-insights-from-complex-data-to-generate-business-insights/,226
22.5,22.5,6.549222797927461,73,"real-time dashboard to monitor infrastructure activity and machines Client Background Client: A leading tech firm in Europe Industry Type: IT Services: Software Services Organization Size: 30+ Project Objective For the current project, we hope to develop a real-time dashboard (* it updates every several minutes). Currently, we have multiple Ubuntu machines that are sending messages every minute to Apache Pulsar. Project Description Developing a realtime updating dashboard to display the metadata of various machines on a server from pandio queue. The dahboard must display the count of “inactive” , “active” and “down” servers with a table displaying the details of all the machines in different color scheme for each type of server/machine. Our Solution We used Django framework to develop the dashboard as it didn’t require the ec2 instance to be active on machine which was the problem with using streamlit. For communication between webpage and fetched data we used django channel . We used django background task module to make the fetching run forever in background. Project Deliverables Real time updating Dashboard with separate color scheme for different types of machines. Storing the historical data in sqlite3 db. Tools used Django Web Channels D3 js Reddis server Skills used Python Django Framework Django web channels HTML/CSS + JS Databases used Django sqlite3 database. Web Cloud Servers used AWS What are the technical Challenges Faced during Project Execution Making the dashboard run forever using streamlit Data updation in realtime when using django channels How the Technical Challenges were Solved Switched the entire dashboard to django framework We redirected data to channels on local reddis server. Project Snapshots Project website url Development hosted URL",24.129533678756477,-3,37.82383419689119,10,-1,1,0.023121387149587357,2.2694300518134716,https://insights.blackcoffer.com/real-time-dashboard-to-monitor-infrastructure-activity-and-machines/,193
24.5,24.5,6.408536585365853,51,"power bi data-driven map dashboard Client Background Client: A leading marketing firm in the USA Industry Type: Market Research Services: Marketing, Consultancy Organization Size: 60+ Project Objective Change bubble colors dynamically. Make table and charts linked. If a user clicks on tables values, then the bubble chart on the map should be highlighted that relates to the table. Project Description “I have a map visual. I would like to dynamically change the colours of some of the bubbles.” The report page has several filters and KPI Dashboard, whose metrics change dynamically when the user clicks a certain element. Similarly the map should also change dynamically relative to the filter. Our Solution Added the website data from Details table to the map visualization, it makes the bubbles get coloured dynamically according to the requirement for websites data. Project Deliverables The Power BI ( .pbix ) file updated with solution Tools used Power BI Skills used Power BI Data Visualization Data Analysis Databases used The database that came in with the Power BI file received from client What are the technical Challenges Faced during Project Execution The map was not linked Map Bubbles were not dynamic How the Technical Challenges were Solved Refactoring the data model and using appropriate keys to link the data together That made Map to change according to Slicers/Filters To Change the colour, Bookmark buttons were used in the dashboard to bring up the dynamic colour changing with slicing (works after being published) Project Snapshots Project Video",22.2390243902439,0,31.097560975609756,4,0.9999998333333611,6,0.04285714255102041,2.201219512195122,https://insights.blackcoffer.com/power-bi-data-driven-map-dashboard/,164
23.15,23.15,6.41875,116,"electric vehicles (ev) load management system to forecast energy demand Client Background Client: A leading energy consulting firm in the USA Industry Type: Energy Services: Energy solutions, Consultancy Organization Size: 100+ Project Objective Create a Machine learning solution to manage electricity for electric vehicles. Main Tasks: Percentage probability of user plugin his vehicle today by user’s plugin date history Reduce the probability of plugin time according to user’s plugin time history Project Description We need to calculate the date and time probability that the user will plugin his vehicle today based on his plugin date and plugin time history. We also need to decrease time probability based on the user’s past time range. Our Solution We converted the user’s plugin data into binary values like 0 if the user hasn’t plugged-in his vehicle on that day and 1 if he plugged-in. We identified the driven distance based on the amount of charge used between two plug-in times. Then we trained the Ridge Regression ML model for identifying each day driven kilometer. From these kilometres we have identified the probability that user’s will plug-in today and it will increase day by day till the user does not plug-in his vehicle. For time probability we have used Probability Distribution Function (PDF) and Cumulative Distribution Function (CDF). These functions will decrease probability according to the user’s time range. Project Deliverables 2 python scripts to: Train regression model every day. Use model weights to generate probability values. Tools used Google Colab, VS Code, Google Drive, and MS Excel. Language/techniques used Python programming language, Data Analytics with numpy and pandas, Data Visualization with matplotlib, Statistics and Mathematics, Machine learning with SKlearn. Models used Ridge Regression Model Skills used Data Analytics, Data Visualization, Machine learning, Python, Statistics Databases used local data from MS Excel Sheet What are the technical Challenges Faced during Project Execution There are a lot of challenges faced during project execution At the start, we have only imaginary data so need to convert in a good format to apply machine learning models. Find the best machine learning model for the data. Decrease the time probability according to user’s time range How the Technical Challenges were Solved We have converted the data into weekday’s binary values like marked 0 if not plugged-in vehicle on that day and 1 if plugged and calculated driven distance by amount of charge used between two plugin dates. Tried different regression based machine learning models like Random Forest Regressor, XGBoost Regressor, Ridge Regression and checked accuracies of all models and choosed best one. For decreasing time probability we used Probability Distribution Function (PDF) and Cumulative Distribution Function (CDF). These functions decrease probability according to the user’s time range. Project Snapshots",23.76,-6,36.25,18,1,9,0.05434782589004411,2.31875,https://insights.blackcoffer.com/electric-vehicles-ev-load-management-system-to-forecast-energy-demand/,320
14.733333333333333,14.733333333333333,6.3428571428571425,52,"creating a custom report and dashboard using the data got from atera api Client Background Client: A leading Marketing firm in USA Industry Type: Marketing Services: Marketing, consulting, ads, business solutions Organization Size: 20+ Project Description Atera.com is used as our RMM, we have an agent on every machine. Which tracks the if a machine goes down, initial response time etc.., The website doesn’t provide any standard reports, So we needed to create a custom report. Our Solution Importing the data from Atera API into Jupyter Using Web Scraping download the JSON data Convert the JSON data to Data Frame and download it into PC. Clean the data with only required columns Upload the data into google sheets. Connect google sheets and google data studio Create the dashboard with the data Tools used Python (Pandas, requests) Google Sheets Google Data Studio Skills used Analytics Programming Language Databases used Contacts.csv Customers.csv Tickets.csv Alerts.csv What are the technical Challenges Faced during Project Execution? I found it difficult on downloading the data. How the Technical Challenges were Solved Once I figured I have been using the wrong Authorization key to login I was able to solve the issue, and convert the curl command into python Project Snapshots Project website url https://datastudio.google.com/reporting/5e61aecb-a420-41cc-afba-d0ca37f69132 Project Video",20.750476190476192,-3,37.142857142857146,10,-1,2,0.03968253936759889,2.2785714285714285,https://insights.blackcoffer.com/creating-a-custom-report-and-dashboard-using-the-data-got-from-atera-api/,140
26.923076923076923,26.923076923076923,7.096385542168675,117,"azure data lake and power bi dashboard Client Background Overview Stone is a video bibliographic tool for journalists and other researchers. It allows users to capture, annotate and share their journeys through digital and physical space, producing verifiable logs and generating monetizeable video highlight reels that can be embedded in digital and other media – showcasing key moments and telling the story behind the story. Our mission is to address distrust and disinformation with transparency and authenticity, while simultaneously tilting the information ecosystem in favour of quality original work. Research is valuable. Make it Visible. Write In Stone. Website http://www.writeinstone.com Company size 2-10 employees Headquarters Blackheath, New South Wales Founded 2017 Specialties Research Transparency, Trust, Video Content, Journalism, Proof Of Work, and Bibliographic Standards Project Objective Working on Microsoft Azure Analytics Services Verifying that indicators are being gathered in an intended manner, in line with GDPR provisions Building and analyzing dashboards and, specifically, conversion funnels Project Description To determine whether the already implemented indicators in are in intended fashion (separated by where these indicators are placed in the currently constituted funnel) Implement New Indicators Research Logged Average Number of Highlights per Project Total Hours of Content Produced Total Hours of Content Watched Daily unique visitors engaging with Stone, including the landing page, public research page(s), and the research portal Assess the dashboard set up in Azure, refine the existing dashboard, and determine whether an alternative is preferable. Review, refine, and optimize the WIS conversion funnel(s) Our Solution Built a Power BI dashboard as per the requirement. Also built a separate dashboard for the metric data from Azure. Project Deliverables Power BI dashboard which contains indicators funnels, new indicators(Research logged, Average number of Highlights per projects, Total hours of content watched etc), visualizations extracted from metric data. Tools used Power BI Azure Language/techniques used Power BI DAX Kusto Query Azure Skills used Data collection Data Analysis Data cleaning Feature engineering Querying Visualization Databases used Azure database Web Cloud Servers used Azure What are the technical Challenges Faced during Project Execution Difficulty in data collection.",29.56441149212234,-3,46.98795180722892,5,1,10,0.059090908822314055,2.6224899598393576,https://insights.blackcoffer.com/azure-data-lake-and-power-bi-dashboard/,249
19.40625,19.40625,5.720403022670025,105,"aws lex voice and chatbot Client Background Client: A leading tech firm in USA Industry Type: IT Services: eCommerce Organization Size: 40+ Project Objective Create a Voice and chatbot using AWS lex which can book flights, hotels, cars and book some fun activities in a city. Project Description We need to create a voice and chatbot using AWS lex and lambda function. The bot should book a flight, a hotel, and a car by asking some relevant questions to the user like destination, origin, date, etc. We also need to create a combination of all these which can plan the whole trip, flight, hotel, car and book some fun activities. Our Solution We have created aws lex intents and lambda functions for all bookings. Intents manage front ends like utterances (user can ask to the bot) and slots (bot replies with relevant questions). Lambda functions manage backend parts like which intent should be triggered if the user says “ book a flight” or “book a hotel” or “book a car”. For search results we have used some external APIs like Amadeus for flight, sabre for hotels and blablacar for car booking. We have modified search results by using Data Analytics (for getting the cheapest and good star flight and hotel), Machine learning (for getting user’s preferences by analyzing user’s history) and NLP (Differentiate search results by text analysis) techniques so users can get the best search results. Project Deliverables An aws lex voice and chatbot which can book flight, hotel, car and fun activities. This can be integrated with IOS applications. Tools used AWS Lex, AWS Lambda, AWS Cognito, AWS EC2, Google colab, VS code, FAST API, Uvicorn. Language/techniques used python, machine learning, data analytics, NLP. Models used TfIdf-Vectorizer and cosine similarity Skills used Data Analytics, Machine learning, NLP, Python, AWS, REST APIs. Databases used MySQL Web Cloud Servers used AWS What are the technical Challenges Faced during Project Execution The first challenge we have faced is the integration of AWS lex and lambda functions. Amadeus and Sabre APIs data was not in a good format so we have to clean some data and organize it in a usable format. We need to make some APIs so we can pass flight or hotel parameters and the APIs will give flight or hotel related data. Create a book button in the bot for booking flights, hotel,s and car. How the Technical Challenges were Solved So the integration of AWS lex and lambda function was very tough for us. Because lex uses some intentes to show responses from the lambda function. So we have created different lex intents to pass messages to lex bot from lambda function. And put some good coding to the lambda function so different messages can be handled by different intents. For flight, hotel and car search results we were using some external apis like amadeus, sabre and blablacars apis. These APIs have a lot of data and are not in a format we need. So first we cleaned data and then sorted data according to cheaper and best ratings results. We have used the best two results among all the results. We cannot use all the machine learning and data analytics part in aws lambda function. So what we did was we created some REST APIs which can handle all the data analytics and machine learning part and we hosted these APIs on AWS EC2 instance. We used these APIs in our lambda functions. So Creating a button in a chat bot or voice bot is always so different from providing text messages. For creating a button we used a response card structure in lambda function which can handle button and button related responses. Project Snapshots Project Video",18.34184508816121,-1,26.448362720403022,24,1,22,0.0707692305514793,2.0025188916876573,https://insights.blackcoffer.com/aws-lex-voice-and-chatbot/,397
16.55,16.55,6.2772277227722775,74,"impact of news, media, and press on innovation, startups, and investments Client Background Client: A leading research institution in the word Industry Type: Research, R&D Services: R&D Organization Size: 1000+ Project Objective Make data ready for predictive modelling. Making Google Data Studio dashboard. Project Description Phase – 1: In this project first of all we have to clean the data as the data was very noisy, we have to filter out only the needed columns of the data. Phase – 2: Finding co-relation between the pitchbook data and the other output files. Phase – 3: Making dashboard in Google Data Studio for the project. Our Solution We used pandas and numpy to clean the data and make useful for it to be used in predictive modelling. We have found the co-relation between the tempa msa pitchbook data and the output files like textual file, ai_ml_tm file etc. We have made the dashboard using the Google Data Studio. Project Deliverables We have provided a excel file consisting of clean data and the Google Data Studio report. Tools used Python, Google Data Studio, Google Chrome Language/techniques used Python Programming Models used Waterfall model used in this project. Skills used Data cleaning, Data Pre-processing, Data Visualisation are used in this project. Databases used We have used the traditional file systems as database storage. What are the technical Challenges Faced during Project Execution Cleaning the data was the major challenge faced while executing the project. The data has a lot of noise. It was difficult to find which data was useful and which data is not useful in this project. Secondly the co relation between the output files and pitchbook data. There was nothing common between both the datasets. So was difficult to find co-relation between them. How the Technical Challenges were Solved We used pandas dataframe to clean the data and make it ready for predictive modelling and used the Google Data studio to find insights between the different datasets. Project Snapshots Project Video",21.273465346534653,-4,36.633663366336634,13,1,13,0.09392265141479199,2.3267326732673266,https://insights.blackcoffer.com/impact-of-news-media-and-press-on-innovation-startups-and-investments/,202
29.11111111111111,29.11111111111111,6.02710027100271,105,"aws quicksight reporting dashboard Client Background Overview As a Singapore and Australia based startup, Drive lah (known as Drive mate in Australia) is a peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value. All trips on Drive lah are comprehensively insured through our insurance partners so car owners don’t have to worry about their insurance. The idea is simple: car ownership is expensive in Singapore (per month yet only use the car 5% of the time – cars are mostly parked. With Drive lah you can reduce the cost of ownership by renting it out when you don’t need it in a safe way. Renters can rent those cars when they are not used by their owners at good value. In a fast-growing non-ownership economy where taxi, food, beauty is available on-demand, Drive lah is envisioning to take the lead in distance travel and simplifying car access Website http://www.drivelah.sg Company size 11-50 employees Founded 2019 Project Objective Automating the process to get updated Metrics every week. Evaluate the following Performance Metrics which will be used on AWS Quick Sight for Performance Evaluations: Total Cancellations Cancellations by Host Weekly Guest Success Rate. Monthly Active User’s {MAUs} Monthly Active Listings {MALs} Total Approved & Live Listings Approved & Live InstantBookings Approved & Live Dl Go Delivery Booking Listings Weekly Active Listings {WALs} Successful HDM Unsuccessful HDM Booking Acceptance Rate Total Requested Trips New Listings Made Live Percentage of Live Listings Made Active Map Location Metrics Table with Postal Districts. DL Live Cars & DL L3M Active Cars Host Experience Team Weekly Dashboard New Weekly Listings Dashboard Two Transaction Metrics Build Code for extracting Daily Agent Activity Report on Daily Basis. Our Solution For Performance Metrics, we suggested that we will Code for each Metric & will store them in a Table on AWS RDS which will be directly synced to the AWS Quick Sight for Performance Evaluations. For Automating the process to get updated Tables of Metrics every week, we suggested to use a Virtual Machine on which we can upload all code files & can run a Cron Job for each file to automatically get updated on specified time every week. Tools used Jupyter Notebook PyCharm MySQL Workbench AWS Quicksight Language used Python Database Used Amazon Relational Database Service (RDS) What are the technical Challenges Faced during Project Execution? Tried with AWS Lambda Function to update tables on AWS RDS but Lambda Function was unable to run complete code. How the Technical Challenges Were Solved? Suggested to use a Virtual Machine on which we can upload our Code Files & can run Cron Job for automatically updating tables on regularly basis. Project Snapshots Metrics from Listings Table: Host Experience Metric: New Live Listings of Last 7 Days: Line Chart of Total Cancellations & Cancellations by Host: Line Chart of Monthly Active Users (MAU’s): Area Chart of Percentage of Live Listings Made Active: Line Chart of Number of DL GO Listings & Number of Instant Booking Listings: Line Chart of Monthly Active Listings (MAL’s): Line Chart of New Listings Made Live: Vertical Bar Chart of Total Approved & Live Listings: Project Video Link",23.026558265582658,-4,28.455284552845526,17,1,11,0.04545454531680441,2.127371273712737,https://insights.blackcoffer.com/aws-quicksight-reporting-dashboard/,369
26.608695652173914,26.608695652173914,5.97,116,"big data solution to an online multivendor marketplace ecommerce business Client Background Client: A leading eCommerce firm in the USA, Columbia, India, and Latin America Gangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for … Industry Type: eCommerce Services: e-commerce, retail business Organization Size: 100+ Project Objective To give User experience of easy and convenient Shopping by searching all the products like any medicines , Clothes , Gadgets etc in a single Website without going through all the E-Commerce Sites and make shopping easy and get the most affordable and best product. Project Description It’s an E-Commerce Sites that’s helps customer to compare different products that were available on different E-Commerce Sites like Flipkart , Amazon , Netmeds etc.It’s helps the user to visit only one sites to get what they need and find the perfect product without visiting all the sites.The gives the user a great and friendly Experience in Buying any Products.It’s Also have some Unique Similar Products Recommendation Based on user search and also have a ChatBot That’s solves User Query .It’s uses Big data and Rest API that’s help the projects for regular updates and regular fetching of the new products. Our Solution In BlackCoffer We create the flow of the Big Data and all Backend Solution That is requires for this futuristic E-Commerce Sites.We Create Pipelines for the data of all the products and their price and url fetch from different E-Commerce Sites using Custom made APIs And perform many data cleaning, data transformation and data validation techniques to make sure the standard of data to be used by Our Sites .We also get Additional Feature from the scraped data By using Different APIs . We also create automation and custom python scripts that helps us to achieve some outstanding data related tasks. Project Deliverables Python script for performing ETL and Cypher Query for big data Handling. Tools used Jupyter Notebook DSS VS Code Language/techniques used Python No SQl Cypher ETL Models used Similar Price API Whatsapp Chat API Similarity Server to get similar products Skills used Data Engineering Data Analysis Python Programming Rest APIs Databases used DSS NEO4J MongoDB Web Cloud Servers used Linode AWS What are the technical Challenges Faced during Project Execution Data Cleaning : -The Scraped that will be used by our sites is coming from different sources and also it’s not’s that clean to be used by sites .This is the very first problem every data scientist faced during the whole process. Data Merging :- The data is scraped from around 140 sources that’s why it’s very difficult to maintain the attributes that should be used by all the sources and we can get a clean and sufficient amount of data to process. Data Validation :- There are many records that have null values and missing values that disturb the users experience a lot .That should be handle with very care. How the Technical Challenges were Solved Data Cleaning : – For Data cleaning we used Python Data Frame and Pandas and data structure and handles the data cleaning and optimize our data for get correct data format and useful data. Data Merging : – For data Merging and data transformation we used pandas that help to get the appropriate data that can used further And also make Python pipelines for future updation. Data Validation :- For that data validation we use some fundamental property and feature selection that’s help us to make the appropriate data format and records to be used In our sites. Project Snapshots Project website url https://gangala.in/",22.243478260869566,-4,28.999999999999996,23,1,27,0.09281437097959769,2.15,https://insights.blackcoffer.com/big-data-solution-to-an-online-multivendor-marketplace-ecommerce-business/,400
24.5,24.5,6.094650205761317,76,"gangala.in: e-commerce big data etl / elt solution and data warehouse Client Background Client: A leading eCommerce firm in the USA, Columbia, India, and Latin America Gangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for … Industry Type: eCommerce Services: e-commerce, retail business Organization Size: 100+ Project Title Gangala.in: E-commerce site gathering data of different products from various sources and providing it on a single platform Project Objective Provide up-to-date data of any given product on the website along with 3-5 prices of that product from different sites for the customer to compare and buy. Project Description A platform in which users can get price data of any product from multiple sites. The client provided us with raw data. We were tasked with building a pipeline for the data, build API’s to get product data such as price and update them and make sure that all the data is available for the front end team to access. Our Solution We built them a pipeline to process and clean the raw data provided. We built API’s to fetch the updated data of the products. Neo4j was used as the intermediary data and mongoDB was used as our primary database. We also process the images of each product and remove any unwanted texts from it and add the client’s watermark. Project Deliverables A fully-updated database with up to date data on all the products and each product having atleast 3-5 prices from different sites. Tools used Numpy package Json package csv package concurrent futures package (for multithreading) Py2neo package (to connect to neo4j using python) Language/techniques used Python Cypher Query Language (CQL) APOC Queries Databases used Neo4j MongoDB Dataiku Odoo DSS Web Cloud Servers used Linode cloud servers What are the technical Challenges Faced during Project Execution We were asked to process 3million products per day and this was a challenge as the VM’s we used were not able to handle the load. How the Technical Challenges were Solved We were able to overcome the challenge by using Asynchronous processing of the data thereby increasing the speed of the processing reducing the cost on the client side as well Project website url https://gangala.in/",22.310288065843622,-3,31.275720164609055,15,1,9,0.05769230741494083,2.2016460905349793,https://insights.blackcoffer.com/gangala-in-e-commerce-big-data-etl-elt-solution-and-data-warehouse/,243
21.951612903225808,21.951612903225808,6.076923076923077,361,"quickbooks dashboard to find patterns in finance, sales, and forecasts Client Background Client: A leading marketing firm in the USA Industry Type: Marketing Services: e-commerce, retail business, marketing Organization Size: 100+ Project Objective Build a fully Integrated BI Platform in PowerBI using native connectors and APIs(QuickBooks and Airtable) to pull real time data from many sources. Project Description For building a fully integrated BI Platform , the data has to come from the following sources to feed it to PowerBI – · QuickBooks : An accounting software that accepts real-time business payments , manage and pay bills, manage organisation’s deposits/expenses , customers ,and payroll functions. The following data/tables has to be fetched from Quickbooks – o Customer o Invoices o Product & Services o Payments o Expenses o Deposits o Accounts o Vendors o Departments o Classes · Airtable : An online database hybrid platform for creating and sharing relational databases with friendly user interfaces. The following databases with multiple data table has to be fetched from Airtable – o Marketing Data Analytics Base (Google Ads , Facebook Ads) o Payroll Tracking (Payroll , Hours Log) This Quickbook and Airtable real time data has to go to the powerBI service ( https://app.powerbi.com ). Then create useful visualisation and dashboards based on plan and feedback from the executive team. All visuals in dashboards should automatically update without any intervention to make it fully integrated. Our Solution Collecting data tables from data sources : Data Pipeline(QuickBooks to Airtable) – We have built a Data Pipeline in Python that uses quickbooks API( https://pypi.org/project/python-quickbooks/ ) to get raw data tables from QuickBooks and uses Airtable API ( https://api.airtable.com/v0/base_key/Table_name?api_key=YOUR_API_KEY ) to write/update data in Airtable. It fetches all the below raw tables after making requests to QuickBooks API – Customers , Invoices , Expenses , Deposits Accounts , Departments , Vendors etc. After getting these raw data tables , pipeline converts it into DataFrame , then writes/updates it into Airtable. The Pipeline is deployed in a server that runs every night , it fetches the data from QuickBooks API and writes/updates to Airtable. Airtable to PowerBI – As there is no connector available to sync data from Airtable to PowerBI. We have used pagination using DAX queries to get data from Web Sources i.e. Airtable API. Pagination fetches the data page by page using a source and offset technique set by the Airtable API developers. It successfully fetches all the below bases from Airtable API – Marketing Data Analytics Data (Google Ads , Facebook Ads) Payroll Data (Payroll , Hours Log) Scheduled Refresh : To refresh visualization/dashboard (If incoming data from Airtable API has updated) , set refresh time in powerBI service. Preprocessing of Data – We have used DAX queries to prepare and process the raw data coming from Airtable like – Split data , typecast data Filter data (fill missing values , delete irrelevant rows etc.) Create visualizations/Dashboards – We have used following techniques to create visualizations – Used M code queries to extract useful/desired data Used measure to perform calculations on data Use a calculated table to create a relationship between two tables. Used data joining (Union , Intersection) to get desired data Project Deliverables Below are the services that we provided to client after completion of this project – Deployed Data Pipeline : A Data Pipeline connecting QuickBooks to Airtable to sync in the following data tables – Customers Invoices Product & Services Expense Deposits Payments Accounts Vendors Departments Classes QuickBooks Data Dashboard : It contains following visualizations – KPIs – Total Revenue Total Spend Total Profit Profit Margin No. of Customer Line Charts – Revenue/Expense over days Bar Charts – Revenue & Expenses by Businesses Profit/loss by Businesses Revenue & Expense by Class Profit/loss by Class Pie Chart Expenses by Category Paid/Unpaid Invoices Tables – [Class , Business , Revenue , Spend , Profit , Profit Margin) [Customer , Balance , Due(in days)] [Customer , Balance , OverDue] [Account , QuickBooks Balance] Filters/Slicer – Transaction Date Business Class Marketing Analytics (Facebook Ads) Dashboard – KPIs – All Impressions Total Reach Total Link Clicks Average CPM Amount Spent on Ads Total Budget Budget Left Line Charts – Avg. Frequency Over Days Avg. CPC over days Impressions , Reach and Page Engagement over days Link Clicks by day and Account Name Results , Cost per Results over days Ad set Budget and Amount Spent Over days Bar Charts – Ad set Budget and Amount Spent by Account Name Gauge – Daily Avg. Links Count of Ongoing Campaigns Tables – Top Compeigns [Account name , Compeign name , Link Clicks , Impressions , Reach , Avg. Frequency , Social Impressions] Filters/Slicer – Account name Date Range Marketing Analytics (Google Ads) Dashboard – KPIs – Total Impressions Total Clicks Total Conversions Total Cost Daily Avg. Cost Daily Avg. CTR Daily Avg. Conversion Rate Daily Avg. Cost per Conversion Line Charts – Clicks and Conversions over days Avg. CPC over days by Day and Google Ad Account Clicks per Impressions by Day and Google Ad Account Impressions by Day and Google Ad Account Cost by Day and Google Ad Account Clicks by Day and Google Ad Account Gauge – Avg. Daily new Conversions Pie Chart – Count of Google Ad Accounts Tables – Top Ads [Ad name , Ad Group , Conversions] [Google Ad Account , Impressions , Clicks , Conversions] Filters/Slicer – Date Range Google Ad Account Name Payroll Dashboard – KPIs – $ Total Payroll $ Avg. Rate Count of Invoice Total Payroll Time (in hrs.) Avg. TurnArroundTime (in Days) Total Hours Line Charts – Avg. Rate over Days Avg. daily Pay Amount Bar Chart – Payroll time by Employee $ Payroll by Employee Hours by Entity Total hours by Employee Pie Chart – Paid/Unpaid Invoices Tables – Payroll [Employee , Count of Invoice , Total Due , Paid Before/After Due Date] Filters/Slicer – Date Range Employee name Entity name Tools used PowerBI Language/techniques used Python Pagination Skills used Programming in Python Data Structure & Algorithm API Integration (QuickBooks , Airtable) File Handling PowerBI(with DAX , M code queries) Data Analytics What are the technical Challenges Faced during Project Execution? QuickBooks Refresh Token Expired Issue : As stated in QuickBooks Developer Guide , we need refresh token to access QuickBooks API and it expires after 101 days. But that is not the case , it usually expires within 2 to 4 days depending on how frequently we access the API. In that case our deployed Pipeline does not work if the token expired. Getting data from Airtable to PowerBI : As PowerBI has no Airtable data source connector to fetch data from Airtable , we did use Web Source connector using Airtable data web links. It only fetches the 1st page that is 100 rows from Airtable base because Airtable API gives only 100 rows/request. Dynamic Data Source Refresh Issue : As the URL of Airtable bases data changes based on the size of data. PowerBI recognizes it as Dynamic Data Source , hence it gives the error “Dynamic Data Source Refresh Error” in PowerBI Service. How the Technical Challenges were Solved QuickBooks Refresh Token Expired Issue : As the token may expire anytime after 2 days , so to resolve this we have added a gui element in Pipeline so that if token expires a pop up will appear asking for a new refresh token , until the consumer enters a valid new token from their QuickBook developer account , a pop up will keep coming and pipeline will be paused. Once the user enters a new token , the pipeline will continue working. Getting data from Airtable to PowerBI : To resolve this issue , we have used Pagination technique as below – First request Airtable API with proper URL , api_key and blank_offset (data_url?API_KEY=api_key?OFFSET=blank_offset) This request returns first 100 rows of data and a new offset value Now replace the previous offset value with a new offset value in the URL , and again make an API request. This request will return the next 100 rows of data and a new offset. Do this until you get a null offset (null offset means , all data has been fetched) This is how we get all the data of any size from Airtable bases. Dynamic Data Source Refresh Issue : The above mentioned Pagination technique converts dynamic URLs of Airtable bases data into static URLs. So PowerBI gives no error as it has been converted to a static data source. Now the client can refresh the dashboard manually by clicking the refresh button or can set automatic refresh daily at some given time. Project Snapshots Project Video https://www.youtube.com/watch?v=iemcyRtWPNU&ab_channel=Blackcoffer",23.590901571546738,-17,37.02564102564103,30,1,31,0.0498960498441829,2.2553846153846155,https://insights.blackcoffer.com/quickbooks-dashboard-to-find-patterns-in-finance-sales-and-forecasts/,975
19.605263157894736,19.605263157894736,6.08893280632411,159,"google data studio pipeline with gcp/mysql Client Background Client: A leading IT firm in Europe Industry Type: IT Services: e-commerce, retail business, marketing, Consulting Organization Size: 100+ Project Objective Creating a Data Pipeline to sync live data from FieldPulse to Google Data Studio using GCP/MySQL. Project Description There is a Virtual Machine up and running and MySQL in Google Cloud(GCP). Get the following live data from FieldPulse to Google Data Studio(GDS) for making Business Dashboard in GDS – Job Data Tag Data Team Member Data Team Data Such that if data changes in FieldPulse , GDS Dashboard should update automatically. Our Solution For fetching data from FieldPulse – Data Pipeline (FieldPulse to GCP MySQL) : We have created a Data Pipeline that uses web scraping to fetch data from FieldPulse. It makes a GET request to the FieldPulse API , and the API returns raw data. Convert this into json format then in Dataframe. Now , create new tables in GCP MySQL and insert/update the data accordingly. Insertion & Updation of Data : Insertion : If any data fetched from Fieldpulse is not present in their respective database table , then insert that data in the table. Updation : If any data fetched from Fieldpulse is present in their respective database table , then update that data in the table. Deploy the above Data Pipeline in GCP VM instance : Deploy the above data pipeline in GCP VM so that data gets updated every hour from FieldPulse to MySQL. For getting data from GCP MySQL to Google Data Studio(GDS) : Connecting GCP MySQL to Google Data Studio : Connect GCP MySQL to GDS as follows – Open a new report Click on add data Choose MySQL connector Enter following fields : Host Name or IP : xxx.xxx.xxx.xxx Database : xyz Username : xyz Password : ********** Enable SSL Upload server-ca.pem certificate Upload client-cert.pem certificate Upload client-key.pem certificate Click Authenticate Add whatever table you want Build Visualization Project Deliverables Below are the services that we provided to client after completion of this project – Deployed Data Pipeline in GCP : A Data Pipeline connecting FieldPulse( https://webapp.fieldpulse.com/ ) to GCP MySQL that is deployed on a client’s GCP VM instance. It updates the data in MySQL every hour. It extracts the following data tables from FieldPulse – Job Data Tag Data Team Member Data Team Data Maintaining a log file in Google Cloud : There is a log file in cloud to resolve unexpected error quickly if any , log file stores following details – last pipeline synced time Error type if any Error location if any Work Order Data : Job id Work order no. Tags titles Start_time Job_type Created By Status Invoice_status Assigned teams name Project_id Assignment_count Assignable_type Notes Customer_notes Customer_first_name Customer_last_name Location Assigned_team_members name End_time created_at Job Tag Data : Tag ids Company_id Mongo_id Title (Tag name) Type Color Created_at Updated_at deleted_at Setup to Connect GCP MySQL to Google Data Studio(GDS) : Provided a setup to connect GCP MySQL to GDS easily. Client can access his live data from MySQL to GDS and make visualizations out of it. Tools used Google Colab Language/techniques used Python Web Scraping MySQL Skills used Programming in Python Data Structure & Algorithm Web Scraping File Handling Google Cloud Google Data Studio Databases used MySQL Web Cloud Servers used Google Cloud Platform (GCP) What are the technical Challenges Faced during Project Execution Getting Data from FieldPulse : As there is no open source package/library in Python for accessing Fieldpulse API , we struggled a lot to get the desired data from FieldPulse. Setting Up Connection from GCP MySQL to GDS : Due to firewall and VPN , connection was not set up as IP address changes while using VPN. It was showing an error every time someone tries to connect to MySQL from their Google Studio account. How the Technical Challenges were Solved Getting Data from FieldPulse : We did use web scraping for this. We explored all the API addresses. We connected to each possible address and got the data then explored the data. Made a list of addresses which contains data of our interest. Also data is stored in a scattered and cascaded manner in FieldPulse with ids. So , we had to fetch a lot of extra tables and then join multiple tables to get a desired data table. Setting Up Connection from GCP MySQL to GDS : To resolve this issue , we did as below – set up the IP address in GCP MySQL security to 0.0.0.0 , so that any system can access it. (VPN issue resolved) Enabled the SSL in GCP MySQL. (to prevent it from unauthorized access) Project Video",20.411275223632202,-14,31.422924901185773,24,-1,3,0.03406813620427227,2.108695652173913,https://insights.blackcoffer.com/google-data-studio-pipeline-with-gcp-mysql/,506
27.555555555555557,27.555555555555557,6.1925675675675675,187,"marketing, sales, and financial data business dashboard (wink report) Client Background Client: A leading retail firm in Australia Industry Type: Retail Services: e-commerce, retail business, marketing Organization Size: 100+ Project Objective Bringing in data from many sources(Google Analytics , ServiceM8 and Xero etc.) and making Business Dashboard KPIs in Wink Report. Project Description For building Business Dashboards in Wink Report , collect data from the following sources – ServiceM8 Xero Facebook Google Ads Communiqa Explore/analyze the underlying data tables from each Data Source. Make useful reports using different tables from different data sources based on client’s requirement. Set up formulas in each report to calculate desired fields. Add a custom visualization to each report for making dashlets. Add dashlets to newly created dashboards. Our Solution For collecting the data from the sources (ServiceM8 , Xero , Facebook , Google Ad) native connectors have been used , available in the Wink Report. It fetches the following data/tables from around the given data sources – ServiceM8 Connector – Assets Client Invoices Job Allocations Jobs Materials Payments Xero Connector – Bank Transaction Items Budget Vs Actual Employees Payments Payslip Products Purchase Orders Purchase Invoices Sales Invoices Transaction Facebook Connector – Facebook Ad Insights Google Ads Connector – Ad Insights Google Analytics Connector – eCommerce Campaign Totals Data Pipeline : For collecting data from Communiqa website ( https://www.communiqa.com.au/ ) , web scraping has been used as there is no connector available for Communiqa to Wink Report. By scraping Communiqa , we get the following data – Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc. Then , we have merged different tables from different sources to get desired reports. Store all reports belonging to the same dashboard in a separate folder. Do this for all the dashboard , then setup formula for calculating desired fields. Add appropriate visualization to each report for each folder. Then , finally add all dashlets belonging to the same folder to a newly created dashboard. Project Deliverables Below are the services that we provided to client after completion of this project – Data Pipeline(Communiqa to Wink Report) : A Data Pipeline connecting Communiqa to Wink Report to sync in the following data tables – CSR calls [Account , Date , Total calls , Total unanswered calls , Total engaged calls , Total answered calls , Total minutes etc] Company Performance Dashboard : It contains following visualizations – KPIs – Sales This Month Leads Booked Today Sales Today Revenue This Month Cash Payment This Month Conversion Rate Open Warranty Jobs Bar Charts – Scheduled Jobs by Category Sales by Month Revenue by Month Tables – Open Jobs from Last month[Job Id , Opened Date , Status, Invoice Amount, Amount Paid] Filters/Slicer – Date Range Job Status Date Grouping(Daily/Monthly/Yearly) Lead Generation Dashboard – KPIs – Total Website Traffic this month Average Daily Website Traffic this month No. of Conversion this month Total Marketing Investment this month Marketing Budget Tracking Cost per Acquisition Line Charts – Link Clicks and conversion by month Total marketing spend by month Bar Chart – Lead Generation Count by Source Pie Chart – Lead Generation Source by Invoice Amount Filters/Slicer – Date Range Job Status Lead Conversion Dashboard – KPIs – All Employees monthly Sales Target All Employees monthly Conversion Rate Filters/Slicer – Date Range Job Status Company Leads/Target Dashboard – KPIs – Total Hi-pages Lead today Total Hi-pages Lead this month Total OneFlare Lead today Total OneFlare Lead this month Total Google Ads Lead today Total Google Ads Lead this month Total Facebook Ads Lead today Total Facebook Ads Lead this month Company Daily Sales Target Company Monthly Sales Target Filters/Slicer – Date Range Job Status Tools used Wink Report Language/techniques used Python Web Scraping Skills used Data Analytics Data Visualization Programming in Python Data Structure & Algorithm Web Scraping File Handling What are the technical Challenges Faced during Project Execution Merging reports from different data sources : Faced the issue of making the cross report from different data sources. Take live parameter input daily in Dashboards from User : Taking live user parameter input daily to feed in Wink report Dashboard. So that dashboard KPIs can change accordingly. How the Technical Challenges were Solved Merging reports from different data sources : Resolved this issue by using merge report configuration. Using this we were able to join tables from different data sources like – Left join , Right join , Union etc. Take live parameter input daily in Dashboards from User : To resolve this issue , we added a custom field in reports with input tag. Users can enter their parameter in this custom field and all dashlets in the dashboard would update automatically. Project Snapshots Project Video",23.65735735735736,-3,31.58783783783784,9,1,20,0.04166666659118357,2.189189189189189,https://insights.blackcoffer.com/marketing-sales-and-financial-data-business-dashboard-wink-report/,592
3.3055555555555554,3.3055555555555554,6.333333333333333,6,react native apps in the development portfolio Here are the list of react native apps developed by the team and the resources: https://itunes.apple.com/us/app/truckmap-truck-gps-routes/id1198422047?mt=8 https://play.google.com/store/apps/details?id=com.truckmap.truckmap https://play.google.com/store/apps/details?id=com.verifai.standalone https://apps.apple.com/nl/app/verifai/id1504214033 https://apps.apple.com/de/app/meetlist-lokale-aktivit%C3%A4ten/id1439183715 https://play.google.com/store/apps/details?id=de.mlug.meetlist https://play.google.com/store/apps/details?id=com.payroo.employee https://play.google.com/store/apps/details?id=com.vahcare https://play.google.com/store/apps/details?id=com.candorivf,21.322222222222223,0,50.0,0,0.0,0,0,2.4166666666666665,https://insights.blackcoffer.com/react-native-apps-in-the-development-portfolio/,12
14.333333333333334,14.333333333333334,6.688679245283019,57,"a leading law firm in the usa, website seo & optimization Client Background Client: A leading marketing firm in the USA Industry Type: Marketing Services: Marketing consulting Organization Size: 100+ Project Objective Connect website to Search Console, Google Analytics and Facebook Pixel through Google Tag Manager. Fix SEO of the website. Project Description Connecting website to Google Search Console, Google Analytics and Facebook Pixel through Google Tag Manager. Fixing SEO of the website. Our Solution Website connected to Google Search Console, Google Analytics and Facebook Pixel successfully. Fixed the meta description error broken link error 404 error, etc. Tools used Squarespace Google Tag Manager Google Analytics Google Search Console Language/techniques used JavaScript Skills used Squarespace Google Tag Manager Google Analytics Google Search Console JavaScript Project Snapshots Project website URL https://www.keepingorlandomoving.com/",27.24276729559749,-4,53.77358490566038,1,-1,3,0.06862745030757401,2.5754716981132075,https://insights.blackcoffer.com/a-leading-firm-website-seo-optimization/,106
